{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Required Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import random\n",
    "import pdb\n",
    "from torch.utils.data import DataLoader\n",
    "import logging\n",
    "import itertools\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import argparse\n",
    "from torch import optim\n",
    "import time\n",
    "import os\n",
    "from bleu_score import BLEU_SCORE\n",
    "# from models_viet import EncoderRNN, AttentionDecoderRNN, DecoderRNN\n",
    "from load_dataset_viet import *\n",
    "# from define_training_viet import *\n",
    "import torchtext\n",
    "from torch.utils.data import BatchSampler\n",
    "from torch.utils.data import SequentialSampler\n",
    "from torch.utils.data import Sampler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import copy\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Build data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vietnamese(Dataset):\n",
    "    def __init__(self, df, val = False):\n",
    "        self.df = df\n",
    "        self.val = val\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        english = self.df.iloc[idx,:]['en_idized']\n",
    "        viet = self.df.iloc[idx,:]['vi_idized']\n",
    "        en_len = self.df.iloc[idx,:]['en_len']\n",
    "        vi_len = self.df.iloc[idx,:]['vi_len']\n",
    "        if self.val:\n",
    "            en_data = self.df.iloc[idx,:]['en_data'].lower()\n",
    "            return [viet,english,vi_len,en_len,en_data]\n",
    "        else:\n",
    "            return [viet,english,vi_len,en_len]\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "def vocab_collate_func(batch):\n",
    "    MAX_LEN_EN = 48\n",
    "    MAX_LEN_VI = 48\n",
    "    en_data = []\n",
    "    vi_data = []\n",
    "    en_len = []\n",
    "    vi_len = []\n",
    "    for datum in batch:\n",
    "        en_len.append(datum[3])\n",
    "        vi_len.append(datum[2])\n",
    "    max_batch_length_en = max(en_len)\n",
    "    max_batch_length_vi = max(vi_len)\n",
    "    if max_batch_length_en < MAX_LEN_EN:\n",
    "        MAX_LEN_EN = max_batch_length_en\n",
    "    if max_batch_length_vi < MAX_LEN_VI:\n",
    "        MAX_LEN_VI = max_batch_length_vi\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        if datum[2]>MAX_LEN_VI:\n",
    "            padded_vec_s1 = np.array(datum[0])[:MAX_LEN_VI]\n",
    "        else:\n",
    "            padded_vec_s1 = np.pad(np.array(datum[0]),\n",
    "                                pad_width=((0,MAX_LEN_VI - datum[2])),\n",
    "                                mode=\"constant\", constant_values=PAD_IDX)\n",
    "        if datum[3]>MAX_LEN_EN:\n",
    "            padded_vec_s2 = np.array(datum[1])[:MAX_LEN_EN]\n",
    "        else:\n",
    "            padded_vec_s2 = np.pad(np.array(datum[1]),\n",
    "                                pad_width=((0,MAX_LEN_EN - datum[3])),\n",
    "                                mode=\"constant\", constant_values=PAD_IDX)\n",
    "        en_data.append(padded_vec_s2)\n",
    "        vi_data.append(padded_vec_s1)\n",
    "    vi_data = np.array(vi_data)\n",
    "    en_data = np.array(en_data)\n",
    "    vi_len = np.array(vi_len)\n",
    "    en_len = np.array(en_len)\n",
    "#     sorted_vi_len = np.argsort(-vi_len)\n",
    "#     vi_data = vi_data[sorted_vi_len]\n",
    "#     en_data = en_data[sorted_vi_len]\n",
    "#     vi_len = vi_len[sorted_vi_len]\n",
    "#     en_len = en_len[sorted_vi_len]\n",
    "#     print(en_len)\n",
    "    vi_len[vi_len>MAX_LEN_VI] = MAX_LEN_VI\n",
    "    en_len[en_len>MAX_LEN_EN] = MAX_LEN_EN\n",
    "        \n",
    "    return [torch.from_numpy(vi_data), torch.from_numpy(en_data),\n",
    "            torch.from_numpy(vi_len), torch.from_numpy(en_len)]\n",
    "\n",
    "def convert_idx_2_sent(tensor, lang_obj):\n",
    "    word_list = []\n",
    "    for i in tensor:\n",
    "        if i.item() not in set([PAD_IDX,EOS_token,SOS_token]):\n",
    "            word_list.append(lang_obj.index2word[i.item()])\n",
    "    return (' ').join(word_list)\n",
    "\n",
    "def convert_id_list_2_sent(list_idx, lang_obj):\n",
    "    word_list = []\n",
    "    if type(list_idx) == list:\n",
    "        for i in list_idx:\n",
    "            if i not in set([EOS_token]):\n",
    "                word_list.append(lang_obj.index2word[i])\n",
    "    else:\n",
    "        for i in list_idx:\n",
    "            if i.item() not in set([EOS_token,SOS_token,PAD_IDX]):\n",
    "                word_list.append(lang_obj.index2word[i.item()])\n",
    "    return (' ').join(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_collate_func_val(batch):\n",
    "    return [torch.from_numpy(np.array(batch[0][0])).unsqueeze(0), torch.from_numpy(np.array(batch[0][1])).unsqueeze(0),\n",
    "            torch.from_numpy(np.array(batch[0][2])).unsqueeze(0), torch.from_numpy(np.array(batch[0][3])).unsqueeze(0),batch[0][4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_LEN = 48\n",
    "train,val,test,en_lang,vi_lang = train_val_load(48, \"\", '/scratch/ark576/machine_translation_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_sorted_batches(train, bs):\n",
    "    batch_samp_list = list(BatchSampler(SequentialSampler(train), bs, drop_last = False))\n",
    "    np.random.shuffle(batch_samp_list)\n",
    "    batch_samp_list_merged = list(itertools.chain(*batch_samp_list))\n",
    "    return train.iloc[batch_samp_list_merged,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en_data</th>\n",
       "      <th>vi_data</th>\n",
       "      <th>en_tokenized</th>\n",
       "      <th>vi_tokenized</th>\n",
       "      <th>en_idized</th>\n",
       "      <th>vi_idized</th>\n",
       "      <th>en_len</th>\n",
       "      <th>vi_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rachel Pike : The science behind a climate hea...</td>\n",
       "      <td>Khoa_học đằng_sau một tiêu_đề về khí_hậu</td>\n",
       "      <td>[rachel, pike, :, the, science, behind, a, cli...</td>\n",
       "      <td>[khoa_học, đằng_sau, một, tiêu_đề, về, khí_hậu]</td>\n",
       "      <td>[8225, 2, 137, 4, 150, 573, 9, 15, 3717, 1]</td>\n",
       "      <td>[7, 1433, 5, 3054, 4, 13, 1]</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In 4 minutes , atmospheric chemist Rachel Pike...</td>\n",
       "      <td>Trong 4 phút , chuyên_gia hoá_học khí_quyển Ra...</td>\n",
       "      <td>[in, 4, minutes, ,, atmospheric, chemist, rach...</td>\n",
       "      <td>[trong, 4, phút, ,, chuyên_gia, hoá_học, khí_q...</td>\n",
       "      <td>[12, 5080, 264, 7, 20, 4552, 8225, 2, 3006, 9,...</td>\n",
       "      <td>[23, 812, 253, 8, 1043, 1342, 20, 8510, 2, 161...</td>\n",
       "      <td>51</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I &amp;apos;d like to talk to you today about the ...</td>\n",
       "      <td>Tôi muốn cho các bạn biết về sự to_lớn của nhữ...</td>\n",
       "      <td>[i, &amp;apos;d, like, to, talk, to, you, today, a...</td>\n",
       "      <td>[tôi, muốn, cho, các, bạn, biết, về, sự, to_lớ...</td>\n",
       "      <td>[23, 247, 16, 13, 272, 13, 33, 273, 100, 4, 14...</td>\n",
       "      <td>[26, 152, 47, 11, 38, 149, 4, 92, 787, 9, 15, ...</td>\n",
       "      <td>28</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Headlines that look like this when they have t...</td>\n",
       "      <td>Có những dòng trông như thế_này khi bàn về biế...</td>\n",
       "      <td>[headlines, that, look, like, this, when, they...</td>\n",
       "      <td>[có, những, dòng, trông, như, thế_này, khi, bà...</td>\n",
       "      <td>[6, 11, 24, 16, 21, 49, 35, 31, 13, 36, 51, 15...</td>\n",
       "      <td>[21, 15, 530, 419, 17, 193, 34, 488, 4, 1227, ...</td>\n",
       "      <td>32</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>They are both two branches of the same field o...</td>\n",
       "      <td>Cả hai đều là một nhánh của cùng một lĩnh_vực ...</td>\n",
       "      <td>[they, are, both, two, branches, of, the, same...</td>\n",
       "      <td>[cả, hai, đều, là, một, nhánh, của, cùng, một,...</td>\n",
       "      <td>[35, 55, 402, 118, 3364, 5, 4, 154, 260, 5, 20...</td>\n",
       "      <td>[41, 273, 70, 16, 5, 3618, 9, 178, 5, 198, 23,...</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             en_data  \\\n",
       "0  Rachel Pike : The science behind a climate hea...   \n",
       "1  In 4 minutes , atmospheric chemist Rachel Pike...   \n",
       "2  I &apos;d like to talk to you today about the ...   \n",
       "3  Headlines that look like this when they have t...   \n",
       "4  They are both two branches of the same field o...   \n",
       "\n",
       "                                             vi_data  \\\n",
       "0           Khoa_học đằng_sau một tiêu_đề về khí_hậu   \n",
       "1  Trong 4 phút , chuyên_gia hoá_học khí_quyển Ra...   \n",
       "2  Tôi muốn cho các bạn biết về sự to_lớn của nhữ...   \n",
       "3  Có những dòng trông như thế_này khi bàn về biế...   \n",
       "4  Cả hai đều là một nhánh của cùng một lĩnh_vực ...   \n",
       "\n",
       "                                        en_tokenized  \\\n",
       "0  [rachel, pike, :, the, science, behind, a, cli...   \n",
       "1  [in, 4, minutes, ,, atmospheric, chemist, rach...   \n",
       "2  [i, &apos;d, like, to, talk, to, you, today, a...   \n",
       "3  [headlines, that, look, like, this, when, they...   \n",
       "4  [they, are, both, two, branches, of, the, same...   \n",
       "\n",
       "                                        vi_tokenized  \\\n",
       "0    [khoa_học, đằng_sau, một, tiêu_đề, về, khí_hậu]   \n",
       "1  [trong, 4, phút, ,, chuyên_gia, hoá_học, khí_q...   \n",
       "2  [tôi, muốn, cho, các, bạn, biết, về, sự, to_lớ...   \n",
       "3  [có, những, dòng, trông, như, thế_này, khi, bà...   \n",
       "4  [cả, hai, đều, là, một, nhánh, của, cùng, một,...   \n",
       "\n",
       "                                           en_idized  \\\n",
       "0        [8225, 2, 137, 4, 150, 573, 9, 15, 3717, 1]   \n",
       "1  [12, 5080, 264, 7, 20, 4552, 8225, 2, 3006, 9,...   \n",
       "2  [23, 247, 16, 13, 272, 13, 33, 273, 100, 4, 14...   \n",
       "3  [6, 11, 24, 16, 21, 49, 35, 31, 13, 36, 51, 15...   \n",
       "4  [35, 55, 402, 118, 3364, 5, 4, 154, 260, 5, 20...   \n",
       "\n",
       "                                           vi_idized  en_len  vi_len  \n",
       "0                       [7, 1433, 5, 3054, 4, 13, 1]      10       7  \n",
       "1  [23, 812, 253, 8, 1043, 1342, 20, 8510, 2, 161...      51      57  \n",
       "2  [26, 152, 47, 11, 38, 149, 4, 92, 787, 9, 15, ...      28      26  \n",
       "3  [21, 15, 530, 419, 17, 193, 34, 488, 4, 1227, ...      32      25  \n",
       "4  [41, 273, 70, 16, 5, 3618, 9, 178, 5, 198, 23,...      14      16  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_dict = {'train':128,'validate':1, 'train_val':1,'val_train':128, 'test':1}\n",
    "shuffle_dict = {'train':True,'validate':False, 'train_val':False,'val_train':True, 'test':False}\n",
    "# train_used = shuffle_sorted_batches(train_sorted, bs_dict['train'])\n",
    "# train_used = train.iloc[:50]\n",
    "train_used = train\n",
    "val_used = val\n",
    "# val_used = val.iloc[:20]\n",
    "collate_fn_dict = {'train':vocab_collate_func, 'validate':vocab_collate_func_val,\\\n",
    "                   'train_val':vocab_collate_func_val,'val_train':vocab_collate_func,'test': vocab_collate_func_val}\n",
    "transformed_dataset = {'train': Vietnamese(train_used),\n",
    "                       'validate': Vietnamese(val_used, val = True),\n",
    "                       'train_val':Vietnamese(train.iloc[:50], val = True),\n",
    "                       'val_train':Vietnamese(val_used),\n",
    "                       'test':Vietnamese(test, val= True)\n",
    "                                               }\n",
    "\n",
    "dataloader = {x: DataLoader(transformed_dataset[x], batch_size=bs_dict[x], collate_fn=collate_fn_dict[x],\n",
    "                    shuffle=shuffle_dict[x], num_workers=0) for x in ['train', 'validate', 'train_val','val_train', 'test']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Build Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, embed_dim, hidden_size,n_layers, rnn_type = 'lstm', device = 'cuda'):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = Embedding(input_size, embed_dim, PAD_IDX)\n",
    "        self.rnn_type =  rnn_type\n",
    "        self.dropout_in = nn.Dropout(p = 0.1)\n",
    "        self.n_layers = n_layers\n",
    "        self.device = device\n",
    "        if rnn_type == 'gru':\n",
    "            self.rnn = nn.GRU(embed_dim, hidden_size,batch_first=True,bidirectional=True, num_layers = self.n_layers, dropout = 0.2)\n",
    "        elif rnn_type == 'lstm':\n",
    "            self.rnn = LSTM(embed_dim, hidden_size, batch_first=True,bidirectional=True, num_layers = n_layers,dropout = 0.2)\n",
    "\n",
    "    def forward(self, enc_inp, src_len):\n",
    "        sorted_idx = torch.sort(src_len, descending=True)[1]\n",
    "        orig_idx = torch.sort(sorted_idx)[1]\n",
    "        embedded = self.embedding(enc_inp)\n",
    "        bs = embedded.size(0)\n",
    "        output = self.dropout_in(embedded)\n",
    "        if self.rnn_type == 'gru':\n",
    "            hidden =  self.initHidden(bs)\n",
    "            sorted_output = output[sorted_idx]\n",
    "            sorted_len = src_len[sorted_idx]\n",
    "            packed_output = nn.utils.rnn.pack_padded_sequence(sorted_output, sorted_len.data.tolist(), batch_first = True)\n",
    "            packed_outs, hiddden = self.rnn(packed_output,(hidden, c))\n",
    "            hidden = hidden[:,orig_idx,:]\n",
    "            output, _ = nn.utils.rnn.pad_packed_sequence(packed_outs, padding_value=PAD_IDX, batch_first = True)\n",
    "            output = output[orig_idx]\n",
    "            hidden = hidden.view(self.n_layers, 2, bs, -1).transpose(1, 2).contiguous().view(self.n_layers, bs, -1)\n",
    "            return output, hidden, hidden\n",
    "        elif self.rnn_type == 'lstm':\n",
    "            hidden, c = self.initHidden(bs)\n",
    "            sorted_output = output[sorted_idx]\n",
    "            sorted_len = src_len[sorted_idx]\n",
    "            packed_output = nn.utils.rnn.pack_padded_sequence(sorted_output, sorted_len.data.tolist(), batch_first = True)\n",
    "            packed_outs, (hiddden, c) = self.rnn(packed_output,(hidden, c))\n",
    "            hidden = hidden[:,orig_idx,:]\n",
    "            c = c[:,orig_idx,:]\n",
    "            output, _ = nn.utils.rnn.pad_packed_sequence(packed_outs, padding_value=PAD_IDX, batch_first = True)\n",
    "            output = output[orig_idx]\n",
    "            c = c.view(self.n_layers, 2, bs, -1).transpose(1, 2).contiguous().view(self.n_layers, bs, -1)\n",
    "            hidden = hidden.view(self.n_layers, 2, bs, -1).transpose(1, 2).contiguous().view(self.n_layers, bs, -1)\n",
    "            return output, hidden, c\n",
    "        \n",
    "    def initHidden(self,bs):\n",
    "        if self.rnn_type == 'gru' :\n",
    "            return torch.zeros(self.n_layers*2, bs, self.hidden_size).to(self.device)\n",
    "        elif self.rnn_type == 'lstm':\n",
    "            return torch.zeros(self.n_layers*2,bs,self.hidden_size).to(self.device),torch.zeros(self.n_layers*2,bs,self.hidden_size).to(self.device)\n",
    "\n",
    "class Attention_Module(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim, device = 'cuda'):\n",
    "        super(Attention_Module, self).__init__()\n",
    "        self.l1 = Linear(hidden_dim, output_dim, bias = False)\n",
    "        self.l2 = Linear(hidden_dim+output_dim, output_dim, bias =  False)\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, hidden, encoder_outs, src_lens):\n",
    "        ''' hiddden: bsz x hidden_dim\n",
    "        encoder_outs: bsz x sq_len x encoder dim (output_dim)\n",
    "        src_lens: bsz\n",
    "        \n",
    "        x: bsz x output_dim\n",
    "        attn_score: bsz x sq_len'''\n",
    "        x = self.l1(hidden)\n",
    "        att_score = (encoder_outs.transpose(0,1) * x.unsqueeze(0)).sum(dim = 2)\n",
    "        seq_mask = sequence_mask(src_lens, max_len = max(src_lens).item(), device = self.device).transpose(0,1)\n",
    "        masked_att = seq_mask*att_score\n",
    "        masked_att[masked_att==0] = -1e10\n",
    "        attn_scores = F.softmax(masked_att, dim=0)\n",
    "        x = (attn_scores.unsqueeze(2) * encoder_outs.transpose(0,1)).sum(dim=0)\n",
    "        x = torch.tanh(self.l2(torch.cat((x, hidden), dim=1)))\n",
    "        return x, attn_scores\n",
    "        \n",
    "class AttentionDecoderRNN(nn.Module):\n",
    "    def __init__(self, output_size, embed_dim, hidden_size, n_layers = 1, attention = True, device = 'cuda'):\n",
    "        super(AttentionDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        encoder_output_size = hidden_size\n",
    "        self.embedding = Embedding(output_size, embed_dim, PAD_IDX)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.n_layers = n_layers\n",
    "        self.device = device\n",
    "        self.att_layer = Attention_Module(self.hidden_size, encoder_output_size,self.device) if attention else None\n",
    "        self.layers = nn.ModuleList([\n",
    "            LSTMCell(\n",
    "                input_size=self.hidden_size + embed_dim if ((layer == 0) and attention) else embed_dim if layer == 0 else hidden_size,\n",
    "                hidden_size=hidden_size,\n",
    "            )\n",
    "            for layer in range(self.n_layers)\n",
    "        ])\n",
    "        self.fc_out = nn.Linear(self.hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, dec_input,context_vector, prev_hiddens,prev_cs,encoder_outputs,src_len):\n",
    "        bsz = dec_input.size(0)\n",
    "        output = self.embedding(dec_input)\n",
    "        output = self.dropout(output)\n",
    "        if self.att_layer is not None:\n",
    "            cated_input = torch.cat([output.squeeze(1),context_vector], dim = 1)\n",
    "        else:\n",
    "            cated_input = output.squeeze(1)\n",
    "        new_hiddens = []\n",
    "        new_cs = []\n",
    "        for i, rnn in enumerate(self.layers):\n",
    "            hidden, c = rnn(cated_input, (prev_hiddens[i], prev_cs[i]))\n",
    "            cated_input = self.dropout(hidden)\n",
    "            new_hiddens.append(hidden.unsqueeze(0))\n",
    "            new_cs.append(c.unsqueeze(0))\n",
    "        new_hiddens = torch.cat(new_hiddens, dim = 0)\n",
    "        new_cs = torch.cat(new_cs, dim = 0)\n",
    "\n",
    "        # apply attention using the last layer's hidden state\n",
    "        if self.att_layer is not None:\n",
    "            out, attn_score = self.att_layer(hidden, encoder_outputs, src_len)\n",
    "        else:\n",
    "            out = hidden\n",
    "            attn_score = None\n",
    "        context_vec = out\n",
    "        out = self.dropout(out)\n",
    "        out_vocab = self.softmax(self.fc_out(out))\n",
    "\n",
    "        return out_vocab, context_vec, new_hiddens, new_cs, attn_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Utilites functions, training and evaluation functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Embedding(num_embeddings, embedding_dim, padding_idx):\n",
    "    m = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_idx)\n",
    "    nn.init.uniform_(m.weight, -0.1, 0.1)\n",
    "    nn.init.constant_(m.weight[padding_idx], 0)\n",
    "    return m\n",
    "\n",
    "\n",
    "def LSTM(input_size, hidden_size, **kwargs):\n",
    "    m = nn.LSTM(input_size, hidden_size,**kwargs)\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name or 'bias' in name:\n",
    "            param.data.uniform_(-0.1, 0.1)\n",
    "    return m\n",
    "\n",
    "\n",
    "def LSTMCell(input_size, hidden_size, **kwargs):\n",
    "    m = nn.LSTMCell(input_size, hidden_size,**kwargs)\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name or 'bias' in name:\n",
    "            param.data.uniform_(-0.1, 0.1)\n",
    "    return m\n",
    "\n",
    "\n",
    "def Linear(in_features, out_features, bias=True, dropout=0):\n",
    "    \"\"\"Linear layer (input: N x T x C)\"\"\"\n",
    "    m = nn.Linear(in_features, out_features, bias=bias)\n",
    "    m.weight.data.uniform_(-0.1, 0.1)\n",
    "    if bias:\n",
    "        m.bias.data.uniform_(-0.1, 0.1)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_mask(sequence_length, max_len=None, device = 'cuda'):\n",
    "    if max_len is None:\n",
    "        max_len = sequence_length.max().item()\n",
    "    batch_size = sequence_length.size(0)\n",
    "    seq_range = torch.arange(0, max_len).long()\n",
    "    seq_range_expand = seq_range.unsqueeze(0).repeat([batch_size,1])\n",
    "    seq_range_expand = seq_range_expand.to(device)\n",
    "    seq_length_expand = (sequence_length.unsqueeze(1)\n",
    "                         .expand_as(seq_range_expand))\n",
    "    return (seq_range_expand < seq_length_expand).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_decode(encoder,decoder,data_en,data_de,src_len,tar_len,rand_num = 0.95, val = False):\n",
    "    if not val:\n",
    "        use_teacher_forcing = True if random.random() < rand_num else False\n",
    "    #     print(\"tar_len\",tar_len)\n",
    "        bss = data_en.size(0)\n",
    "        en_out,en_hid,en_c = encoder(data_en, src_len)\n",
    "        max_src_len_batch = max(src_len).item()\n",
    "        max_tar_len_batch = max(tar_len).item()\n",
    "        prev_hiddens = en_hid\n",
    "        prev_cs = en_c\n",
    "        decoder_input = torch.tensor([[SOS_token]]*bss).to(device)\n",
    "        prev_output = torch.zeros((bss, en_out.size(-1))).to(device)\n",
    "        if use_teacher_forcing:\n",
    "            d_out = []\n",
    "            for i in range(max_tar_len_batch):\n",
    "                out_vocab, prev_output,prev_hiddens, prev_cs, attention_score = decoder(decoder_input,prev_output, \\\n",
    "                                                                                        prev_hiddens,prev_cs, en_out,\\\n",
    "                                                                                        src_len)\n",
    "                d_out.append(out_vocab.unsqueeze(-1))\n",
    "                decoder_input = data_de[:,i].view(-1,1)\n",
    "            d_out = torch.cat(d_out,dim=-1)\n",
    "        else:\n",
    "            d_out = []\n",
    "            for i in range(max_tar_len_batch):\n",
    "                out_vocab, prev_output,prev_hiddens, prev_cs, attention_score = decoder(decoder_input,prev_output, \\\n",
    "                                                                                        prev_hiddens,prev_cs, en_out,\\\n",
    "                                                                                        src_len)\n",
    "                d_out.append(out_vocab.unsqueeze(-1))\n",
    "                topv, topi = out_vocab.topk(1)\n",
    "                decoder_input = topi.squeeze().detach().view(-1,1)\n",
    "            d_out = torch.cat(d_out,dim=-1)\n",
    "        return d_out\n",
    "    else:\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        bss = data_en.size(0)\n",
    "        en_out,en_hid,en_c = encoder(data_en, src_len)\n",
    "        max_src_len_batch = max(src_len).item()\n",
    "        max_tar_len_batch = max(tar_len).item()\n",
    "        prev_hiddens = en_hid\n",
    "        prev_cs = en_c\n",
    "        decoder_input = torch.tensor([[SOS_token]]*bss).to(device)\n",
    "        prev_output = torch.zeros((bss, en_out.size(-1))).to(device)\n",
    "        d_out = []\n",
    "        for i in range(max_tar_len_batch):\n",
    "            out_vocab, prev_output,prev_hiddens, prev_cs, attention_score = decoder(decoder_input,prev_output, \\\n",
    "                                                                                    prev_hiddens,prev_cs, en_out,\\\n",
    "                                                                                    src_len)\n",
    "            d_out.append(out_vocab.unsqueeze(-1))\n",
    "            topv, topi = out_vocab.topk(1)\n",
    "            decoder_input = topi.squeeze().detach().view(-1,1)\n",
    "        d_out = torch.cat(d_out,dim=-1)\n",
    "        return d_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_cel_loss(input,target,nll):\n",
    "    input = input.transpose(1,2)\n",
    "    bs, sl = input.size()[:2]\n",
    "    return nll(input.contiguous().view(bs*sl,-1),target.contiguous().view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_new(encoder, decoder, val_dataloader, lang_en,lang_vi,m_type, verbose = False, replace_unk = False):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    pred_corpus = []\n",
    "    true_corpus = []\n",
    "    src_corpus = []\n",
    "    running_loss = 0\n",
    "    running_total = 0\n",
    "    bl = BLEU_SCORE()\n",
    "    attention_scores_for_all_val = []\n",
    "    for data in val_dataloader:\n",
    "        encoder_i = data[0].to(device)\n",
    "        src_len = data[2].to(device)\n",
    "        bs,sl = encoder_i.size()[:2]\n",
    "        en_out,en_hid,en_c = encoder(encoder_i,src_len)\n",
    "        max_src_len_batch = max(src_len).item()\n",
    "        prev_hiddens = en_hid\n",
    "        prev_cs = en_c\n",
    "        decoder_input = torch.tensor([[SOS_token]]*bs).to(device)\n",
    "        prev_output = torch.zeros((bs, en_out.size(-1))).to(device)\n",
    "        d_out = []\n",
    "        attention_scores = []\n",
    "        for i in range(sl*2):\n",
    "            out_vocab, prev_output,prev_hiddens, prev_cs, attention_score = decoder(decoder_input,prev_output, \\\n",
    "                                                                                    prev_hiddens,prev_cs, en_out,\\\n",
    "                                                                                    src_len)\n",
    "            topv, topi = out_vocab.topk(1)\n",
    "#             decoder_input = topi.squeeze().detach().view(-1,1)\n",
    "            d_out.append(topi.item())\n",
    "            decoder_input = topi.squeeze().detach().view(-1,1)\n",
    "            if m_type == 'attention':\n",
    "                attention_scores.append(attention_score.unsqueeze(-1))\n",
    "            if topi.item() == EOS_token:\n",
    "                break\n",
    "        \n",
    "\n",
    "#         true_sent = convert_id_list_2_sent(data[1][0],lang_en)\n",
    "#         true_corpus.append(true_sent)\n",
    "        if replace_unk:\n",
    "            true_sent = convert_id_list_2_sent(data[1][0],lang_en)\n",
    "            true_corpus.append(true_sent)\n",
    "        else:\n",
    "            true_corpus.append(data[-1])\n",
    "        src_sent = convert_id_list_2_sent(data[0][0],lang_vi)\n",
    "        src_corpus.append(src_sent)\n",
    "        pred_sent = convert_id_list_2_sent(d_out,lang_en)\n",
    "        pred_corpus.append(pred_sent)\n",
    "        if m_type == 'attention':\n",
    "            attention_scores = torch.cat(attention_scores, dim = -1)\n",
    "            attention_scores_for_all_val.append(attention_scores)\n",
    "        if verbose:\n",
    "            print(\"True Sentence:\",data[-1])\n",
    "            print(\"Pred Sentence:\", pred_sent)\n",
    "            print('-*'*50)\n",
    "    score = bl.corpus_bleu(pred_corpus,[true_corpus],lowercase=True)[0]\n",
    "    return score, attention_scores_for_all_val, pred_corpus, src_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_beam_search(encoder, decoder, val_dataloader,lang_en,lang_vi,m_type, beam_size, verbose = False,\\\n",
    "                           device = 'cuda', replace_unk = False):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    encoder = encoder.to(device)\n",
    "    decoder = decoder.to(device)\n",
    "    pred_corpus = []\n",
    "    true_corpus = []\n",
    "    src_corpus = []\n",
    "    running_loss = 0\n",
    "    running_total = 0\n",
    "    bl = BLEU_SCORE()\n",
    "    j = 0\n",
    "    attention_scores_for_all_val = []\n",
    "    for data in val_dataloader:\n",
    "#         print(j)\n",
    "        encoder_i = data[0].to(device)\n",
    "        src_len = data[2].to(device)\n",
    "        bs,sl = encoder_i.size()[:2]\n",
    "        en_out,en_hid,en_c = encoder(encoder_i,src_len)\n",
    "        max_src_len_batch = max(src_len).item()\n",
    "        prev_hiddens = en_hid\n",
    "        prev_cs = en_c\n",
    "        decoder_input = torch.tensor([[SOS_token]]*bs).to(device)\n",
    "        prev_output = torch.zeros((bs, en_out.size(-1))).to(device)\n",
    "        list_decoder_input = [None]*beam_size\n",
    "        beam_stop_flags = [False]*beam_size\n",
    "        beam_score = torch.zeros((bs,beam_size)).to(device)\n",
    "        list_d_outs = [[] for _ in range(beam_size)]\n",
    "        select_beam_size = beam_size\n",
    "        attention_scores = [[] for _ in range(beam_size)]\n",
    "        for i in range(sl+20):\n",
    "            if i == 0:\n",
    "                out_vocab, prev_output,prev_hiddens, prev_cs, attention_score = decoder(decoder_input,prev_output, \\\n",
    "                                                                                    prev_hiddens,prev_cs, en_out,\\\n",
    "                                                                                    src_len)\n",
    "                bss, vocab_size = out_vocab.size()\n",
    "                topv, topi = out_vocab.topk(beam_size)\n",
    "                list_prev_output = [prev_output]*beam_size\n",
    "                list_prev_hiddens = [prev_hiddens]*beam_size\n",
    "                list_prev_cs = [prev_cs]*beam_size\n",
    "                for b in range(beam_size):\n",
    "                    beam_score[0][b] = topv[0][b].item()\n",
    "                    list_decoder_input[b] = topi[0][b].squeeze().detach().view(-1,1)\n",
    "                    list_d_outs[b].append(topi[0][b].item())\n",
    "                    if m_type == 'attention':\n",
    "                        attention_scores[b].append(attention_score.unsqueeze(-1))\n",
    "                    if topi[0][b].item() == EOS_token:\n",
    "                        beam_stop_flags[b] = True\n",
    "            else:\n",
    "                beam_out_vocab = [None]*beam_size\n",
    "                temp_out = [None]*beam_size\n",
    "                temp_hid = [None]*beam_size\n",
    "                temp_c = [None]*beam_size\n",
    "                temp_attention_score = [[] for _ in range(beam_size)]\n",
    "                prev_d_outs = copy.deepcopy(list_d_outs)\n",
    "                for b in range(beam_size):\n",
    "                    if not beam_stop_flags[b]:\n",
    "                        beam_out_vocab[b], temp_out[b], temp_hid[b], temp_c[b], temp_attention_score[b] =\\\n",
    "                            decoder(list_decoder_input[b],list_prev_output[b],list_prev_hiddens[b],list_prev_cs[b],\\\n",
    "                                    en_out,src_len)\n",
    "                        beam_out_vocab[b] = beam_out_vocab[b] + beam_score[0][b]\n",
    "                    if beam_stop_flags[b]:\n",
    "                        beam_out_vocab[b] = torch.zeros(bss,vocab_size).fill_(float('-inf')).to(device)\n",
    "                beam_out_vocab = torch.cat(beam_out_vocab,dim = 1)\n",
    "                \n",
    "                topv, topi = beam_out_vocab.topk(beam_size)\n",
    "                id_for_hid = topi//vocab_size\n",
    "                topi_idx = topi%vocab_size\n",
    "                for b in range(beam_size):\n",
    "                    if not beam_stop_flags[b]:\n",
    "                        beam_score[0][b] = topv[0][b].item()\n",
    "                        list_decoder_input[b] = topi_idx[0][b].squeeze().detach().view(-1,1)\n",
    "                        list_d_outs[b] = copy.deepcopy(prev_d_outs[id_for_hid[0][b]])\n",
    "                        list_d_outs[b].append(topi_idx[0][b].item())\n",
    "                        if m_type == 'attention':\n",
    "                            attention_scores[b].append(temp_attention_score[b].unsqueeze(-1))\n",
    "                        if topi_idx[0][b].item() == EOS_token:\n",
    "                            beam_stop_flags[b] = True\n",
    "                        else:\n",
    "                            list_prev_output[b] = temp_out[id_for_hid[0][b]]\n",
    "                            list_prev_hiddens[b] = temp_hid[id_for_hid[0][b]]\n",
    "                            list_prev_cs[b] = temp_c[id_for_hid[0][b]]\n",
    "                if all(beam_stop_flags):\n",
    "                    break\n",
    "        id_max_score = np.argmax(beam_score)\n",
    "        d_out = list_d_outs[id_max_score]\n",
    "        if m_type == 'attention':\n",
    "            att_score = attention_scores[id_max_score]\n",
    "            att_score = torch.cat(att_score, dim = -1)\n",
    "            attention_scores_for_all_val.append(att_score)\n",
    "        if replace_unk:\n",
    "            true_sent = convert_id_list_2_sent(data[1][0],lang_en)\n",
    "            true_corpus.append(true_sent)\n",
    "        else:\n",
    "            true_corpus.append(data[-1])\n",
    "        pred_sent = convert_id_list_2_sent(d_out,lang_en)\n",
    "        pred_corpus.append(pred_sent)\n",
    "        src_sent = convert_id_list_2_sent(data[0][0], lang_vi)\n",
    "        src_corpus.append(src_sent)\n",
    "        if verbose:\n",
    "            print(\"True Sentence:\",data[-1])\n",
    "            print(\"Pred Sentence:\", pred_sent)\n",
    "            print('-*'*50)\n",
    "    score = bl.corpus_bleu(pred_corpus,[true_corpus],lowercase=True)[0]\n",
    "    return score, attention_scores_for_all_val, pred_corpus, src_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(encoder_optimizer,decoder_optimizer, encoder, decoder, loss_fun,m_type, dataloader, en_lang,vi_lang,\\\n",
    "                num_epochs=60, val_every = 1, train_bleu_every = 10,clip = 0.1, rm = 0.8, enc_scheduler = None,\\\n",
    "               dec_scheduler = None, enc_dec_fn = encode_decode, val_fn = validation_new):\n",
    "    best_score = 0\n",
    "    best_bleu = 0\n",
    "    loss_hist = {'train': [], 'val_train': []}\n",
    "    bleu_hist = {'train': [], 'validate': []}\n",
    "    best_encoder_wts = None\n",
    "    best_decoder_wts = None\n",
    "    phases = ['train','val_train']\n",
    "    for epoch in range(num_epochs):\n",
    "        for ex, phase in enumerate(phases):\n",
    "            start = time.time()\n",
    "            total = 0\n",
    "            top1_correct = 0\n",
    "            running_loss = 0\n",
    "            running_total = 0\n",
    "            if phase == 'train':\n",
    "                encoder.train()\n",
    "                decoder.train()\n",
    "            else:\n",
    "                encoder.eval()\n",
    "                decoder.eval()\n",
    "            for data in dataloader[phase]:\n",
    "                encoder_optimizer.zero_grad()\n",
    "                decoder_optimizer.zero_grad()\n",
    "\n",
    "                encoder_i = data[0].to(device)\n",
    "                decoder_i = data[1].to(device)\n",
    "                src_len = data[2].to(device)\n",
    "                tar_len = data[3].to(device)\n",
    "                if phase == 'val_train':  \n",
    "                    out = enc_dec_fn(encoder,decoder,encoder_i,decoder_i,src_len,tar_len,rand_num=rm,val = True )\n",
    "                else:\n",
    "                    out = enc_dec_fn(encoder,decoder,encoder_i,decoder_i,src_len,tar_len,rand_num=rm,val = False )\n",
    "                N = decoder_i.size(0)\n",
    "                loss = loss_fun(out.float(), decoder_i.long())\n",
    "                running_loss += loss.item() * N\n",
    "                \n",
    "                total += N\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "                    torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "                    encoder_optimizer.step()\n",
    "                    decoder_optimizer.step()\n",
    "                    \n",
    "            epoch_loss = running_loss / total \n",
    "            loss_hist[phase].append(epoch_loss)\n",
    "            print(\"epoch {} {} loss = {}, time = {}\".format(epoch, phase, epoch_loss,\n",
    "                                                                           time.time() - start))\n",
    "        if (enc_scheduler is not None) and (dec_scheduler is not None):\n",
    "            enc_scheduler.step(loss_hist['train'][-1])\n",
    "            dec_scheduler.step(loss_hist['train'][-1])\n",
    "        if epoch%val_every == 0:\n",
    "            val_bleu_score, _,_,_ = val_fn(encoder,decoder,dataloader['validate'],\\\n",
    "                                                                               en_lang,vi_lang,m_type, verbose=False, \\\n",
    "                                                                               replace_unk=True)\n",
    "            bleu_hist['validate'].append(val_bleu_score)\n",
    "            print(\"validation BLEU = \", val_bleu_score)\n",
    "            if val_bleu_score > best_bleu:\n",
    "                best_bleu = val_bleu_score\n",
    "                best_encoder_wts = encoder.state_dict()\n",
    "                best_decoder_wts = decoder.state_dict()\n",
    "        print('='*50)\n",
    "    encoder.load_state_dict(best_encoder_wts)\n",
    "    decoder.load_state_dict(best_decoder_wts)\n",
    "    print(\"Training completed. Best BLEU is {}\".format(best_bleu))\n",
    "    return encoder,decoder,loss_hist,bleu_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.sans-serif'] = ['SimHei']\n",
    "mpl.rcParams['font.serif'] = ['SimHei']\n",
    "seaborn.set_style(\"darkgrid\",{\"Droid Sans Fallback\":['simhei', 'Arial']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw(att_mat, x, y):\n",
    "    x = x.split(' ')\n",
    "    y = y.split(' ')\n",
    "    x += ['EOS']\n",
    "    y += ['EOS']\n",
    "    att_mat = att_mat.data.cpu().squeeze(1).numpy()\n",
    "    seaborn.heatmap(att_mat, \n",
    "                    xticklabels=x, square=True, yticklabels=y, vmin=0.0, vmax=1.0, \n",
    "                    cbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Without attention (Training and Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ark576/.conda/envs/nlp/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "encoder_wo_att = EncoderRNN(vi_lang.n_words,512,512, 1).to(device)\n",
    "decoder_wo_att = AttentionDecoderRNN(en_lang.n_words,512,1024,n_layers=1, attention = False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_optimizer = optim.Adam(encoder_wo_att.parameters(), lr = 5e-3)\n",
    "# decoder_optimizer = optim.Adam(decoder_wo_att.parameters(), lr = 5e-3)\n",
    "encoder_optimizer = optim.SGD(encoder_wo_att.parameters(), lr=0.25,nesterov=True, momentum = 0.99)\n",
    "enc_scheduler = ReduceLROnPlateau(encoder_optimizer, min_lr=1e-4,factor = 0.5,  patience=0)\n",
    "decoder_optimizer = optim.SGD(decoder_wo_att.parameters(), lr=0.25,nesterov=True, momentum = 0.99)\n",
    "dec_scheduler = ReduceLROnPlateau(decoder_optimizer, min_lr=1e-4,factor = 0.5,  patience=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 train loss = 4.471057017540522, time = 511.3138496875763\n",
      "epoch 0 val_train loss = 6.035842107297494, time = 2.741291046142578\n",
      "validation BLEU =  4.108723532227416\n",
      "==================================================\n",
      "epoch 1 train loss = 3.752824043531392, time = 511.75773429870605\n",
      "epoch 1 val_train loss = 5.889547539058168, time = 2.7434656620025635\n",
      "validation BLEU =  4.9344798773584\n",
      "==================================================\n",
      "epoch 2 train loss = 3.4114095061071623, time = 511.4242386817932\n",
      "epoch 2 val_train loss = 5.707443820941335, time = 2.741786003112793\n",
      "validation BLEU =  4.952099453454834\n",
      "==================================================\n",
      "epoch 3 train loss = 3.1696411752518037, time = 511.6245563030243\n",
      "epoch 3 val_train loss = 5.475485320346965, time = 2.7678661346435547\n",
      "validation BLEU =  5.168079969243536\n",
      "==================================================\n",
      "epoch 4 train loss = 2.9475005343048397, time = 511.8350863456726\n",
      "epoch 4 val_train loss = 5.993232623259728, time = 2.716867446899414\n",
      "validation BLEU =  5.5870477939277405\n",
      "==================================================\n",
      "epoch 5 train loss = 2.8478485474557553, time = 511.7394149303436\n",
      "epoch 5 val_train loss = 5.812662578907675, time = 2.7401883602142334\n",
      "validation BLEU =  5.540911479753939\n",
      "==================================================\n",
      "epoch 6 train loss = 2.7593742847456935, time = 511.52871775627136\n",
      "epoch 6 val_train loss = 6.10899664250082, time = 2.7540173530578613\n",
      "validation BLEU =  5.694798846924077\n",
      "==================================================\n",
      "epoch 7 train loss = 2.692884615572701, time = 511.2188949584961\n",
      "epoch 7 val_train loss = 6.452963110024244, time = 2.7275946140289307\n",
      "validation BLEU =  6.15421415747277\n",
      "==================================================\n",
      "epoch 8 train loss = 2.605451447584028, time = 512.0406992435455\n",
      "epoch 8 val_train loss = 6.335530574389437, time = 2.737623929977417\n",
      "validation BLEU =  5.349165122252841\n",
      "==================================================\n",
      "epoch 9 train loss = 2.6147061753332204, time = 511.6643216609955\n",
      "epoch 9 val_train loss = 6.228603647334342, time = 2.7569849491119385\n",
      "validation BLEU =  5.744914044469885\n",
      "==================================================\n",
      "epoch 10 train loss = 2.279731397201653, time = 511.45383954048157\n",
      "epoch 10 val_train loss = 6.524412159664021, time = 2.7470247745513916\n",
      "validation BLEU =  5.89862133143838\n",
      "==================================================\n",
      "epoch 11 train loss = 2.240439302066442, time = 511.30108880996704\n",
      "epoch 11 val_train loss = 6.62755900151346, time = 2.7607762813568115\n",
      "validation BLEU =  6.1488882974256525\n",
      "==================================================\n",
      "epoch 12 train loss = 2.158705530567075, time = 512.0522847175598\n",
      "epoch 12 val_train loss = 6.813991913660091, time = 2.73470401763916\n",
      "validation BLEU =  6.112182947198608\n",
      "==================================================\n",
      "epoch 13 train loss = 2.1191345612327868, time = 511.6892557144165\n",
      "epoch 13 val_train loss = 6.643052001856855, time = 2.7302114963531494\n",
      "validation BLEU =  5.854504632592733\n",
      "==================================================\n",
      "epoch 14 train loss = 2.124697576447809, time = 512.126608133316\n",
      "epoch 14 val_train loss = 6.871155984018127, time = 2.731900691986084\n",
      "validation BLEU =  6.491336515231129\n",
      "==================================================\n",
      "Training completed. Best BLEU is 6.491336515231129\n"
     ]
    }
   ],
   "source": [
    "encoder_wo_att, decoder_wo_att, loss_hist, acc_hist = train_model(encoder_optimizer, decoder_optimizer, encoder_wo_att, decoder_wo_att, criterion,\\\n",
    "                                            \"non_attention\", dataloader,en_lang,vi_lang, num_epochs = 15, rm = 0.95,\\\n",
    "                                           enc_scheduler = enc_scheduler, dec_scheduler = dec_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder_wo_att.state_dict(), 'lstm_wo_att_enc_1_layer.pth')\n",
    "torch.save(decoder_wo_att.state_dict(), 'lstm_wo_att_dec_1_layer.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation using BLEU with beam size 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_3_no_unk, att_score_wo, pred_wo, src_wo = validation_beam_search(encoder_wo_att, decoder_wo_att,dataloader['validate'],en_lang,\\\n",
    "                                                                      vi_lang, 'no_attention',3,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.7921625726904304"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_3_no_unk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_3_unk, att_score_wo, pred_wo, src_wo = validation_beam_search(encoder_wo_att, decoder_wo_att,dataloader['validate'],en_lang,\\\n",
    "                                                                      vi_lang, 'no_attention',3,verbose=False,\\\n",
    "                                                                  replace_unk = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.136687650346082"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_3_unk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. With Attention (Training and Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ark576/.conda/envs/nlp/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "encoder_w_att = EncoderRNN(vi_lang.n_words,512,512, 1).to(device)\n",
    "decoder_w_att = AttentionDecoderRNN(en_lang.n_words,512,1024,n_layers=1, attention = True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_optimizer = optim.Adam(encoder_wo_att.parameters(), lr = 5e-3)\n",
    "# decoder_optimizer = optim.Adam(decoder_wo_att.parameters(), lr = 5e-3)\n",
    "encoder_optimizer = optim.SGD(encoder_w_att.parameters(), lr=0.25,nesterov=True, momentum = 0.99)\n",
    "enc_scheduler = ReduceLROnPlateau(encoder_optimizer, min_lr=1e-4,factor = 0.5,  patience=0)\n",
    "decoder_optimizer = optim.SGD(decoder_w_att.parameters(), lr=0.25,nesterov=True, momentum = 0.99)\n",
    "dec_scheduler = ReduceLROnPlateau(decoder_optimizer, min_lr=1e-4,factor = 0.5,  patience=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 train loss = 4.471057017540522, time = 511.3138496875763\n",
      "epoch 0 val_train loss = 6.035842107297494, time = 2.741291046142578\n",
      "validation BLEU =  4.108723532227416\n",
      "==================================================\n",
      "epoch 1 train loss = 3.752824043531392, time = 511.75773429870605\n",
      "epoch 1 val_train loss = 5.889547539058168, time = 2.7434656620025635\n",
      "validation BLEU =  4.9344798773584\n",
      "==================================================\n",
      "epoch 2 train loss = 3.4114095061071623, time = 511.4242386817932\n",
      "epoch 2 val_train loss = 5.707443820941335, time = 2.741786003112793\n",
      "validation BLEU =  4.952099453454834\n",
      "==================================================\n",
      "epoch 3 train loss = 3.1696411752518037, time = 511.6245563030243\n",
      "epoch 3 val_train loss = 5.475485320346965, time = 2.7678661346435547\n",
      "validation BLEU =  5.168079969243536\n",
      "==================================================\n",
      "epoch 4 train loss = 2.9475005343048397, time = 511.8350863456726\n",
      "epoch 4 val_train loss = 5.993232623259728, time = 2.716867446899414\n",
      "validation BLEU =  5.5870477939277405\n",
      "==================================================\n",
      "epoch 5 train loss = 2.8478485474557553, time = 511.7394149303436\n",
      "epoch 5 val_train loss = 5.812662578907675, time = 2.7401883602142334\n",
      "validation BLEU =  5.540911479753939\n",
      "==================================================\n",
      "epoch 6 train loss = 2.7593742847456935, time = 511.52871775627136\n",
      "epoch 6 val_train loss = 6.10899664250082, time = 2.7540173530578613\n",
      "validation BLEU =  5.694798846924077\n",
      "==================================================\n",
      "epoch 7 train loss = 2.692884615572701, time = 511.2188949584961\n",
      "epoch 7 val_train loss = 6.452963110024244, time = 2.7275946140289307\n",
      "validation BLEU =  6.15421415747277\n",
      "==================================================\n",
      "epoch 8 train loss = 2.605451447584028, time = 512.0406992435455\n",
      "epoch 8 val_train loss = 6.335530574389437, time = 2.737623929977417\n",
      "validation BLEU =  5.349165122252841\n",
      "==================================================\n",
      "epoch 9 train loss = 2.6147061753332204, time = 511.6643216609955\n",
      "epoch 9 val_train loss = 6.228603647334342, time = 2.7569849491119385\n",
      "validation BLEU =  5.744914044469885\n",
      "==================================================\n",
      "epoch 10 train loss = 2.279731397201653, time = 511.45383954048157\n",
      "epoch 10 val_train loss = 6.524412159664021, time = 2.7470247745513916\n",
      "validation BLEU =  5.89862133143838\n",
      "==================================================\n",
      "epoch 11 train loss = 2.240439302066442, time = 511.30108880996704\n",
      "epoch 11 val_train loss = 6.62755900151346, time = 2.7607762813568115\n",
      "validation BLEU =  6.1488882974256525\n",
      "==================================================\n",
      "epoch 12 train loss = 2.158705530567075, time = 512.0522847175598\n",
      "epoch 12 val_train loss = 6.813991913660091, time = 2.73470401763916\n",
      "validation BLEU =  6.112182947198608\n",
      "==================================================\n",
      "epoch 13 train loss = 2.1191345612327868, time = 511.6892557144165\n",
      "epoch 13 val_train loss = 6.643052001856855, time = 2.7302114963531494\n",
      "validation BLEU =  5.854504632592733\n",
      "==================================================\n",
      "epoch 14 train loss = 2.124697576447809, time = 512.126608133316\n",
      "epoch 14 val_train loss = 6.871155984018127, time = 2.731900691986084\n",
      "validation BLEU =  6.491336515231129\n",
      "==================================================\n",
      "Training completed. Best BLEU is 6.491336515231129\n"
     ]
    }
   ],
   "source": [
    "encoder_w_att, decoder_w_att, loss_hist, acc_hist = train_model(encoder_optimizer, decoder_optimizer, encoder_w_att, decoder_w_att, criterion,\\\n",
    "                                            \"non_attention\", dataloader,en_lang,vi_lang, num_epochs = 15, rm = 0.95,\\\n",
    "                                           enc_scheduler = enc_scheduler, dec_scheduler = dec_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder_w_att.state_dict(), 'lstm_w_att_enc_1_layer.pth')\n",
    "torch.save(decoder_w_att.state_dict(), 'lstm_w_att_dec_1_layer.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation using BLEU with beam size 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_10_no_unk, att_score_w, pred_w, src_w = validation_beam_search(encoder_w_att, decoder_w_att,dataloader['validate'],en_lang,\\\n",
    "                                                                      vi_lang, 'no_attention',10,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_10_unk, att_score_w, pred_w, src_wo = validation_beam_search(encoder_wo_att, decoder_wo_att,dataloader['validate'],en_lang,\\\n",
    "                                                                      vi_lang, 'no_attention',10,verbose=False,\\\n",
    "                                                                   replace_unk = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plotting Attention maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 5\n",
    "draw(attention_scores[n],pred_corpus[n],src_corpus[n])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
