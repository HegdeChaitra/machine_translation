{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pdb\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_train = pd.read_csv(\"./iwslt-vi-en/train.tok.en\",sep=\"\\t\",header=None)\n",
    "viet_train = pd.read_csv(\"./iwslt-vi-en/train.tok.vi\",sep=\"\\t\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_val = pd.read_csv(\"./iwslt-vi-en/dev.tok.en\",sep=\"\\t\",header=None)\n",
    "viet_val = pd.read_csv(\"./iwslt-vi-en/dev.tok.vi\",sep=\"\\t\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_test = pd.read_csv(\"./iwslt-vi-en/test.tok.en\",sep=\"\\t\",header=None)\n",
    "viet_test = pd.read_csv(\"./iwslt-vi-en/test.tok.vi\",sep=\"\\t\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(df):\n",
    "    df['tokenized'] = df[0].apply(lambda x:x.split( ))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_train = split(eng_train)\n",
    "viet_train = split(viet_train)\n",
    "\n",
    "eng_test = split(eng_test)\n",
    "viet_test = split(viet_test)\n",
    "\n",
    "eng_val = split(eng_val)\n",
    "viet_val = split(viet_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(133168, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rachel Pike : The science behind a climate hea...</td>\n",
       "      <td>[Rachel, Pike, :, The, science, behind, a, cli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In 4 minutes , atmospheric chemist Rachel Pike...</td>\n",
       "      <td>[In, 4, minutes, ,, atmospheric, chemist, Rach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I &amp;apos;d like to talk to you today about the ...</td>\n",
       "      <td>[I, &amp;apos;d, like, to, talk, to, you, today, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Headlines that look like this when they have t...</td>\n",
       "      <td>[Headlines, that, look, like, this, when, they...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>They are both two branches of the same field o...</td>\n",
       "      <td>[They, are, both, two, branches, of, the, same...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  Rachel Pike : The science behind a climate hea...   \n",
       "1  In 4 minutes , atmospheric chemist Rachel Pike...   \n",
       "2  I &apos;d like to talk to you today about the ...   \n",
       "3  Headlines that look like this when they have t...   \n",
       "4  They are both two branches of the same field o...   \n",
       "\n",
       "                                           tokenized  \n",
       "0  [Rachel, Pike, :, The, science, behind, a, cli...  \n",
       "1  [In, 4, minutes, ,, atmospheric, chemist, Rach...  \n",
       "2  [I, &apos;d, like, to, talk, to, you, today, a...  \n",
       "3  [Headlines, that, look, like, this, when, they...  \n",
       "4  [They, are, both, two, branches, of, the, same...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(eng_train.shape)\n",
    "eng_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132449, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Khoa_học đằng_sau một tiêu_đề về khí_hậu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trong 4 phút , chuyên_gia hoá_học khí_quyển Ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tôi muốn cho các bạn biết về sự to_lớn của nhữ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Có những dòng trông như thế_này khi bàn về biế...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cả hai đều là một nhánh của cùng một lĩnh_vực ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0           Khoa_học đằng_sau một tiêu_đề về khí_hậu\n",
       "1  Trong 4 phút , chuyên_gia hoá_học khí_quyển Ra...\n",
       "2  Tôi muốn cho các bạn biết về sự to_lớn của nhữ...\n",
       "3  Có những dòng trông như thế_này khi bàn về biế...\n",
       "4  Cả hai đều là một nhánh của cùng một lĩnh_vực ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(viet_train.shape)\n",
    "viet_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In 4 minutes , atmospheric chemist Rachel Pike provides a glimpse of the massive scientific effort behind the bold headlines on climate change , with her team -- one of thousands who contributed -- taking a risky flight over the rainforest in pursuit of data on a key molecule .'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng.iloc[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('And', 'So', 'So', 'And', 'For', 'Trophic', 'When', 'I', 'The', 'Well'),\n",
       "  ('you',\n",
       "   'what',\n",
       "   'this',\n",
       "   'then',\n",
       "   'every',\n",
       "   'cascades',\n",
       "   'you',\n",
       "   'think',\n",
       "   'box',\n",
       "   ','),\n",
       "  ('can', 'if', 'is', 'of', '100', 'tell', 'ask', 'it', 'jellyfish', 'less'),\n",
       "  ('see', 'we', 'what', 'course', 'girls', 'us', ',', 'comes', ',', 'than'),\n",
       "  ('that', 'could', 'I', 'I', 'with', 'that', '&quot;', 'when', 'the', 'one'),\n",
       "  ('the',\n",
       "   'put',\n",
       "   'mean',\n",
       "   'have',\n",
       "   'an',\n",
       "   'the',\n",
       "   'Where',\n",
       "   'we',\n",
       "   'deadliest',\n",
       "   'comes'),\n",
       "  ('signature',\n",
       "   'all',\n",
       "   'with',\n",
       "   'to',\n",
       "   'emotional',\n",
       "   'natural',\n",
       "   'are',\n",
       "   'start',\n",
       "   'venom',\n",
       "   'from'),\n",
       "  ('of',\n",
       "   'that',\n",
       "   'a',\n",
       "   'milk',\n",
       "   'disturbance',\n",
       "   'world',\n",
       "   'the',\n",
       "   'to',\n",
       "   'in',\n",
       "   'the'),\n",
       "  ('a',\n",
       "   'together',\n",
       "   'failing',\n",
       "   'the',\n",
       "   'diagnosed',\n",
       "   'is',\n",
       "   'Indian',\n",
       "   'value',\n",
       "   'all',\n",
       "   'United'),\n",
       "  ('stream',\n",
       "   'to',\n",
       "   'governance',\n",
       "   'goats',\n",
       "   ',',\n",
       "   'even',\n",
       "   'Googles',\n",
       "   'the',\n",
       "   'of',\n",
       "   'States'),\n",
       "  ('is', 'shoot', 'structure', 'and', 'we', 'more', ',', 'gift', 'the', '.')],\n",
       " [('Nói',\n",
       "   'Vậy_nên',\n",
       "   'Hãy',\n",
       "   'Trên',\n",
       "   'Bé',\n",
       "   'Và',\n",
       "   'Nhiều',\n",
       "   'Nói',\n",
       "   'Đến',\n",
       "   'Những'),\n",
       "  ('về', 'điều', 'xem', 'hết', 'gái', 'để', 'người', 'theo', 'đây', 'phụ_nữ'),\n",
       "  ('mức_độ',\n",
       "   'duy_nhất',\n",
       "   'hệ_thống',\n",
       "   ',',\n",
       "   ':',\n",
       "   'đưa',\n",
       "   'đã',\n",
       "   'cách',\n",
       "   ',',\n",
       "   'mang'),\n",
       "  ('phổ_biến',\n",
       "   'mà',\n",
       "   'y_tế',\n",
       "   'chúng_tôi',\n",
       "   '581',\n",
       "   'cho',\n",
       "   'tìm',\n",
       "   'khác',\n",
       "   'có',\n",
       "   'thai'),\n",
       "  (',', 'tôi', 'ở', 'cũng', '.', 'bạn', 'việc', ',', 'một', 'dương_tính'),\n",
       "  ('chúng_tôi',\n",
       "   'đã',\n",
       "   'nhiều',\n",
       "   'tích_hợp',\n",
       "   'Ngôi_nhà',\n",
       "   'ý_niệm',\n",
       "   'ở',\n",
       "   'dân_cư',\n",
       "   'vấn_đề',\n",
       "   'với'),\n",
       "  ('thuộc',\n",
       "   'xoay',\n",
       "   'nước',\n",
       "   'một',\n",
       "   'này',\n",
       "   'ngắn',\n",
       "   'những',\n",
       "   'ở',\n",
       "   'Chúng_ta',\n",
       "   'HIV'),\n",
       "  ('trong', 'sở', '.', 'mạng', 'là', 'là', 'khu', 'đây', 'có', 'phải')]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataloader['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (133168, 3) (132449, 3)\n",
      "val (1268, 3) (1266, 3)\n",
      "test (1553, 3) (1546, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"train\",eng_train.shape,viet_train.shape)\n",
    "print(\"val\",eng_val.shape,viet_val.shape)\n",
    "print(\"test\",eng_test.shape,viet_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_en_toks = []\n",
    "for i,row in eng_train.iterrows():\n",
    "    all_en_toks+=row['tokenized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Rachel',\n",
       " 'Pike',\n",
       " ':',\n",
       " 'The',\n",
       " 'science',\n",
       " 'behind',\n",
       " 'a',\n",
       " 'climate',\n",
       " 'headline',\n",
       " 'In',\n",
       " '4',\n",
       " 'minutes',\n",
       " ',',\n",
       " 'atmospheric',\n",
       " 'chemist',\n",
       " 'Rachel',\n",
       " 'Pike',\n",
       " 'provides',\n",
       " 'a',\n",
       " 'glimpse',\n",
       " 'of',\n",
       " 'the',\n",
       " 'massive',\n",
       " 'scientific',\n",
       " 'effort',\n",
       " 'behind',\n",
       " 'the',\n",
       " 'bold',\n",
       " 'headlines',\n",
       " 'on',\n",
       " 'climate',\n",
       " 'change',\n",
       " ',',\n",
       " 'with',\n",
       " 'her',\n",
       " 'team',\n",
       " '--',\n",
       " 'one',\n",
       " 'of',\n",
       " 'thousands',\n",
       " 'who',\n",
       " 'contributed',\n",
       " '--',\n",
       " 'taking',\n",
       " 'a',\n",
       " 'risky',\n",
       " 'flight',\n",
       " 'over',\n",
       " 'the',\n",
       " 'rainforest',\n",
       " 'in',\n",
       " 'pursuit',\n",
       " 'of',\n",
       " 'data',\n",
       " 'on',\n",
       " 'a',\n",
       " 'key',\n",
       " 'molecule',\n",
       " '.',\n",
       " 'I',\n",
       " '&apos;d',\n",
       " 'like',\n",
       " 'to',\n",
       " 'talk',\n",
       " 'to',\n",
       " 'you',\n",
       " 'today',\n",
       " 'about',\n",
       " 'the',\n",
       " 'scale',\n",
       " 'of',\n",
       " 'the',\n",
       " 'scientific',\n",
       " 'effort',\n",
       " 'that',\n",
       " 'goes',\n",
       " 'into',\n",
       " 'making',\n",
       " 'the',\n",
       " 'headlines',\n",
       " 'you',\n",
       " 'see',\n",
       " 'in',\n",
       " 'the',\n",
       " 'paper',\n",
       " '.',\n",
       " 'Headlines',\n",
       " 'that',\n",
       " 'look',\n",
       " 'like',\n",
       " 'this',\n",
       " 'when',\n",
       " 'they',\n",
       " 'have',\n",
       " 'to',\n",
       " 'do',\n",
       " 'with',\n",
       " 'climate',\n",
       " 'change',\n",
       " ',',\n",
       " 'and',\n",
       " 'headlines',\n",
       " 'that',\n",
       " 'look',\n",
       " 'like',\n",
       " 'this',\n",
       " 'when',\n",
       " 'they',\n",
       " 'have',\n",
       " 'to',\n",
       " 'do',\n",
       " 'with',\n",
       " 'air',\n",
       " 'quality',\n",
       " 'or',\n",
       " 'smog',\n",
       " '.',\n",
       " 'They',\n",
       " 'are',\n",
       " 'both',\n",
       " 'two',\n",
       " 'branches',\n",
       " 'of',\n",
       " 'the',\n",
       " 'same',\n",
       " 'field',\n",
       " 'of',\n",
       " 'atmospheric',\n",
       " 'science',\n",
       " '.',\n",
       " 'Recently',\n",
       " 'the',\n",
       " 'headlines',\n",
       " 'looked',\n",
       " 'like',\n",
       " 'this',\n",
       " 'when',\n",
       " 'the',\n",
       " 'Intergovernmental',\n",
       " 'Panel',\n",
       " 'on',\n",
       " 'Climate',\n",
       " 'Change',\n",
       " ',',\n",
       " 'or',\n",
       " 'IPCC',\n",
       " ',',\n",
       " 'put',\n",
       " 'out',\n",
       " 'their',\n",
       " 'report',\n",
       " 'on',\n",
       " 'the',\n",
       " 'state',\n",
       " 'of',\n",
       " 'understanding',\n",
       " 'of',\n",
       " 'the',\n",
       " 'atmospheric',\n",
       " 'system',\n",
       " '.',\n",
       " 'That',\n",
       " 'report',\n",
       " 'was',\n",
       " 'written',\n",
       " 'by',\n",
       " '620',\n",
       " 'scientists',\n",
       " 'from',\n",
       " '40',\n",
       " 'countries',\n",
       " '.',\n",
       " 'They',\n",
       " 'wrote',\n",
       " 'almost',\n",
       " 'a',\n",
       " 'thousand',\n",
       " 'pages',\n",
       " 'on',\n",
       " 'the',\n",
       " 'topic',\n",
       " '.',\n",
       " 'And',\n",
       " 'all',\n",
       " 'of',\n",
       " 'those',\n",
       " 'pages',\n",
       " 'were',\n",
       " 'reviewed',\n",
       " 'by',\n",
       " 'another',\n",
       " '400-plus',\n",
       " 'scientists',\n",
       " 'and',\n",
       " 'reviewers',\n",
       " ',',\n",
       " 'from',\n",
       " '113',\n",
       " 'countries',\n",
       " '.',\n",
       " 'It',\n",
       " '&apos;s',\n",
       " 'a',\n",
       " 'big',\n",
       " 'community',\n",
       " '.',\n",
       " 'It',\n",
       " '&apos;s',\n",
       " 'such',\n",
       " 'a',\n",
       " 'big',\n",
       " 'community',\n",
       " ',',\n",
       " 'in',\n",
       " 'fact',\n",
       " ',',\n",
       " 'that',\n",
       " 'our',\n",
       " 'annual',\n",
       " 'gathering',\n",
       " 'is',\n",
       " 'the',\n",
       " 'largest',\n",
       " 'scientific',\n",
       " 'meeting',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " '.',\n",
       " 'Over',\n",
       " '15,000',\n",
       " 'scientists',\n",
       " 'go',\n",
       " 'to',\n",
       " 'San',\n",
       " 'Francisco',\n",
       " 'every',\n",
       " 'year',\n",
       " 'for',\n",
       " 'that',\n",
       " '.',\n",
       " 'And',\n",
       " 'every',\n",
       " 'one',\n",
       " 'of',\n",
       " 'those',\n",
       " 'scientists',\n",
       " 'is',\n",
       " 'in',\n",
       " 'a',\n",
       " 'research',\n",
       " 'group',\n",
       " ',',\n",
       " 'and',\n",
       " 'every',\n",
       " 'research',\n",
       " 'group',\n",
       " 'studies',\n",
       " 'a',\n",
       " 'wide',\n",
       " 'variety',\n",
       " 'of',\n",
       " 'topics',\n",
       " '.',\n",
       " 'For',\n",
       " 'us',\n",
       " 'at',\n",
       " 'Cambridge',\n",
       " ',',\n",
       " 'it',\n",
       " '&apos;s',\n",
       " 'as',\n",
       " 'varied',\n",
       " 'as',\n",
       " 'the',\n",
       " 'El',\n",
       " 'Niño',\n",
       " 'oscillation',\n",
       " ',',\n",
       " 'which',\n",
       " 'affects',\n",
       " 'weather',\n",
       " 'and',\n",
       " 'climate',\n",
       " ',',\n",
       " 'to',\n",
       " 'the',\n",
       " 'assimilation',\n",
       " 'of',\n",
       " 'satellite',\n",
       " 'data',\n",
       " ',',\n",
       " 'to',\n",
       " 'emissions',\n",
       " 'from',\n",
       " 'crops',\n",
       " 'that',\n",
       " 'produce',\n",
       " 'biofuels',\n",
       " ',',\n",
       " 'which',\n",
       " 'is',\n",
       " 'what',\n",
       " 'I',\n",
       " 'happen',\n",
       " 'to',\n",
       " 'study',\n",
       " '.',\n",
       " 'And',\n",
       " 'in',\n",
       " 'each',\n",
       " 'one',\n",
       " 'of',\n",
       " 'these',\n",
       " 'research',\n",
       " 'areas',\n",
       " ',',\n",
       " 'of',\n",
       " 'which',\n",
       " 'there',\n",
       " 'are',\n",
       " 'even',\n",
       " 'more',\n",
       " ',',\n",
       " 'there',\n",
       " 'are',\n",
       " 'PhD',\n",
       " 'students',\n",
       " ',',\n",
       " 'like',\n",
       " 'me',\n",
       " ',',\n",
       " 'and',\n",
       " 'we',\n",
       " 'study',\n",
       " 'incredibly',\n",
       " 'narrow',\n",
       " 'topics',\n",
       " ',',\n",
       " 'things',\n",
       " 'as',\n",
       " 'narrow',\n",
       " 'as',\n",
       " 'a',\n",
       " 'few',\n",
       " 'processes',\n",
       " 'or',\n",
       " 'a',\n",
       " 'few',\n",
       " 'molecules',\n",
       " '.',\n",
       " 'And',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'molecules',\n",
       " 'I',\n",
       " 'study',\n",
       " 'is',\n",
       " 'called',\n",
       " 'isoprene',\n",
       " ',',\n",
       " 'which',\n",
       " 'is',\n",
       " 'here',\n",
       " '.',\n",
       " 'It',\n",
       " '&apos;s',\n",
       " 'a',\n",
       " 'small',\n",
       " 'organic',\n",
       " 'molecule',\n",
       " '.',\n",
       " 'You',\n",
       " '&apos;ve',\n",
       " 'probably',\n",
       " 'never',\n",
       " 'heard',\n",
       " 'of',\n",
       " 'it',\n",
       " '.',\n",
       " 'The',\n",
       " 'weight',\n",
       " 'of',\n",
       " 'a',\n",
       " 'paper',\n",
       " 'clip',\n",
       " 'is',\n",
       " 'approximately',\n",
       " 'equal',\n",
       " 'to',\n",
       " '900',\n",
       " 'zeta-illion',\n",
       " '--',\n",
       " '10',\n",
       " 'to',\n",
       " 'the',\n",
       " '21st',\n",
       " '--',\n",
       " 'molecules',\n",
       " 'of',\n",
       " 'isoprene',\n",
       " '.',\n",
       " 'But',\n",
       " 'despite',\n",
       " 'its',\n",
       " 'very',\n",
       " 'small',\n",
       " 'weight',\n",
       " ',',\n",
       " 'enough',\n",
       " 'of',\n",
       " 'it',\n",
       " 'is',\n",
       " 'emitted',\n",
       " 'into',\n",
       " 'the',\n",
       " 'atmosphere',\n",
       " 'every',\n",
       " 'year',\n",
       " 'to',\n",
       " 'equal',\n",
       " 'the',\n",
       " 'weight',\n",
       " 'of',\n",
       " 'all',\n",
       " 'the',\n",
       " 'people',\n",
       " 'on',\n",
       " 'the',\n",
       " 'planet',\n",
       " '.',\n",
       " 'It',\n",
       " '&apos;s',\n",
       " 'a',\n",
       " 'huge',\n",
       " 'amount',\n",
       " 'of',\n",
       " 'stuff',\n",
       " '.',\n",
       " 'It',\n",
       " '&apos;s',\n",
       " 'equal',\n",
       " 'to',\n",
       " 'the',\n",
       " 'weight',\n",
       " 'of',\n",
       " 'methane',\n",
       " '.',\n",
       " 'And',\n",
       " 'because',\n",
       " 'it',\n",
       " '&apos;s',\n",
       " 'so',\n",
       " 'much',\n",
       " 'stuff',\n",
       " ',',\n",
       " 'it',\n",
       " '&apos;s',\n",
       " 'really',\n",
       " 'important',\n",
       " 'for',\n",
       " 'the',\n",
       " 'atmospheric',\n",
       " 'system',\n",
       " '.',\n",
       " 'Because',\n",
       " 'it',\n",
       " '&apos;s',\n",
       " 'important',\n",
       " 'to',\n",
       " 'the',\n",
       " 'atmospheric',\n",
       " 'system',\n",
       " ',',\n",
       " 'we',\n",
       " 'go',\n",
       " 'to',\n",
       " 'all',\n",
       " 'lengths',\n",
       " 'to',\n",
       " 'study',\n",
       " 'this',\n",
       " 'thing',\n",
       " '.',\n",
       " 'We',\n",
       " 'blow',\n",
       " 'it',\n",
       " 'up',\n",
       " 'and',\n",
       " 'look',\n",
       " 'at',\n",
       " 'the',\n",
       " 'pieces',\n",
       " '.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'the',\n",
       " 'EUPHORE',\n",
       " 'Smog',\n",
       " 'Chamber',\n",
       " 'in',\n",
       " 'Spain',\n",
       " '.',\n",
       " 'Atmospheric',\n",
       " 'explosions',\n",
       " ',',\n",
       " 'or',\n",
       " 'full',\n",
       " 'combustion',\n",
       " ',',\n",
       " 'takes',\n",
       " 'about',\n",
       " '15,000',\n",
       " 'times',\n",
       " 'longer',\n",
       " 'than',\n",
       " 'what',\n",
       " 'happens',\n",
       " 'in',\n",
       " 'your',\n",
       " 'car',\n",
       " '.',\n",
       " 'But',\n",
       " 'still',\n",
       " ',',\n",
       " 'we',\n",
       " 'look',\n",
       " 'at',\n",
       " 'the',\n",
       " 'pieces',\n",
       " '.',\n",
       " 'We',\n",
       " 'run',\n",
       " 'enormous',\n",
       " 'models',\n",
       " 'on',\n",
       " 'supercomputers',\n",
       " ';',\n",
       " 'this',\n",
       " 'is',\n",
       " 'what',\n",
       " 'I',\n",
       " 'happen',\n",
       " 'to',\n",
       " 'do',\n",
       " '.',\n",
       " 'Our',\n",
       " 'models',\n",
       " 'have',\n",
       " 'hundreds',\n",
       " 'of',\n",
       " 'thousands',\n",
       " 'of',\n",
       " 'grid',\n",
       " 'boxes',\n",
       " 'calculating',\n",
       " 'hundreds',\n",
       " 'of',\n",
       " 'variables',\n",
       " 'each',\n",
       " ',',\n",
       " 'on',\n",
       " 'minute',\n",
       " 'timescales',\n",
       " '.',\n",
       " 'And',\n",
       " 'it',\n",
       " 'takes',\n",
       " 'weeks',\n",
       " 'to',\n",
       " 'perform',\n",
       " 'our',\n",
       " 'integrations',\n",
       " '.',\n",
       " 'And',\n",
       " 'we',\n",
       " 'perform',\n",
       " 'dozens',\n",
       " 'of',\n",
       " 'integrations',\n",
       " 'in',\n",
       " 'order',\n",
       " 'to',\n",
       " 'understand',\n",
       " 'what',\n",
       " '&apos;s',\n",
       " 'happening',\n",
       " '.',\n",
       " 'We',\n",
       " 'also',\n",
       " 'fly',\n",
       " 'all',\n",
       " 'over',\n",
       " 'the',\n",
       " 'world',\n",
       " 'looking',\n",
       " 'for',\n",
       " 'this',\n",
       " 'thing',\n",
       " '.',\n",
       " 'I',\n",
       " 'recently',\n",
       " 'joined',\n",
       " 'a',\n",
       " 'field',\n",
       " 'campaign',\n",
       " 'in',\n",
       " 'Malaysia',\n",
       " '.',\n",
       " 'There',\n",
       " 'are',\n",
       " 'others',\n",
       " '.',\n",
       " 'We',\n",
       " 'found',\n",
       " 'a',\n",
       " 'global',\n",
       " 'atmospheric',\n",
       " 'watchtower',\n",
       " 'there',\n",
       " ',',\n",
       " 'in',\n",
       " 'the',\n",
       " 'middle',\n",
       " 'of',\n",
       " 'the',\n",
       " 'rainforest',\n",
       " ',',\n",
       " 'and',\n",
       " 'hung',\n",
       " 'hundreds',\n",
       " 'of',\n",
       " 'thousands',\n",
       " 'of',\n",
       " 'dollars',\n",
       " 'worth',\n",
       " 'of',\n",
       " 'scientific',\n",
       " 'equipment',\n",
       " 'off',\n",
       " 'this',\n",
       " 'tower',\n",
       " ',',\n",
       " 'to',\n",
       " 'look',\n",
       " 'for',\n",
       " 'isoprene',\n",
       " ',',\n",
       " 'and',\n",
       " 'of',\n",
       " 'course',\n",
       " ',',\n",
       " 'other',\n",
       " 'things',\n",
       " 'while',\n",
       " 'we',\n",
       " 'were',\n",
       " 'there',\n",
       " '.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'the',\n",
       " 'tower',\n",
       " 'in',\n",
       " 'the',\n",
       " 'middle',\n",
       " 'of',\n",
       " 'the',\n",
       " 'rainforest',\n",
       " ',',\n",
       " 'from',\n",
       " 'above',\n",
       " '.',\n",
       " 'And',\n",
       " 'this',\n",
       " 'is',\n",
       " 'the',\n",
       " 'tower',\n",
       " 'from',\n",
       " 'below',\n",
       " '.',\n",
       " 'And',\n",
       " 'on',\n",
       " 'part',\n",
       " 'of',\n",
       " 'that',\n",
       " 'field',\n",
       " 'campaign',\n",
       " 'we',\n",
       " 'even',\n",
       " 'brought',\n",
       " 'an',\n",
       " 'aircraft',\n",
       " 'with',\n",
       " 'us',\n",
       " '.',\n",
       " 'And',\n",
       " 'this',\n",
       " 'plane',\n",
       " ',',\n",
       " 'the',\n",
       " 'model',\n",
       " ',',\n",
       " 'BA146',\n",
       " ',',\n",
       " 'which',\n",
       " 'was',\n",
       " 'run',\n",
       " 'by',\n",
       " 'FAAM',\n",
       " ',',\n",
       " 'normally',\n",
       " 'flies',\n",
       " '120',\n",
       " 'to',\n",
       " '130',\n",
       " 'people',\n",
       " '.',\n",
       " 'So',\n",
       " 'maybe',\n",
       " 'you',\n",
       " 'took',\n",
       " 'a',\n",
       " 'similar',\n",
       " 'aircraft',\n",
       " 'to',\n",
       " 'get',\n",
       " 'here',\n",
       " 'today',\n",
       " '.',\n",
       " 'But',\n",
       " 'we',\n",
       " 'didn',\n",
       " '&apos;t',\n",
       " 'just',\n",
       " 'fly',\n",
       " 'it',\n",
       " '.',\n",
       " 'We',\n",
       " 'were',\n",
       " 'flying',\n",
       " 'at',\n",
       " '100',\n",
       " 'meters',\n",
       " 'above',\n",
       " 'the',\n",
       " 'top',\n",
       " 'of',\n",
       " 'the',\n",
       " 'canopy',\n",
       " 'to',\n",
       " 'measure',\n",
       " 'this',\n",
       " 'molecule',\n",
       " '--',\n",
       " 'incredibly',\n",
       " 'dangerous',\n",
       " 'stuff',\n",
       " '.',\n",
       " 'We',\n",
       " 'have',\n",
       " 'to',\n",
       " 'fly',\n",
       " 'at',\n",
       " 'a',\n",
       " 'special',\n",
       " 'incline',\n",
       " 'in',\n",
       " 'order',\n",
       " 'to',\n",
       " 'make',\n",
       " 'the',\n",
       " 'measurements',\n",
       " '.',\n",
       " 'We',\n",
       " 'hire',\n",
       " 'military',\n",
       " 'and',\n",
       " 'test',\n",
       " 'pilots',\n",
       " 'to',\n",
       " 'do',\n",
       " 'the',\n",
       " 'maneuvering',\n",
       " '.',\n",
       " 'We',\n",
       " 'have',\n",
       " 'to',\n",
       " 'get',\n",
       " 'special',\n",
       " 'flight',\n",
       " 'clearance',\n",
       " '.',\n",
       " 'And',\n",
       " 'as',\n",
       " 'you',\n",
       " 'come',\n",
       " 'around',\n",
       " 'the',\n",
       " 'banks',\n",
       " 'in',\n",
       " 'these',\n",
       " 'valleys',\n",
       " ',',\n",
       " 'the',\n",
       " 'forces',\n",
       " 'can',\n",
       " 'get',\n",
       " 'up',\n",
       " 'to',\n",
       " 'two',\n",
       " 'Gs',\n",
       " '.',\n",
       " 'And',\n",
       " 'the',\n",
       " 'scientists',\n",
       " 'have',\n",
       " 'to',\n",
       " 'be',\n",
       " 'completely',\n",
       " 'harnessed',\n",
       " 'in',\n",
       " 'in',\n",
       " 'order',\n",
       " 'to',\n",
       " 'make',\n",
       " 'measurements',\n",
       " 'while',\n",
       " 'they',\n",
       " '&apos;re',\n",
       " 'on',\n",
       " 'board',\n",
       " '.',\n",
       " 'So',\n",
       " ',',\n",
       " 'as',\n",
       " 'you',\n",
       " 'can',\n",
       " 'imagine',\n",
       " ',',\n",
       " 'the',\n",
       " 'inside',\n",
       " 'of',\n",
       " 'this',\n",
       " 'aircraft',\n",
       " 'doesn',\n",
       " '&apos;t',\n",
       " 'look',\n",
       " 'like',\n",
       " 'any',\n",
       " 'plane',\n",
       " 'you',\n",
       " 'would',\n",
       " 'take',\n",
       " 'on',\n",
       " 'vacation',\n",
       " '.',\n",
       " 'It',\n",
       " '&apos;s',\n",
       " 'a',\n",
       " 'flying',\n",
       " 'laboratory',\n",
       " 'that',\n",
       " 'we',\n",
       " 'took',\n",
       " 'to',\n",
       " 'make',\n",
       " 'measurements',\n",
       " 'in',\n",
       " 'the',\n",
       " 'region',\n",
       " 'of',\n",
       " 'this',\n",
       " 'molecule',\n",
       " '.',\n",
       " 'We',\n",
       " 'do',\n",
       " 'all',\n",
       " 'of',\n",
       " 'this',\n",
       " 'to',\n",
       " 'understand',\n",
       " 'the',\n",
       " 'chemistry',\n",
       " 'of',\n",
       " 'one',\n",
       " 'molecule',\n",
       " '.',\n",
       " 'And',\n",
       " 'when',\n",
       " 'one',\n",
       " 'student',\n",
       " 'like',\n",
       " 'me',\n",
       " 'has',\n",
       " 'some',\n",
       " 'sort',\n",
       " 'of',\n",
       " 'inclination',\n",
       " 'or',\n",
       " 'understanding',\n",
       " 'about',\n",
       " 'that',\n",
       " 'molecule',\n",
       " ',',\n",
       " 'they',\n",
       " 'write',\n",
       " 'one',\n",
       " 'scientific',\n",
       " 'paper',\n",
       " 'on',\n",
       " 'the',\n",
       " 'subject',\n",
       " '.',\n",
       " 'And',\n",
       " 'out',\n",
       " 'of',\n",
       " 'that',\n",
       " 'field',\n",
       " 'campaign',\n",
       " 'we',\n",
       " '&apos;ll',\n",
       " 'probably',\n",
       " 'get',\n",
       " 'a',\n",
       " 'few',\n",
       " 'dozen',\n",
       " 'papers',\n",
       " 'on',\n",
       " 'a',\n",
       " 'few',\n",
       " 'dozen',\n",
       " 'processes',\n",
       " 'or',\n",
       " 'molecules',\n",
       " '.',\n",
       " 'And',\n",
       " 'as',\n",
       " 'a',\n",
       " 'body',\n",
       " 'of',\n",
       " 'knowledge',\n",
       " 'builds',\n",
       " 'up',\n",
       " ',',\n",
       " 'it',\n",
       " 'will',\n",
       " 'form',\n",
       " 'one',\n",
       " 'subsection',\n",
       " ',',\n",
       " 'or',\n",
       " 'one',\n",
       " 'sub-subsection',\n",
       " 'of',\n",
       " 'an',\n",
       " 'assessment',\n",
       " 'like',\n",
       " 'the',\n",
       " 'IPCC',\n",
       " ',',\n",
       " 'although',\n",
       " 'we',\n",
       " 'have',\n",
       " 'others',\n",
       " '.',\n",
       " 'And',\n",
       " 'each',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " '11',\n",
       " 'chapters',\n",
       " 'of',\n",
       " 'the',\n",
       " 'IPCC',\n",
       " 'has',\n",
       " 'six',\n",
       " 'to',\n",
       " 'ten',\n",
       " 'subsections',\n",
       " '.',\n",
       " 'So',\n",
       " 'you',\n",
       " 'can',\n",
       " 'imagine',\n",
       " 'the',\n",
       " 'scale',\n",
       " 'of',\n",
       " 'the',\n",
       " 'effort',\n",
       " '.',\n",
       " 'In',\n",
       " 'each',\n",
       " 'one',\n",
       " 'of',\n",
       " 'those',\n",
       " 'assessments',\n",
       " 'that',\n",
       " ...]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_en_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_toks(df):\n",
    "    all_toks = []\n",
    "    for i,row in df.iterrows():\n",
    "        all_toks+=row['tokenized']\n",
    "    return all_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_en_toks = get_all_toks(eng_train)\n",
    "all_viet_toks = get_all_toks(viet_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Rachel',\n",
       " 'Pike',\n",
       " ':',\n",
       " 'The',\n",
       " 'science',\n",
       " 'behind',\n",
       " 'a',\n",
       " 'climate',\n",
       " 'headline',\n",
       " 'In',\n",
       " '4',\n",
       " 'minutes',\n",
       " ',',\n",
       " 'atmospheric',\n",
       " 'chemist',\n",
       " 'Rachel',\n",
       " 'Pike',\n",
       " 'provides',\n",
       " 'a',\n",
       " 'glimpse',\n",
       " 'of',\n",
       " 'the',\n",
       " 'massive',\n",
       " 'scientific',\n",
       " 'effort',\n",
       " 'behind',\n",
       " 'the',\n",
       " 'bold',\n",
       " 'headlines',\n",
       " 'on',\n",
       " 'climate',\n",
       " 'change',\n",
       " ',',\n",
       " 'with',\n",
       " 'her',\n",
       " 'team',\n",
       " '--',\n",
       " 'one',\n",
       " 'of',\n",
       " 'thousands',\n",
       " 'who',\n",
       " 'contributed',\n",
       " '--',\n",
       " 'taking',\n",
       " 'a',\n",
       " 'risky',\n",
       " 'flight',\n",
       " 'over',\n",
       " 'the',\n",
       " 'rainforest',\n",
       " 'in',\n",
       " 'pursuit',\n",
       " 'of',\n",
       " 'data',\n",
       " 'on',\n",
       " 'a',\n",
       " 'key',\n",
       " 'molecule',\n",
       " '.',\n",
       " 'I',\n",
       " '&apos;d',\n",
       " 'like',\n",
       " 'to',\n",
       " 'talk',\n",
       " 'to',\n",
       " 'you',\n",
       " 'today',\n",
       " 'about',\n",
       " 'the',\n",
       " 'scale',\n",
       " 'of',\n",
       " 'the',\n",
       " 'scientific',\n",
       " 'effort',\n",
       " 'that',\n",
       " 'goes',\n",
       " 'into',\n",
       " 'making',\n",
       " 'the',\n",
       " 'headlines',\n",
       " 'you',\n",
       " 'see',\n",
       " 'in',\n",
       " 'the',\n",
       " 'paper',\n",
       " '.',\n",
       " 'Headlines',\n",
       " 'that',\n",
       " 'look',\n",
       " 'like',\n",
       " 'this',\n",
       " 'when',\n",
       " 'they',\n",
       " 'have',\n",
       " 'to',\n",
       " 'do',\n",
       " 'with',\n",
       " 'climate',\n",
       " 'change',\n",
       " ',',\n",
       " 'and',\n",
       " 'headlines',\n",
       " 'that',\n",
       " 'look',\n",
       " 'like',\n",
       " 'this',\n",
       " 'when',\n",
       " 'they',\n",
       " 'have',\n",
       " 'to',\n",
       " 'do',\n",
       " 'with',\n",
       " 'air',\n",
       " 'quality',\n",
       " 'or',\n",
       " 'smog',\n",
       " '.',\n",
       " 'They',\n",
       " 'are',\n",
       " 'both',\n",
       " 'two',\n",
       " 'branches',\n",
       " 'of',\n",
       " 'the',\n",
       " 'same',\n",
       " 'field',\n",
       " 'of',\n",
       " 'atmospheric',\n",
       " 'science',\n",
       " '.',\n",
       " 'Recently',\n",
       " 'the',\n",
       " 'headlines',\n",
       " 'looked',\n",
       " 'like',\n",
       " 'this',\n",
       " 'when',\n",
       " 'the',\n",
       " 'Intergovernmental',\n",
       " 'Panel',\n",
       " 'on',\n",
       " 'Climate',\n",
       " 'Change',\n",
       " ',',\n",
       " 'or',\n",
       " 'IPCC',\n",
       " ',',\n",
       " 'put',\n",
       " 'out',\n",
       " 'their',\n",
       " 'report',\n",
       " 'on',\n",
       " 'the',\n",
       " 'state',\n",
       " 'of',\n",
       " 'understanding',\n",
       " 'of',\n",
       " 'the',\n",
       " 'atmospheric',\n",
       " 'system',\n",
       " '.',\n",
       " 'That',\n",
       " 'report',\n",
       " 'was',\n",
       " 'written',\n",
       " 'by',\n",
       " '620',\n",
       " 'scientists',\n",
       " 'from',\n",
       " '40',\n",
       " 'countries',\n",
       " '.',\n",
       " 'They',\n",
       " 'wrote',\n",
       " 'almost',\n",
       " 'a',\n",
       " 'thousand',\n",
       " 'pages',\n",
       " 'on',\n",
       " 'the',\n",
       " 'topic',\n",
       " '.',\n",
       " 'And',\n",
       " 'all',\n",
       " 'of',\n",
       " 'those',\n",
       " 'pages',\n",
       " 'were',\n",
       " 'reviewed',\n",
       " 'by',\n",
       " 'another',\n",
       " '400-plus',\n",
       " 'scientists',\n",
       " 'and',\n",
       " 'reviewers',\n",
       " ',',\n",
       " 'from',\n",
       " '113',\n",
       " 'countries',\n",
       " '.',\n",
       " 'It',\n",
       " '&apos;s',\n",
       " 'a',\n",
       " 'big',\n",
       " 'community',\n",
       " '.',\n",
       " 'It',\n",
       " '&apos;s',\n",
       " 'such',\n",
       " 'a',\n",
       " 'big',\n",
       " 'community',\n",
       " ',',\n",
       " 'in',\n",
       " 'fact',\n",
       " ',',\n",
       " 'that',\n",
       " 'our',\n",
       " 'annual',\n",
       " 'gathering',\n",
       " 'is',\n",
       " 'the',\n",
       " 'largest',\n",
       " 'scientific',\n",
       " 'meeting',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " '.',\n",
       " 'Over',\n",
       " '15,000',\n",
       " 'scientists',\n",
       " 'go',\n",
       " 'to',\n",
       " 'San',\n",
       " 'Francisco',\n",
       " 'every',\n",
       " 'year',\n",
       " 'for',\n",
       " 'that',\n",
       " '.',\n",
       " 'And',\n",
       " 'every',\n",
       " 'one',\n",
       " 'of',\n",
       " 'those',\n",
       " 'scientists',\n",
       " 'is',\n",
       " 'in',\n",
       " 'a',\n",
       " 'research',\n",
       " 'group',\n",
       " ',',\n",
       " 'and',\n",
       " 'every',\n",
       " 'research',\n",
       " 'group',\n",
       " 'studies',\n",
       " 'a',\n",
       " 'wide',\n",
       " 'variety',\n",
       " 'of',\n",
       " 'topics',\n",
       " '.',\n",
       " 'For',\n",
       " 'us',\n",
       " 'at',\n",
       " 'Cambridge',\n",
       " ',',\n",
       " 'it',\n",
       " '&apos;s',\n",
       " 'as',\n",
       " 'varied',\n",
       " 'as',\n",
       " 'the',\n",
       " 'El',\n",
       " 'Niño',\n",
       " 'oscillation',\n",
       " ',',\n",
       " 'which',\n",
       " 'affects',\n",
       " 'weather',\n",
       " 'and',\n",
       " 'climate',\n",
       " ',',\n",
       " 'to',\n",
       " 'the',\n",
       " 'assimilation',\n",
       " 'of',\n",
       " 'satellite',\n",
       " 'data',\n",
       " ',',\n",
       " 'to',\n",
       " 'emissions',\n",
       " 'from',\n",
       " 'crops',\n",
       " 'that',\n",
       " 'produce',\n",
       " 'biofuels',\n",
       " ',',\n",
       " 'which',\n",
       " 'is',\n",
       " 'what',\n",
       " 'I',\n",
       " 'happen',\n",
       " 'to',\n",
       " 'study',\n",
       " '.',\n",
       " 'And',\n",
       " 'in',\n",
       " 'each',\n",
       " 'one',\n",
       " 'of',\n",
       " 'these',\n",
       " 'research',\n",
       " 'areas',\n",
       " ',',\n",
       " 'of',\n",
       " 'which',\n",
       " 'there',\n",
       " 'are',\n",
       " 'even',\n",
       " 'more',\n",
       " ',',\n",
       " 'there',\n",
       " 'are',\n",
       " 'PhD',\n",
       " 'students',\n",
       " ',',\n",
       " 'like',\n",
       " 'me',\n",
       " ',',\n",
       " 'and',\n",
       " 'we',\n",
       " 'study',\n",
       " 'incredibly',\n",
       " 'narrow',\n",
       " 'topics',\n",
       " ',',\n",
       " 'things',\n",
       " 'as',\n",
       " 'narrow',\n",
       " 'as',\n",
       " 'a',\n",
       " 'few',\n",
       " 'processes',\n",
       " 'or',\n",
       " 'a',\n",
       " 'few',\n",
       " 'molecules',\n",
       " '.',\n",
       " 'And',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'molecules',\n",
       " 'I',\n",
       " 'study',\n",
       " 'is',\n",
       " 'called',\n",
       " 'isoprene',\n",
       " ',',\n",
       " 'which',\n",
       " 'is',\n",
       " 'here',\n",
       " '.',\n",
       " 'It',\n",
       " '&apos;s',\n",
       " 'a',\n",
       " 'small',\n",
       " 'organic',\n",
       " 'molecule',\n",
       " '.',\n",
       " 'You',\n",
       " '&apos;ve',\n",
       " 'probably',\n",
       " 'never',\n",
       " 'heard',\n",
       " 'of',\n",
       " 'it',\n",
       " '.',\n",
       " 'The',\n",
       " 'weight',\n",
       " 'of',\n",
       " 'a',\n",
       " 'paper',\n",
       " 'clip',\n",
       " 'is',\n",
       " 'approximately',\n",
       " 'equal',\n",
       " 'to',\n",
       " '900',\n",
       " 'zeta-illion',\n",
       " '--',\n",
       " '10',\n",
       " 'to',\n",
       " 'the',\n",
       " '21st',\n",
       " '--',\n",
       " 'molecules',\n",
       " 'of',\n",
       " 'isoprene',\n",
       " '.',\n",
       " 'But',\n",
       " 'despite',\n",
       " 'its',\n",
       " 'very',\n",
       " 'small',\n",
       " 'weight',\n",
       " ',',\n",
       " 'enough',\n",
       " 'of',\n",
       " 'it',\n",
       " 'is',\n",
       " 'emitted',\n",
       " 'into',\n",
       " 'the',\n",
       " 'atmosphere',\n",
       " 'every',\n",
       " 'year',\n",
       " 'to',\n",
       " 'equal',\n",
       " 'the',\n",
       " 'weight',\n",
       " 'of',\n",
       " 'all',\n",
       " 'the',\n",
       " 'people',\n",
       " 'on',\n",
       " 'the',\n",
       " 'planet',\n",
       " '.',\n",
       " 'It',\n",
       " '&apos;s',\n",
       " 'a',\n",
       " 'huge',\n",
       " 'amount',\n",
       " 'of',\n",
       " 'stuff',\n",
       " '.',\n",
       " 'It',\n",
       " '&apos;s',\n",
       " 'equal',\n",
       " 'to',\n",
       " 'the',\n",
       " 'weight',\n",
       " 'of',\n",
       " 'methane',\n",
       " '.',\n",
       " 'And',\n",
       " 'because',\n",
       " 'it',\n",
       " '&apos;s',\n",
       " 'so',\n",
       " 'much',\n",
       " 'stuff',\n",
       " ',',\n",
       " 'it',\n",
       " '&apos;s',\n",
       " 'really',\n",
       " 'important',\n",
       " 'for',\n",
       " 'the',\n",
       " 'atmospheric',\n",
       " 'system',\n",
       " '.',\n",
       " 'Because',\n",
       " 'it',\n",
       " '&apos;s',\n",
       " 'important',\n",
       " 'to',\n",
       " 'the',\n",
       " 'atmospheric',\n",
       " 'system',\n",
       " ',',\n",
       " 'we',\n",
       " 'go',\n",
       " 'to',\n",
       " 'all',\n",
       " 'lengths',\n",
       " 'to',\n",
       " 'study',\n",
       " 'this',\n",
       " 'thing',\n",
       " '.',\n",
       " 'We',\n",
       " 'blow',\n",
       " 'it',\n",
       " 'up',\n",
       " 'and',\n",
       " 'look',\n",
       " 'at',\n",
       " 'the',\n",
       " 'pieces',\n",
       " '.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'the',\n",
       " 'EUPHORE',\n",
       " 'Smog',\n",
       " 'Chamber',\n",
       " 'in',\n",
       " 'Spain',\n",
       " '.',\n",
       " 'Atmospheric',\n",
       " 'explosions',\n",
       " ',',\n",
       " 'or',\n",
       " 'full',\n",
       " 'combustion',\n",
       " ',',\n",
       " 'takes',\n",
       " 'about',\n",
       " '15,000',\n",
       " 'times',\n",
       " 'longer',\n",
       " 'than',\n",
       " 'what',\n",
       " 'happens',\n",
       " 'in',\n",
       " 'your',\n",
       " 'car',\n",
       " '.',\n",
       " 'But',\n",
       " 'still',\n",
       " ',',\n",
       " 'we',\n",
       " 'look',\n",
       " 'at',\n",
       " 'the',\n",
       " 'pieces',\n",
       " '.',\n",
       " 'We',\n",
       " 'run',\n",
       " 'enormous',\n",
       " 'models',\n",
       " 'on',\n",
       " 'supercomputers',\n",
       " ';',\n",
       " 'this',\n",
       " 'is',\n",
       " 'what',\n",
       " 'I',\n",
       " 'happen',\n",
       " 'to',\n",
       " 'do',\n",
       " '.',\n",
       " 'Our',\n",
       " 'models',\n",
       " 'have',\n",
       " 'hundreds',\n",
       " 'of',\n",
       " 'thousands',\n",
       " 'of',\n",
       " 'grid',\n",
       " 'boxes',\n",
       " 'calculating',\n",
       " 'hundreds',\n",
       " 'of',\n",
       " 'variables',\n",
       " 'each',\n",
       " ',',\n",
       " 'on',\n",
       " 'minute',\n",
       " 'timescales',\n",
       " '.',\n",
       " 'And',\n",
       " 'it',\n",
       " 'takes',\n",
       " 'weeks',\n",
       " 'to',\n",
       " 'perform',\n",
       " 'our',\n",
       " 'integrations',\n",
       " '.',\n",
       " 'And',\n",
       " 'we',\n",
       " 'perform',\n",
       " 'dozens',\n",
       " 'of',\n",
       " 'integrations',\n",
       " 'in',\n",
       " 'order',\n",
       " 'to',\n",
       " 'understand',\n",
       " 'what',\n",
       " '&apos;s',\n",
       " 'happening',\n",
       " '.',\n",
       " 'We',\n",
       " 'also',\n",
       " 'fly',\n",
       " 'all',\n",
       " 'over',\n",
       " 'the',\n",
       " 'world',\n",
       " 'looking',\n",
       " 'for',\n",
       " 'this',\n",
       " 'thing',\n",
       " '.',\n",
       " 'I',\n",
       " 'recently',\n",
       " 'joined',\n",
       " 'a',\n",
       " 'field',\n",
       " 'campaign',\n",
       " 'in',\n",
       " 'Malaysia',\n",
       " '.',\n",
       " 'There',\n",
       " 'are',\n",
       " 'others',\n",
       " '.',\n",
       " 'We',\n",
       " 'found',\n",
       " 'a',\n",
       " 'global',\n",
       " 'atmospheric',\n",
       " 'watchtower',\n",
       " 'there',\n",
       " ',',\n",
       " 'in',\n",
       " 'the',\n",
       " 'middle',\n",
       " 'of',\n",
       " 'the',\n",
       " 'rainforest',\n",
       " ',',\n",
       " 'and',\n",
       " 'hung',\n",
       " 'hundreds',\n",
       " 'of',\n",
       " 'thousands',\n",
       " 'of',\n",
       " 'dollars',\n",
       " 'worth',\n",
       " 'of',\n",
       " 'scientific',\n",
       " 'equipment',\n",
       " 'off',\n",
       " 'this',\n",
       " 'tower',\n",
       " ',',\n",
       " 'to',\n",
       " 'look',\n",
       " 'for',\n",
       " 'isoprene',\n",
       " ',',\n",
       " 'and',\n",
       " 'of',\n",
       " 'course',\n",
       " ',',\n",
       " 'other',\n",
       " 'things',\n",
       " 'while',\n",
       " 'we',\n",
       " 'were',\n",
       " 'there',\n",
       " '.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'the',\n",
       " 'tower',\n",
       " 'in',\n",
       " 'the',\n",
       " 'middle',\n",
       " 'of',\n",
       " 'the',\n",
       " 'rainforest',\n",
       " ',',\n",
       " 'from',\n",
       " 'above',\n",
       " '.',\n",
       " 'And',\n",
       " 'this',\n",
       " 'is',\n",
       " 'the',\n",
       " 'tower',\n",
       " 'from',\n",
       " 'below',\n",
       " '.',\n",
       " 'And',\n",
       " 'on',\n",
       " 'part',\n",
       " 'of',\n",
       " 'that',\n",
       " 'field',\n",
       " 'campaign',\n",
       " 'we',\n",
       " 'even',\n",
       " 'brought',\n",
       " 'an',\n",
       " 'aircraft',\n",
       " 'with',\n",
       " 'us',\n",
       " '.',\n",
       " 'And',\n",
       " 'this',\n",
       " 'plane',\n",
       " ',',\n",
       " 'the',\n",
       " 'model',\n",
       " ',',\n",
       " 'BA146',\n",
       " ',',\n",
       " 'which',\n",
       " 'was',\n",
       " 'run',\n",
       " 'by',\n",
       " 'FAAM',\n",
       " ',',\n",
       " 'normally',\n",
       " 'flies',\n",
       " '120',\n",
       " 'to',\n",
       " '130',\n",
       " 'people',\n",
       " '.',\n",
       " 'So',\n",
       " 'maybe',\n",
       " 'you',\n",
       " 'took',\n",
       " 'a',\n",
       " 'similar',\n",
       " 'aircraft',\n",
       " 'to',\n",
       " 'get',\n",
       " 'here',\n",
       " 'today',\n",
       " '.',\n",
       " 'But',\n",
       " 'we',\n",
       " 'didn',\n",
       " '&apos;t',\n",
       " 'just',\n",
       " 'fly',\n",
       " 'it',\n",
       " '.',\n",
       " 'We',\n",
       " 'were',\n",
       " 'flying',\n",
       " 'at',\n",
       " '100',\n",
       " 'meters',\n",
       " 'above',\n",
       " 'the',\n",
       " 'top',\n",
       " 'of',\n",
       " 'the',\n",
       " 'canopy',\n",
       " 'to',\n",
       " 'measure',\n",
       " 'this',\n",
       " 'molecule',\n",
       " '--',\n",
       " 'incredibly',\n",
       " 'dangerous',\n",
       " 'stuff',\n",
       " '.',\n",
       " 'We',\n",
       " 'have',\n",
       " 'to',\n",
       " 'fly',\n",
       " 'at',\n",
       " 'a',\n",
       " 'special',\n",
       " 'incline',\n",
       " 'in',\n",
       " 'order',\n",
       " 'to',\n",
       " 'make',\n",
       " 'the',\n",
       " 'measurements',\n",
       " '.',\n",
       " 'We',\n",
       " 'hire',\n",
       " 'military',\n",
       " 'and',\n",
       " 'test',\n",
       " 'pilots',\n",
       " 'to',\n",
       " 'do',\n",
       " 'the',\n",
       " 'maneuvering',\n",
       " '.',\n",
       " 'We',\n",
       " 'have',\n",
       " 'to',\n",
       " 'get',\n",
       " 'special',\n",
       " 'flight',\n",
       " 'clearance',\n",
       " '.',\n",
       " 'And',\n",
       " 'as',\n",
       " 'you',\n",
       " 'come',\n",
       " 'around',\n",
       " 'the',\n",
       " 'banks',\n",
       " 'in',\n",
       " 'these',\n",
       " 'valleys',\n",
       " ',',\n",
       " 'the',\n",
       " 'forces',\n",
       " 'can',\n",
       " 'get',\n",
       " 'up',\n",
       " 'to',\n",
       " 'two',\n",
       " 'Gs',\n",
       " '.',\n",
       " 'And',\n",
       " 'the',\n",
       " 'scientists',\n",
       " 'have',\n",
       " 'to',\n",
       " 'be',\n",
       " 'completely',\n",
       " 'harnessed',\n",
       " 'in',\n",
       " 'in',\n",
       " 'order',\n",
       " 'to',\n",
       " 'make',\n",
       " 'measurements',\n",
       " 'while',\n",
       " 'they',\n",
       " '&apos;re',\n",
       " 'on',\n",
       " 'board',\n",
       " '.',\n",
       " 'So',\n",
       " ',',\n",
       " 'as',\n",
       " 'you',\n",
       " 'can',\n",
       " 'imagine',\n",
       " ',',\n",
       " 'the',\n",
       " 'inside',\n",
       " 'of',\n",
       " 'this',\n",
       " 'aircraft',\n",
       " 'doesn',\n",
       " '&apos;t',\n",
       " 'look',\n",
       " 'like',\n",
       " 'any',\n",
       " 'plane',\n",
       " 'you',\n",
       " 'would',\n",
       " 'take',\n",
       " 'on',\n",
       " 'vacation',\n",
       " '.',\n",
       " 'It',\n",
       " '&apos;s',\n",
       " 'a',\n",
       " 'flying',\n",
       " 'laboratory',\n",
       " 'that',\n",
       " 'we',\n",
       " 'took',\n",
       " 'to',\n",
       " 'make',\n",
       " 'measurements',\n",
       " 'in',\n",
       " 'the',\n",
       " 'region',\n",
       " 'of',\n",
       " 'this',\n",
       " 'molecule',\n",
       " '.',\n",
       " 'We',\n",
       " 'do',\n",
       " 'all',\n",
       " 'of',\n",
       " 'this',\n",
       " 'to',\n",
       " 'understand',\n",
       " 'the',\n",
       " 'chemistry',\n",
       " 'of',\n",
       " 'one',\n",
       " 'molecule',\n",
       " '.',\n",
       " 'And',\n",
       " 'when',\n",
       " 'one',\n",
       " 'student',\n",
       " 'like',\n",
       " 'me',\n",
       " 'has',\n",
       " 'some',\n",
       " 'sort',\n",
       " 'of',\n",
       " 'inclination',\n",
       " 'or',\n",
       " 'understanding',\n",
       " 'about',\n",
       " 'that',\n",
       " 'molecule',\n",
       " ',',\n",
       " 'they',\n",
       " 'write',\n",
       " 'one',\n",
       " 'scientific',\n",
       " 'paper',\n",
       " 'on',\n",
       " 'the',\n",
       " 'subject',\n",
       " '.',\n",
       " 'And',\n",
       " 'out',\n",
       " 'of',\n",
       " 'that',\n",
       " 'field',\n",
       " 'campaign',\n",
       " 'we',\n",
       " '&apos;ll',\n",
       " 'probably',\n",
       " 'get',\n",
       " 'a',\n",
       " 'few',\n",
       " 'dozen',\n",
       " 'papers',\n",
       " 'on',\n",
       " 'a',\n",
       " 'few',\n",
       " 'dozen',\n",
       " 'processes',\n",
       " 'or',\n",
       " 'molecules',\n",
       " '.',\n",
       " 'And',\n",
       " 'as',\n",
       " 'a',\n",
       " 'body',\n",
       " 'of',\n",
       " 'knowledge',\n",
       " 'builds',\n",
       " 'up',\n",
       " ',',\n",
       " 'it',\n",
       " 'will',\n",
       " 'form',\n",
       " 'one',\n",
       " 'subsection',\n",
       " ',',\n",
       " 'or',\n",
       " 'one',\n",
       " 'sub-subsection',\n",
       " 'of',\n",
       " 'an',\n",
       " 'assessment',\n",
       " 'like',\n",
       " 'the',\n",
       " 'IPCC',\n",
       " ',',\n",
       " 'although',\n",
       " 'we',\n",
       " 'have',\n",
       " 'others',\n",
       " '.',\n",
       " 'And',\n",
       " 'each',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " '11',\n",
       " 'chapters',\n",
       " 'of',\n",
       " 'the',\n",
       " 'IPCC',\n",
       " 'has',\n",
       " 'six',\n",
       " 'to',\n",
       " 'ten',\n",
       " 'subsections',\n",
       " '.',\n",
       " 'So',\n",
       " 'you',\n",
       " 'can',\n",
       " 'imagine',\n",
       " 'the',\n",
       " 'scale',\n",
       " 'of',\n",
       " 'the',\n",
       " 'effort',\n",
       " '.',\n",
       " 'In',\n",
       " 'each',\n",
       " 'one',\n",
       " 'of',\n",
       " 'those',\n",
       " 'assessments',\n",
       " 'that',\n",
       " ...]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_viet_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(len(all_tokens)))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54171"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_id2token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "viet_token2id, viet_id2token = build_vocab(all_viet_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42150"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(viet_token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token2index_dataset(df,token2id):\n",
    "    indices_data = []\n",
    "    for tokens in df['tokenized']:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    df['idized'] = indices_data\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Khi tôi còn nhỏ , Tôi nghĩ rằng BắcTriều Tiên ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tôi đã rất tự_hào về đất_nước tôi .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ở trường , chúng_tôi dành rất nhiều thời_gian ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mặc_dù tôi đã từng tự_hỏi không biết thế_giới ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Khi tôi lên 7 , tôi chứng_kiến cảnh người_ta x...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  Khi tôi còn nhỏ , Tôi nghĩ rằng BắcTriều Tiên ...\n",
       "1                Tôi đã rất tự_hào về đất_nước tôi .\n",
       "2  Ở trường , chúng_tôi dành rất nhiều thời_gian ...\n",
       "3  Mặc_dù tôi đã từng tự_hỏi không biết thế_giới ...\n",
       "4  Khi tôi lên 7 , tôi chứng_kiến cảnh người_ta x..."
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viet_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_train = token2index_dataset(eng_train,en_token2id)\n",
    "eng_val = token2index_dataset(eng_val,en_token2id)\n",
    "eng_test = token2index_dataset(eng_test,en_token2id)\n",
    "\n",
    "viet_train = token2index_dataset(viet_train,viet_token2id)\n",
    "viet_val = token2index_dataset(viet_val,viet_token2id)\n",
    "viet_test = token2index_dataset(viet_test,viet_token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>idized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rachel Pike : The science behind a climate hea...</td>\n",
       "      <td>[Rachel, Pike, :, The, science, behind, a, cli...</td>\n",
       "      <td>[6554, 17190, 51, 54, 336, 587, 8, 740, 5466]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In 4 minutes , atmospheric chemist Rachel Pike...</td>\n",
       "      <td>[In, 4, minutes, ,, atmospheric, chemist, Rach...</td>\n",
       "      <td>[127, 3007, 460, 2, 9130, 6555, 6554, 17190, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I &amp;apos;d like to talk to you today about the ...</td>\n",
       "      <td>[I, &amp;apos;d, like, to, talk, to, you, today, a...</td>\n",
       "      <td>[10, 169, 46, 5, 168, 5, 13, 179, 33, 4, 751, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Headlines that look like this when they have t...</td>\n",
       "      <td>[Headlines, that, look, like, this, when, they...</td>\n",
       "      <td>[24418, 9, 123, 46, 18, 65, 24, 23, 5, 39, 25,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>They are both two branches of the same field o...</td>\n",
       "      <td>[They, are, both, two, branches, of, the, same...</td>\n",
       "      <td>[106, 22, 412, 117, 5068, 6, 4, 155, 719, 6, 9...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  Rachel Pike : The science behind a climate hea...   \n",
       "1  In 4 minutes , atmospheric chemist Rachel Pike...   \n",
       "2  I &apos;d like to talk to you today about the ...   \n",
       "3  Headlines that look like this when they have t...   \n",
       "4  They are both two branches of the same field o...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [Rachel, Pike, :, The, science, behind, a, cli...   \n",
       "1  [In, 4, minutes, ,, atmospheric, chemist, Rach...   \n",
       "2  [I, &apos;d, like, to, talk, to, you, today, a...   \n",
       "3  [Headlines, that, look, like, this, when, they...   \n",
       "4  [They, are, both, two, branches, of, the, same...   \n",
       "\n",
       "                                              idized  \n",
       "0      [6554, 17190, 51, 54, 336, 587, 8, 740, 5466]  \n",
       "1  [127, 3007, 460, 2, 9130, 6555, 6554, 17190, 3...  \n",
       "2  [10, 169, 46, 5, 168, 5, 13, 179, 33, 4, 751, ...  \n",
       "3  [24418, 9, 123, 46, 18, 65, 24, 23, 5, 39, 25,...  \n",
       "4  [106, 22, 412, 117, 5068, 6, 4, 155, 719, 6, 9...  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>idized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Khoa_học đằng_sau một tiêu_đề về khí_hậu</td>\n",
       "      <td>[Khoa_học, đằng_sau, một, tiêu_đề, về, khí_hậu]</td>\n",
       "      <td>[2716, 1360, 6, 3493, 30, 881]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trong 4 phút , chuyên_gia hoá_học khí_quyển Ra...</td>\n",
       "      <td>[Trong, 4, phút, ,, chuyên_gia, hoá_học, khí_q...</td>\n",
       "      <td>[203, 425, 395, 2, 919, 1094, 2071, 6706, 1689...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tôi muốn cho các bạn biết về sự to_lớn của nhữ...</td>\n",
       "      <td>[Tôi, muốn, cho, các, bạn, biết, về, sự, to_lớ...</td>\n",
       "      <td>[35, 70, 26, 16, 11, 59, 30, 37, 990, 9, 8, 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Có những dòng trông như thế_này khi bàn về biế...</td>\n",
       "      <td>[Có, những, dòng, trông, như, thế_này, khi, bà...</td>\n",
       "      <td>[206, 8, 766, 382, 51, 394, 40, 701, 30, 722, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cả hai đều là một nhánh của cùng một lĩnh_vực ...</td>\n",
       "      <td>[Cả, hai, đều, là, một, nhánh, của, cùng, một,...</td>\n",
       "      <td>[1627, 126, 119, 4, 6, 3723, 9, 159, 6, 664, 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0           Khoa_học đằng_sau một tiêu_đề về khí_hậu   \n",
       "1  Trong 4 phút , chuyên_gia hoá_học khí_quyển Ra...   \n",
       "2  Tôi muốn cho các bạn biết về sự to_lớn của nhữ...   \n",
       "3  Có những dòng trông như thế_này khi bàn về biế...   \n",
       "4  Cả hai đều là một nhánh của cùng một lĩnh_vực ...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0    [Khoa_học, đằng_sau, một, tiêu_đề, về, khí_hậu]   \n",
       "1  [Trong, 4, phút, ,, chuyên_gia, hoá_học, khí_q...   \n",
       "2  [Tôi, muốn, cho, các, bạn, biết, về, sự, to_lớ...   \n",
       "3  [Có, những, dòng, trông, như, thế_này, khi, bà...   \n",
       "4  [Cả, hai, đều, là, một, nhánh, của, cùng, một,...   \n",
       "\n",
       "                                              idized  \n",
       "0                     [2716, 1360, 6, 3493, 30, 881]  \n",
       "1  [203, 425, 395, 2, 919, 1094, 2071, 6706, 1689...  \n",
       "2  [35, 70, 26, 16, 11, 59, 30, 37, 990, 9, 8, 10...  \n",
       "3  [206, 8, 766, 382, 51, 394, 40, 701, 30, 722, ...  \n",
       "4  [1627, 126, 119, 4, 6, 3723, 9, 159, 6, 664, 1...  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viet_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vietnamese(Dataset):\n",
    "    def __init__(self, en_df,v_df):\n",
    "        self.en_df = en_df\n",
    "        self.v_df = v_df\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.en_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        english = self.en_df.iloc[idx]['idized']\n",
    "        vietnam = self.v_df.iloc[idx]['idized']\n",
    "        return [english,vietnam]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 10\n",
    "transformed_dataset = {'train': Vietnamese(eng_train,viet_train),\n",
    "                       'validate': Vietnamese(eng_val,viet_val),\n",
    "                       'test': Vietnamese(eng_test,viet_test)\n",
    "                                               }\n",
    "\n",
    "dataloader = {x: DataLoader(transformed_dataset[x], batch_size=bs,\n",
    "                        shuffle=True, num_workers=0) for x in ['train', 'validate','test']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor([  137,   162,    17,    54,   162, 10908,    30,    44,  7309,    55]),\n",
       "  tensor([144,  38, 180, 359,   4,  51,  75,  12,  51, 153]),\n",
       "  tensor([  24,  897,    2,   53,  937,  678,  225,    2,   10, 1078]),\n",
       "  tensor([  176,   233,    14,  3385, 10529,     2,    14,    19,    98,    14]),\n",
       "  tensor([ 29,  11,  15,   3,   5,   9, 133,  72,  31,  68])],\n",
       " [tensor([5730, 4679,   97,  672,  922, 3556,   22,  459, 4366,   97]),\n",
       "  tensor([    4,  6095,     4,    44,    87, 25126,   444,    92,    58,     4]),\n",
       "  tensor([ 5502, 17667,  9635,    15,     2,   502,    18,   601,    97,   629]),\n",
       "  tensor([ 406,  906,  143,  102,  108,    8, 2382,  676,    4,  403]),\n",
       "  tensor([  14, 3112,  128,   53,  232, 1853,   43,    9,    8,   32]),\n",
       "  tensor([  287,   815, 19659,  3844,    18,    30,    20,    44,  4047,   106]),\n",
       "  tensor([   3,   81,    5, 1734,   48,  548,  974,    2, 6645,   81])]]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataloader['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
