{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Required Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import random\n",
    "import pdb\n",
    "from torch.utils.data import DataLoader\n",
    "import logging\n",
    "import itertools\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import argparse\n",
    "from torch import optim\n",
    "import time\n",
    "import os\n",
    "from bleu_score import BLEU_SCORE\n",
    "# from models_viet import EncoderRNN, AttentionDecoderRNN, DecoderRNN\n",
    "from load_dataset_viet import *\n",
    "# from define_training_viet import *\n",
    "import torchtext\n",
    "from torch.utils.data import BatchSampler\n",
    "from torch.utils.data import SequentialSampler\n",
    "from torch.utils.data import Sampler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import copy\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Build data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vietnamese(Dataset):\n",
    "    def __init__(self, df, val = False):\n",
    "        self.df = df\n",
    "        self.val = val\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        english = self.df.iloc[idx,:]['en_idized']\n",
    "        viet = self.df.iloc[idx,:]['vi_idized']\n",
    "        en_len = self.df.iloc[idx,:]['en_len']\n",
    "        vi_len = self.df.iloc[idx,:]['vi_len']\n",
    "        if self.val:\n",
    "            en_data = self.df.iloc[idx,:]['en_data'].lower()\n",
    "            return [viet,english,vi_len,en_len,en_data]\n",
    "        else:\n",
    "            return [viet,english,vi_len,en_len]\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "def vocab_collate_func(batch):\n",
    "    MAX_LEN_EN = 48\n",
    "    MAX_LEN_VI = 48\n",
    "    en_data = []\n",
    "    vi_data = []\n",
    "    en_len = []\n",
    "    vi_len = []\n",
    "    for datum in batch:\n",
    "        en_len.append(datum[3])\n",
    "        vi_len.append(datum[2])\n",
    "    max_batch_length_en = max(en_len)\n",
    "    max_batch_length_vi = max(vi_len)\n",
    "    if max_batch_length_en < MAX_LEN_EN:\n",
    "        MAX_LEN_EN = max_batch_length_en\n",
    "    if max_batch_length_vi < MAX_LEN_VI:\n",
    "        MAX_LEN_VI = max_batch_length_vi\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        if datum[2]>MAX_LEN_VI:\n",
    "            padded_vec_s1 = np.array(datum[0])[:MAX_LEN_VI]\n",
    "        else:\n",
    "            padded_vec_s1 = np.pad(np.array(datum[0]),\n",
    "                                pad_width=((0,MAX_LEN_VI - datum[2])),\n",
    "                                mode=\"constant\", constant_values=PAD_IDX)\n",
    "        if datum[3]>MAX_LEN_EN:\n",
    "            padded_vec_s2 = np.array(datum[1])[:MAX_LEN_EN]\n",
    "        else:\n",
    "            padded_vec_s2 = np.pad(np.array(datum[1]),\n",
    "                                pad_width=((0,MAX_LEN_EN - datum[3])),\n",
    "                                mode=\"constant\", constant_values=PAD_IDX)\n",
    "        en_data.append(padded_vec_s2)\n",
    "        vi_data.append(padded_vec_s1)\n",
    "    vi_data = np.array(vi_data)\n",
    "    en_data = np.array(en_data)\n",
    "    vi_len = np.array(vi_len)\n",
    "    en_len = np.array(en_len)\n",
    "#     sorted_vi_len = np.argsort(-vi_len)\n",
    "#     vi_data = vi_data[sorted_vi_len]\n",
    "#     en_data = en_data[sorted_vi_len]\n",
    "#     vi_len = vi_len[sorted_vi_len]\n",
    "#     en_len = en_len[sorted_vi_len]\n",
    "#     print(en_len)\n",
    "    vi_len[vi_len>MAX_LEN_VI] = MAX_LEN_VI\n",
    "    en_len[en_len>MAX_LEN_EN] = MAX_LEN_EN\n",
    "        \n",
    "    return [torch.from_numpy(vi_data), torch.from_numpy(en_data),\n",
    "            torch.from_numpy(vi_len), torch.from_numpy(en_len)]\n",
    "\n",
    "def convert_idx_2_sent(tensor, lang_obj):\n",
    "    word_list = []\n",
    "    for i in tensor:\n",
    "        if i.item() not in set([PAD_IDX,EOS_token,SOS_token]):\n",
    "            word_list.append(lang_obj.index2word[i.item()])\n",
    "    return (' ').join(word_list)\n",
    "\n",
    "def convert_id_list_2_sent(list_idx, lang_obj):\n",
    "    word_list = []\n",
    "    if type(list_idx) == list:\n",
    "        for i in list_idx:\n",
    "            if i not in set([EOS_token]):\n",
    "                word_list.append(lang_obj.index2word[i])\n",
    "    else:\n",
    "        for i in list_idx:\n",
    "            if i.item() not in set([EOS_token,SOS_token,PAD_IDX]):\n",
    "                word_list.append(lang_obj.index2word[i.item()])\n",
    "    return (' ').join(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_collate_func_val(batch):\n",
    "    return [torch.from_numpy(np.array(batch[0][0])).unsqueeze(0), torch.from_numpy(np.array(batch[0][1])).unsqueeze(0),\n",
    "            torch.from_numpy(np.array(batch[0][2])).unsqueeze(0), torch.from_numpy(np.array(batch[0][3])).unsqueeze(0),batch[0][4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_LEN = 48\n",
    "train,val,test,en_lang,vi_lang = train_val_load(48, \"\", '/scratch/ark576/machine_translation_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_sorted_batches(train, bs):\n",
    "    batch_samp_list = list(BatchSampler(SequentialSampler(train), bs, drop_last = False))\n",
    "    np.random.shuffle(batch_samp_list)\n",
    "    batch_samp_list_merged = list(itertools.chain(*batch_samp_list))\n",
    "    return train.iloc[batch_samp_list_merged,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en_data</th>\n",
       "      <th>vi_data</th>\n",
       "      <th>en_tokenized</th>\n",
       "      <th>vi_tokenized</th>\n",
       "      <th>en_idized</th>\n",
       "      <th>vi_idized</th>\n",
       "      <th>en_len</th>\n",
       "      <th>vi_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rachel Pike : The science behind a climate hea...</td>\n",
       "      <td>Khoa_học đằng_sau một tiêu_đề về khí_hậu</td>\n",
       "      <td>[rachel, pike, :, the, science, behind, a, cli...</td>\n",
       "      <td>[khoa_học, đằng_sau, một, tiêu_đề, về, khí_hậu]</td>\n",
       "      <td>[8225, 2, 137, 4, 150, 573, 9, 15, 3717, 1]</td>\n",
       "      <td>[7, 1433, 5, 3054, 4, 13, 1]</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In 4 minutes , atmospheric chemist Rachel Pike...</td>\n",
       "      <td>Trong 4 phút , chuyên_gia hoá_học khí_quyển Ra...</td>\n",
       "      <td>[in, 4, minutes, ,, atmospheric, chemist, rach...</td>\n",
       "      <td>[trong, 4, phút, ,, chuyên_gia, hoá_học, khí_q...</td>\n",
       "      <td>[12, 5080, 264, 7, 20, 4552, 8225, 2, 3006, 9,...</td>\n",
       "      <td>[23, 812, 253, 8, 1043, 1342, 20, 8510, 2, 161...</td>\n",
       "      <td>51</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I &amp;apos;d like to talk to you today about the ...</td>\n",
       "      <td>Tôi muốn cho các bạn biết về sự to_lớn của nhữ...</td>\n",
       "      <td>[i, &amp;apos;d, like, to, talk, to, you, today, a...</td>\n",
       "      <td>[tôi, muốn, cho, các, bạn, biết, về, sự, to_lớ...</td>\n",
       "      <td>[23, 247, 16, 13, 272, 13, 33, 273, 100, 4, 14...</td>\n",
       "      <td>[26, 152, 47, 11, 38, 149, 4, 92, 787, 9, 15, ...</td>\n",
       "      <td>28</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Headlines that look like this when they have t...</td>\n",
       "      <td>Có những dòng trông như thế_này khi bàn về biế...</td>\n",
       "      <td>[headlines, that, look, like, this, when, they...</td>\n",
       "      <td>[có, những, dòng, trông, như, thế_này, khi, bà...</td>\n",
       "      <td>[6, 11, 24, 16, 21, 49, 35, 31, 13, 36, 51, 15...</td>\n",
       "      <td>[21, 15, 530, 419, 17, 193, 34, 488, 4, 1227, ...</td>\n",
       "      <td>32</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>They are both two branches of the same field o...</td>\n",
       "      <td>Cả hai đều là một nhánh của cùng một lĩnh_vực ...</td>\n",
       "      <td>[they, are, both, two, branches, of, the, same...</td>\n",
       "      <td>[cả, hai, đều, là, một, nhánh, của, cùng, một,...</td>\n",
       "      <td>[35, 55, 402, 118, 3364, 5, 4, 154, 260, 5, 20...</td>\n",
       "      <td>[41, 273, 70, 16, 5, 3618, 9, 178, 5, 198, 23,...</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             en_data  \\\n",
       "0  Rachel Pike : The science behind a climate hea...   \n",
       "1  In 4 minutes , atmospheric chemist Rachel Pike...   \n",
       "2  I &apos;d like to talk to you today about the ...   \n",
       "3  Headlines that look like this when they have t...   \n",
       "4  They are both two branches of the same field o...   \n",
       "\n",
       "                                             vi_data  \\\n",
       "0           Khoa_học đằng_sau một tiêu_đề về khí_hậu   \n",
       "1  Trong 4 phút , chuyên_gia hoá_học khí_quyển Ra...   \n",
       "2  Tôi muốn cho các bạn biết về sự to_lớn của nhữ...   \n",
       "3  Có những dòng trông như thế_này khi bàn về biế...   \n",
       "4  Cả hai đều là một nhánh của cùng một lĩnh_vực ...   \n",
       "\n",
       "                                        en_tokenized  \\\n",
       "0  [rachel, pike, :, the, science, behind, a, cli...   \n",
       "1  [in, 4, minutes, ,, atmospheric, chemist, rach...   \n",
       "2  [i, &apos;d, like, to, talk, to, you, today, a...   \n",
       "3  [headlines, that, look, like, this, when, they...   \n",
       "4  [they, are, both, two, branches, of, the, same...   \n",
       "\n",
       "                                        vi_tokenized  \\\n",
       "0    [khoa_học, đằng_sau, một, tiêu_đề, về, khí_hậu]   \n",
       "1  [trong, 4, phút, ,, chuyên_gia, hoá_học, khí_q...   \n",
       "2  [tôi, muốn, cho, các, bạn, biết, về, sự, to_lớ...   \n",
       "3  [có, những, dòng, trông, như, thế_này, khi, bà...   \n",
       "4  [cả, hai, đều, là, một, nhánh, của, cùng, một,...   \n",
       "\n",
       "                                           en_idized  \\\n",
       "0        [8225, 2, 137, 4, 150, 573, 9, 15, 3717, 1]   \n",
       "1  [12, 5080, 264, 7, 20, 4552, 8225, 2, 3006, 9,...   \n",
       "2  [23, 247, 16, 13, 272, 13, 33, 273, 100, 4, 14...   \n",
       "3  [6, 11, 24, 16, 21, 49, 35, 31, 13, 36, 51, 15...   \n",
       "4  [35, 55, 402, 118, 3364, 5, 4, 154, 260, 5, 20...   \n",
       "\n",
       "                                           vi_idized  en_len  vi_len  \n",
       "0                       [7, 1433, 5, 3054, 4, 13, 1]      10       7  \n",
       "1  [23, 812, 253, 8, 1043, 1342, 20, 8510, 2, 161...      51      57  \n",
       "2  [26, 152, 47, 11, 38, 149, 4, 92, 787, 9, 15, ...      28      26  \n",
       "3  [21, 15, 530, 419, 17, 193, 34, 488, 4, 1227, ...      32      25  \n",
       "4  [41, 273, 70, 16, 5, 3618, 9, 178, 5, 198, 23,...      14      16  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_dict = {'train':128,'validate':1, 'train_val':1,'val_train':128, 'test':1}\n",
    "shuffle_dict = {'train':True,'validate':False, 'train_val':False,'val_train':True, 'test':False}\n",
    "# train_used = shuffle_sorted_batches(train_sorted, bs_dict['train'])\n",
    "# train_used = train.iloc[:50]\n",
    "train_used = train\n",
    "val_used = val\n",
    "# val_used = val.iloc[:20]\n",
    "collate_fn_dict = {'train':vocab_collate_func, 'validate':vocab_collate_func_val,\\\n",
    "                   'train_val':vocab_collate_func_val,'val_train':vocab_collate_func,'test': vocab_collate_func_val}\n",
    "transformed_dataset = {'train': Vietnamese(train_used),\n",
    "                       'validate': Vietnamese(val_used, val = True),\n",
    "                       'train_val':Vietnamese(train.iloc[:50], val = True),\n",
    "                       'val_train':Vietnamese(val_used),\n",
    "                       'test':Vietnamese(test, val= True)\n",
    "                                               }\n",
    "\n",
    "dataloader = {x: DataLoader(transformed_dataset[x], batch_size=bs_dict[x], collate_fn=collate_fn_dict[x],\n",
    "                    shuffle=shuffle_dict[x], num_workers=0) for x in ['train', 'validate', 'train_val','val_train', 'test']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Build Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, input_size, embed_dim, hidden_size,n_layers, device = 'cuda'):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.embedding = Embedding(input_size, embed_dim,PAD_IDX)\n",
    "        self.embedding_pos = Embedding(1024, embed_dim,999)\n",
    "        nn.init.uniform_(self.embedding_pos.weight, -0.05, 0.05)\n",
    "        self.dropout_in = nn.Dropout(p = 0.5)\n",
    "        self.n_layers = n_layers\n",
    "        self.kernel_size = 3\n",
    "        self.hids_in_C = [embed_dim]+[hidden_size]*(n_layers-1)\n",
    "        self.hids_out_C = [hidden_size]*n_layers\n",
    "        self.hids_in_A = [embed_dim]+[hidden_size]*(3*n_layers-1)\n",
    "        self.hids_out_A = [hidden_size]*(3*n_layers)\n",
    "        self.convsA = nn.ModuleList()\n",
    "        self.convsC = nn.ModuleList()\n",
    "        self.device = device\n",
    "        for i in range(len(self.hids_out_A)):\n",
    "            self.convsA.append(nn.Conv1d(in_channels=self.hids_in_A[i],out_channels=self.hids_out_A[i],\n",
    "                                        kernel_size=self.kernel_size, padding=self.kernel_size//2))\n",
    "        for i in range(len(self.hids_out_C)):\n",
    "            self.convsC.append(nn.Conv1d(in_channels=self.hids_in_C[i],out_channels=self.hids_out_C[i],\n",
    "                                        kernel_size=self.kernel_size, padding=self.kernel_size//2))\n",
    "            \n",
    "        for layer in self.convsA:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(layer.weight)\n",
    "            nn.init.uniform_(layer.weight,\n",
    "                             (self.kernel_size * fan_in)**(-0.5) * -1,\n",
    "                             (self.kernel_size * fan_in)**(-0.5))\n",
    "\n",
    "        for layer in self.convsC:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(layer.weight)\n",
    "            nn.init.uniform_(layer.weight,\n",
    "                             (self.kernel_size * fan_in)**(-0.5) * -1,\n",
    "                             (self.kernel_size * fan_in)**(-0.5))\n",
    "\n",
    "        # scale gradients\n",
    "        for layer in self.convsA:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(layer.weight)\n",
    "            layer.weight.register_hook(\n",
    "                lambda grad: grad / math.sqrt(fan_in))\n",
    "            layer.bias.register_hook(\n",
    "                lambda grad: grad / math.sqrt(fan_in))\n",
    "\n",
    "        for layer in self.convsC:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(layer.weight)\n",
    "            layer.weight.register_hook(\n",
    "                lambda grad: grad / math.sqrt(fan_in))\n",
    "            layer.bias.register_hook(\n",
    "                lambda grad: grad / math.sqrt(fan_in))\n",
    "            \n",
    "            \n",
    "    def forward(self, enc_inp, src_len):\n",
    "        batch_size, seqlen = enc_inp.size()\n",
    "        out = self.embedding(enc_inp)\n",
    "        pad_mask = enc_inp.eq(PAD_IDX)\n",
    "        pos_seq = torch.zeros_like(enc_inp)\n",
    "        for i in range(batch_size):\n",
    "            pos_seq[i] = torch.arange(0, seqlen)\n",
    "        pos_seq.masked_fill_(pad_mask, 999)\n",
    "        pos_out = self.embedding_pos(pos_seq)\n",
    "        x = out+pos_out\n",
    "        x_input = self.dropout_in(x.transpose(1, 2))\n",
    "        a = x_input\n",
    "        resid_a = torch.zeros([batch_size, self.hidden_size, seqlen]).to(self.device)\n",
    "        resid_c = torch.zeros([batch_size, self.hidden_size, seqlen]).to(self.device)\n",
    "        resids_a = [resid_a]\n",
    "        for op in self.convsA:\n",
    "            resid_a = resids_a[-1]\n",
    "            a = torch.tanh(op(self.dropout_in(a)))\n",
    "            a = a+resid_a\n",
    "            resids_a.append(a)\n",
    "            \n",
    "        final_out_A = a.transpose(2,1)\n",
    "    \n",
    "        c = x_input\n",
    "        resids_c = [resid_c]\n",
    "        for op in self.convsC:\n",
    "            resid_c = resids_c[-1]\n",
    "            c = torch.tanh(op(self.dropout_in(c)))\n",
    "            c = c+resid_c\n",
    "            resids_c.append(c)\n",
    "        final_out_C = c.transpose(2,1)\n",
    "        return (final_out_A, final_out_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention_Module(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim, device = 'cuda'):\n",
    "        super(Attention_Module, self).__init__()\n",
    "        self.l1 = Linear(hidden_dim, output_dim, bias = False)\n",
    "        self.l2 = Linear(hidden_dim+output_dim, output_dim, bias =  False)\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, hidden, encoder_outs, src_lens):\n",
    "        ''' hiddden: bsz x hidden_dim\n",
    "        encoder_outs: bsz x sq_len x encoder dim (output_dim)\n",
    "        src_lens: bsz\n",
    "        \n",
    "        x: bsz x output_dim\n",
    "        attn_score: bsz x sq_len'''\n",
    "        x = self.l1(hidden)\n",
    "        att_score = (encoder_outs.transpose(0,1) * x.unsqueeze(0)).sum(dim = 2)\n",
    "        seq_mask = sequence_mask(src_lens, max_len = max(src_lens).item(), device = self.device).transpose(0,1)\n",
    "        masked_att = seq_mask*att_score\n",
    "        masked_att[masked_att==0] = -1e10\n",
    "        attn_scores = F.softmax(masked_att, dim=0)\n",
    "        x = (attn_scores.unsqueeze(2) * encoder_outs.transpose(0,1)).sum(dim=0)\n",
    "        x = torch.tanh(self.l2(torch.cat((x, hidden), dim=1)))\n",
    "        return x, attn_scores\n",
    "        \n",
    "class AttentionDecoderRNN(nn.Module):\n",
    "    def __init__(self, output_size, embed_dim, hidden_size, n_layers = 1, attention = True, device = 'cuda'):\n",
    "        super(AttentionDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        encoder_output_size = hidden_size\n",
    "        self.embedding = Embedding(output_size, embed_dim, PAD_IDX)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.n_layers = n_layers\n",
    "        self.device = device\n",
    "        self.att_layer = Attention_Module(self.hidden_size, encoder_output_size,self.device) if attention else None\n",
    "        self.layers = nn.ModuleList([\n",
    "            LSTMCell(\n",
    "                input_size=self.hidden_size + embed_dim if ((layer == 0) and attention) else embed_dim if layer == 0 else hidden_size,\n",
    "                hidden_size=hidden_size,\n",
    "            )\n",
    "            for layer in range(self.n_layers)\n",
    "        ])\n",
    "        self.fc_out = nn.Linear(self.hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, dec_input,context_vector, prev_hiddens,prev_cs,encoder_outputs,src_len):\n",
    "        bsz = dec_input.size(0)\n",
    "        output = self.embedding(dec_input)\n",
    "        output = self.dropout(output)\n",
    "        if self.att_layer is not None:\n",
    "            cated_input = torch.cat([output.squeeze(1),context_vector], dim = 1)\n",
    "        else:\n",
    "            cated_input = output.squeeze(1)\n",
    "        new_hiddens = []\n",
    "        new_cs = []\n",
    "        for i, rnn in enumerate(self.layers):\n",
    "            hidden, c = rnn(cated_input, (prev_hiddens[i], prev_cs[i]))\n",
    "            cated_input = self.dropout(hidden)\n",
    "            new_hiddens.append(hidden.unsqueeze(0))\n",
    "            new_cs.append(c.unsqueeze(0))\n",
    "        new_hiddens = torch.cat(new_hiddens, dim = 0)\n",
    "        new_cs = torch.cat(new_cs, dim = 0)\n",
    "\n",
    "        # apply attention using the last layer's hidden state\n",
    "        if self.att_layer is not None:\n",
    "            out, attn_score = self.att_layer(hidden, encoder_outputs, src_len)\n",
    "        else:\n",
    "            out = hidden\n",
    "            attn_score = None\n",
    "        context_vec = out\n",
    "        out = self.dropout(out)\n",
    "        out_vocab = self.softmax(self.fc_out(out))\n",
    "\n",
    "        return out_vocab, context_vec, new_hiddens, new_cs, attn_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Utilites functions, training and evaluation functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Embedding(num_embeddings, embedding_dim, padding_idx):\n",
    "    m = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_idx)\n",
    "    nn.init.uniform_(m.weight, -0.1, 0.1)\n",
    "    nn.init.constant_(m.weight[padding_idx], 0)\n",
    "    return m\n",
    "\n",
    "\n",
    "def LSTM(input_size, hidden_size, **kwargs):\n",
    "    m = nn.LSTM(input_size, hidden_size,**kwargs)\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name or 'bias' in name:\n",
    "            param.data.uniform_(-0.1, 0.1)\n",
    "    return m\n",
    "\n",
    "\n",
    "def LSTMCell(input_size, hidden_size, **kwargs):\n",
    "    m = nn.LSTMCell(input_size, hidden_size,**kwargs)\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name or 'bias' in name:\n",
    "            param.data.uniform_(-0.1, 0.1)\n",
    "    return m\n",
    "\n",
    "\n",
    "def Linear(in_features, out_features, bias=True, dropout=0):\n",
    "    \"\"\"Linear layer (input: N x T x C)\"\"\"\n",
    "    m = nn.Linear(in_features, out_features, bias=bias)\n",
    "    m.weight.data.uniform_(-0.1, 0.1)\n",
    "    if bias:\n",
    "        m.bias.data.uniform_(-0.1, 0.1)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_mask(sequence_length, max_len=None, device = 'cuda'):\n",
    "    if max_len is None:\n",
    "        max_len = sequence_length.max().item()\n",
    "    batch_size = sequence_length.size(0)\n",
    "    seq_range = torch.arange(0, max_len).long()\n",
    "    seq_range_expand = seq_range.unsqueeze(0).repeat([batch_size,1])\n",
    "    seq_range_expand = seq_range_expand.to(device)\n",
    "    seq_length_expand = (sequence_length.unsqueeze(1)\n",
    "                         .expand_as(seq_range_expand))\n",
    "    return (seq_range_expand < seq_length_expand).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_decode_cnn(encoder,decoder,data_en,data_de,src_len,tar_len,rand_num = 0.95, val = False):\n",
    "    if not val:\n",
    "        use_teacher_forcing = True if random.random() < rand_num else False\n",
    "    #     print(\"tar_len\",tar_len)\n",
    "        bss = data_en.size(0)\n",
    "        en_out = encoder(data_en, src_len)\n",
    "        max_src_len_batch = max(src_len).item()\n",
    "        max_tar_len_batch = max(tar_len).item()\n",
    "        prev_hiddens, prev_cs = decoder.initHidden(bss)\n",
    "#         n_layers, _, _ = prev_hiddens.size()\n",
    "#         print(n_layers)\n",
    "#         prev_hiddens = en_out[1].sum(1).unsqueeze(0).repeat(n_layers,1,1)\n",
    "#         prev_cs = prev_hiddens\n",
    "        prev_output = torch.zeros((bss, en_out[0].size(-1))).to(device)\n",
    "#         prev_output = en_out[1][:,0,:]\n",
    "        decoder_input = torch.tensor([[SOS_token]]*bss).to(device)\n",
    "        if use_teacher_forcing:\n",
    "            d_out = []\n",
    "            for i in range(max_tar_len_batch):\n",
    "                out_vocab, prev_output,prev_hiddens, prev_cs, attention_score = decoder(decoder_input,prev_output, \\\n",
    "                                                                                        prev_hiddens,prev_cs, en_out,\\\n",
    "                                                                                        src_len)\n",
    "                d_out.append(out_vocab.unsqueeze(-1))\n",
    "                decoder_input = data_de[:,i].view(-1,1)\n",
    "#                 print(decoder_input)\n",
    "            d_out = torch.cat(d_out,dim=-1)\n",
    "    #         print(\"softmax mat\", d_out.shape)\n",
    "        else:\n",
    "            d_out = []\n",
    "            for i in range(max_tar_len_batch):\n",
    "                out_vocab, prev_output,prev_hiddens, prev_cs, attention_score = decoder(decoder_input,prev_output, \\\n",
    "                                                                                        prev_hiddens,prev_cs, en_out,\\\n",
    "                                                                                        src_len)\n",
    "                d_out.append(out_vocab.unsqueeze(-1))\n",
    "                topv, topi = out_vocab.topk(1)\n",
    "                decoder_input = topi.squeeze().detach().view(-1,1)\n",
    "#                 print(decoder_input)\n",
    "                decoder_input = topi.squeeze().view(-1,1)\n",
    "            d_out = torch.cat(d_out,dim=-1)\n",
    "        return d_out\n",
    "    else:\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        bss = data_en.size(0)\n",
    "        en_out = encoder(data_en, src_len)\n",
    "        max_src_len_batch = max(src_len).item()\n",
    "        max_tar_len_batch = max(tar_len).item()\n",
    "        prev_hiddens, prev_cs = decoder.initHidden(bss)\n",
    "#         prev_hiddens = enc_c.sum(1)\n",
    "#         prev_cs = prev_hiddens\n",
    "#         prev_hiddens, prev_cs = decoder.initHidden(bss)\n",
    "        decoder_input = torch.tensor([[SOS_token]]*bss).to(device)\n",
    "        prev_output = torch.zeros((bss, en_out[0].size(-1))).to(device)\n",
    "#         prev_output = en_out[1][:,0,:]\n",
    "        d_out = []\n",
    "        for i in range(max_tar_len_batch):\n",
    "            out_vocab, prev_output,prev_hiddens, prev_cs, attention_score = decoder(decoder_input,prev_output, \\\n",
    "                                                                                        prev_hiddens,prev_cs, en_out,\\\n",
    "                                                                                        src_len)\n",
    "            d_out.append(out_vocab.unsqueeze(-1))\n",
    "            topv, topi = out_vocab.topk(1)\n",
    "            decoder_input = topi.squeeze().detach().view(-1,1)\n",
    "#             decoder_input = topi.squeeze().view(-1,1)\n",
    "        d_out = torch.cat(d_out,dim=-1)\n",
    "        return d_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_new_cnn(encoder, decoder, val_dataloader, lang_en,m_type, verbose = False, replace_unk =True):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    pred_corpus = []\n",
    "    true_corpus = []\n",
    "    running_loss = 0\n",
    "    running_total = 0\n",
    "    bl = BLEU_SCORE()\n",
    "    for data in val_dataloader:\n",
    "        encoder_i = data[0].to(device)\n",
    "        src_len = data[2].to(device)\n",
    "        bs,sl = encoder_i.size()[:2]\n",
    "        en_out = encoder(encoder_i,src_len)\n",
    "        max_src_len_batch = max(src_len).item()\n",
    "        prev_hiddens, prev_cs = decoder.initHidden(bs)\n",
    "#         n_layers, _, _ = prev_hiddens.size()\n",
    "#         prev_hiddens = en_out[1].sum(1).unsqueeze(0).repeat(n_layers,1,1)\n",
    "#         prev_cs = prev_hiddens\n",
    "        decoder_input = torch.tensor([[SOS_token]]*bs).to(device)\n",
    "#         prev_output = en_out[1][:,0,:]\n",
    "        prev_output = torch.zeros((bs, en_out[0].size(-1))).to(device)\n",
    "        d_out = []\n",
    "        for i in range(sl*2):\n",
    "            out_vocab, prev_output,prev_hiddens, prev_cs, attention_score = decoder(decoder_input,prev_output, \\\n",
    "                                                                                        prev_hiddens,prev_cs, en_out,\\\n",
    "                                                                                        src_len)\n",
    "            topv, topi = out_vocab.topk(1)\n",
    "#             decoder_input = topi.squeeze().detach().view(-1,1)\n",
    "            d_out.append(topi.item())\n",
    "            decoder_input = topi.squeeze().detach().view(-1,1)\n",
    "            if topi.item() == EOS_token:\n",
    "                break\n",
    "        \n",
    "\n",
    "#         true_sent = convert_id_list_2_sent(data[1][0],lang_en)\n",
    "#         true_corpus.append(true_sent)\n",
    "        if replace_unk:\n",
    "            true_sent = convert_id_list_2_sent(data[1][0],lang_en)\n",
    "            true_corpus.append(true_sent)\n",
    "        else:\n",
    "            true_corpus.append(data[-1])\n",
    "        pred_sent = convert_id_list_2_sent(d_out,lang_en)\n",
    "        pred_corpus.append(pred_sent)\n",
    "        if verbose:\n",
    "            print(\"True Sentence:\",data[-1])\n",
    "            print(\"Pred Sentence:\", pred_sent)\n",
    "            print('-*'*50)\n",
    "    score = bl.corpus_bleu(pred_corpus,[true_corpus],lowercase=True)[0]\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(encoder_optimizer,decoder_optimizer, encoder, decoder, loss_fun,m_type, dataloader, en_lang,\\\n",
    "                num_epochs=60, val_every = 1, train_bleu_every = 10,clip = 0.1, rm = 0.8, enc_scheduler = None,\\\n",
    "               dec_scheduler = None, enc_dec_fn = encode_decode_cnn, val_fn = validation_new_cnn):\n",
    "    best_score = 0\n",
    "    best_bleu = 0\n",
    "    loss_hist = {'train': [], 'val_train': []}\n",
    "    bleu_hist = {'train': [], 'validate': []}\n",
    "    best_encoder_wts = None\n",
    "    best_decoder_wts = None\n",
    "    phases = ['train','val_train']\n",
    "    for epoch in range(num_epochs):\n",
    "        for ex, phase in enumerate(phases):\n",
    "            start = time.time()\n",
    "            total = 0\n",
    "            top1_correct = 0\n",
    "            running_loss = 0\n",
    "            running_total = 0\n",
    "            if phase == 'train':\n",
    "                encoder.train()\n",
    "                decoder.train()\n",
    "            else:\n",
    "                encoder.eval()\n",
    "                decoder.eval()\n",
    "            for data in dataloader[phase]:\n",
    "                encoder_optimizer.zero_grad()\n",
    "                decoder_optimizer.zero_grad()\n",
    "\n",
    "                encoder_i = data[0].to(device)\n",
    "                decoder_i = data[1].to(device)\n",
    "                src_len = data[2].to(device)\n",
    "                tar_len = data[3].to(device)\n",
    "                if phase == 'val_train':  \n",
    "                    out = enc_dec_fn(encoder,decoder,encoder_i,decoder_i,src_len,tar_len,rand_num=rm,val = True)\n",
    "                else:\n",
    "                    out = enc_dec_fn(encoder,decoder,encoder_i,decoder_i,src_len,tar_len,rand_num=rm,val = False)\n",
    "                N = decoder_i.size(0)\n",
    "                loss = loss_fun(out.float(), decoder_i.long())\n",
    "                running_loss += loss.item() * N\n",
    "                \n",
    "                total += N\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "                    torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "                    encoder_optimizer.step()\n",
    "                    decoder_optimizer.step()\n",
    "                    \n",
    "            epoch_loss = running_loss / total \n",
    "            loss_hist[phase].append(epoch_loss)\n",
    "            print(\"epoch {} {} loss = {}, time = {}\".format(epoch, phase, epoch_loss,\n",
    "                                                                           time.time() - start))\n",
    "            \n",
    "        if (enc_scheduler is not None) and (dec_scheduler is not None):\n",
    "            enc_scheduler.step(epoch_loss)\n",
    "            dec_scheduler.step(epoch_loss)\n",
    "        if epoch%val_every == 0:\n",
    "            val_bleu_score = val_fn(encoder,decoder, dataloader['validate'], en_lang, m_type)\n",
    "            bleu_hist['validate'].append(val_bleu_score)\n",
    "            print(\"validation BLEU = \", val_bleu_score)\n",
    "            if val_bleu_score > best_bleu:\n",
    "                best_bleu = val_bleu_score\n",
    "                best_encoder_wts = encoder.state_dict()\n",
    "                best_decoder_wts = decoder.state_dict()\n",
    "        print('='*50)\n",
    "    encoder.load_state_dict(best_encoder_wts)\n",
    "    decoder.load_state_dict(best_decoder_wts)\n",
    "    print(\"Training completed. Best BLEU is {}\".format(best_bleu))\n",
    "    return encoder,decoder,loss_hist,bleu_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.sans-serif'] = ['SimHei']\n",
    "mpl.rcParams['font.serif'] = ['SimHei']\n",
    "seaborn.set_style(\"darkgrid\",{\"Droid Sans Fallback\":['simhei', 'Arial']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw(att_mat, x, y):\n",
    "    x = x.split(' ')\n",
    "    y = y.split(' ')\n",
    "    x += ['EOS']\n",
    "    y += ['EOS']\n",
    "    att_mat = att_mat.data.cpu().squeeze(1).numpy()\n",
    "    seaborn.heatmap(att_mat, \n",
    "                    xticklabels=x, square=True, yticklabels=y, vmin=0.0, vmax=1.0, \n",
    "                    cbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Without attention (Training and Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ark576/.conda/envs/nlp/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "enc_cnn = EncoderCNN(vi_lang.n_words, 512,512,2).to(device)\n",
    "dec_cnn = AttentionDecoderRNN(en_lang.n_words, 512, 512, n_layers=1,attention=True,\\\n",
    "                              device = 'cuda', enc_type = 'conv').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_optimizer = optim.Adam(encoder_wo_att.parameters(), lr = 5e-3)\n",
    "# decoder_optimizer = optim.Adam(decoder_wo_att.parameters(), lr = 5e-3)\n",
    "encoder_optimizer = optim.SGD(encoder_wo_att.parameters(), lr=0.25,nesterov=True, momentum = 0.99)\n",
    "enc_scheduler = ReduceLROnPlateau(encoder_optimizer, min_lr=1e-4,factor = 0.5,  patience=0)\n",
    "decoder_optimizer = optim.SGD(decoder_wo_att.parameters(), lr=0.25,nesterov=True, momentum = 0.99)\n",
    "dec_scheduler = ReduceLROnPlateau(decoder_optimizer, min_lr=1e-4,factor = 0.5,  patience=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "enc_cnn, dec_cnn, loss_hist, acc_hist = train_model(encoder_optimizer, decoder_optimizer, enc_cnn, dec_cnn, criterion,\\\n",
    "                                            \"attention\", dataloader,en_lang, num_epochs = 150,rm = 0.95,\\\n",
    "                                           enc_scheduler = None, dec_scheduler = None, clip = 0.1, \\\n",
    "                                                    enc_dec_fn = encode_decode_cnn_2, val_fn = validation_new_cnn_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(enc_cnn.state_dict(), 'conv_enc.pth')\n",
    "torch.save(dec_cnn.state_dict(), 'conv_dec.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
