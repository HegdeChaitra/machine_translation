{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import random\n",
    "import pdb\n",
    "from torch.utils.data import DataLoader\n",
    "import logging\n",
    "import itertools\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import argparse\n",
    "from torch import optim\n",
    "import time\n",
    "import os\n",
    "from bleu_score import BLEU_SCORE\n",
    "# from models_viet import EncoderRNN, AttentionDecoderRNN, DecoderRNN\n",
    "from load_dataset_viet import *\n",
    "# from define_training_viet import *\n",
    "import torchtext\n",
    "from torch.utils.data import BatchSampler\n",
    "from torch.utils.data import SequentialSampler\n",
    "from torch.utils.data import Sampler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vietnamese(Dataset):\n",
    "    def __init__(self, df, val = False):\n",
    "        self.df = df\n",
    "        self.val = val\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        english = self.df.iloc[idx,:]['en_idized']\n",
    "        viet = self.df.iloc[idx,:]['vi_idized']\n",
    "        en_len = self.df.iloc[idx,:]['en_len']\n",
    "        vi_len = self.df.iloc[idx,:]['vi_len']\n",
    "        if self.val:\n",
    "            en_data = self.df.iloc[idx,:]['en_data'].lower()\n",
    "            return [viet,english,vi_len,en_len,en_data]\n",
    "        else:\n",
    "            return [viet,english,vi_len,en_len]\n",
    "    \n",
    "    \n",
    "def vocab_collate_func(batch):\n",
    "    MAX_LEN_EN = 48\n",
    "    MAX_LEN_VI = 48\n",
    "    en_data = []\n",
    "    vi_data = []\n",
    "    en_len = []\n",
    "    vi_len = []\n",
    "    for datum in batch:\n",
    "        en_len.append(datum[3])\n",
    "        vi_len.append(datum[2])\n",
    "    max_batch_length_en = max(en_len)\n",
    "    max_batch_length_vi = max(vi_len)\n",
    "    if max_batch_length_en < MAX_LEN_EN:\n",
    "        MAX_LEN_EN = max_batch_length_en\n",
    "    if max_batch_length_vi < MAX_LEN_VI:\n",
    "        MAX_LEN_VI = max_batch_length_vi\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        if datum[2]>MAX_LEN_VI:\n",
    "            padded_vec_s1 = np.array(datum[0])[:MAX_LEN_VI]\n",
    "        else:\n",
    "            padded_vec_s1 = np.pad(np.array(datum[0]),\n",
    "                                pad_width=((0,MAX_LEN_VI - datum[2])),\n",
    "                                mode=\"constant\", constant_values=PAD_IDX)\n",
    "        if datum[3]>MAX_LEN_EN:\n",
    "            padded_vec_s2 = np.array(datum[1])[:MAX_LEN_EN]\n",
    "        else:\n",
    "            padded_vec_s2 = np.pad(np.array(datum[1]),\n",
    "                                pad_width=((0,MAX_LEN_EN - datum[3])),\n",
    "                                mode=\"constant\", constant_values=PAD_IDX)\n",
    "        en_data.append(padded_vec_s2)\n",
    "        vi_data.append(padded_vec_s1)\n",
    "    vi_data = np.array(vi_data)\n",
    "    en_data = np.array(en_data)\n",
    "    vi_len = np.array(vi_len)\n",
    "    en_len = np.array(en_len)\n",
    "#     sorted_vi_len = np.argsort(-vi_len)\n",
    "#     vi_data = vi_data[sorted_vi_len]\n",
    "#     en_data = en_data[sorted_vi_len]\n",
    "#     vi_len = vi_len[sorted_vi_len]\n",
    "#     en_len = en_len[sorted_vi_len]\n",
    "#     print(en_len)\n",
    "    vi_len[vi_len>MAX_LEN_VI] = MAX_LEN_VI\n",
    "    en_len[en_len>MAX_LEN_EN] = MAX_LEN_EN\n",
    "        \n",
    "    return [torch.from_numpy(vi_data), torch.from_numpy(en_data),\n",
    "            torch.from_numpy(vi_len), torch.from_numpy(en_len)]\n",
    "\n",
    "def convert_idx_2_sent(tensor, lang_obj):\n",
    "    word_list = []\n",
    "    for i in tensor:\n",
    "        if i.item() not in set([PAD_IDX,EOS_token,SOS_token]):\n",
    "            word_list.append(lang_obj.index2word[i.item()])\n",
    "    return (' ').join(word_list)\n",
    "\n",
    "def convert_id_list_2_sent(list_idx, lang_obj):\n",
    "    word_list = []\n",
    "    if type(list_idx) == list:\n",
    "        for i in list_idx:\n",
    "            if i not in set([EOS_token]):\n",
    "                word_list.append(lang_obj.index2word[i])\n",
    "    else:\n",
    "        for i in list_idx:\n",
    "            if i.item() not in set([EOS_token,SOS_token,PAD_IDX]):\n",
    "                word_list.append(lang_obj.index2word[i.item()])\n",
    "    return (' ').join(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_collate_func_val(batch):\n",
    "    return [torch.from_numpy(np.array(batch[0][0])).unsqueeze(0), torch.from_numpy(np.array(batch[0][1])).unsqueeze(0),\n",
    "            torch.from_numpy(np.array(batch[0][2])).unsqueeze(0), torch.from_numpy(np.array(batch[0][3])).unsqueeze(0),batch[0][4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_LEN = 48\n",
    "train,val,en_lang,vi_lang = train_val_load(48, \"\", '/scratch/ark576/machine_translation_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_sorted_batches(train, bs):\n",
    "    batch_samp_list = list(BatchSampler(SequentialSampler(train), bs, drop_last = False))\n",
    "    np.random.shuffle(batch_samp_list)\n",
    "    batch_samp_list_merged = list(itertools.chain(*batch_samp_list))\n",
    "    return train.iloc[batch_samp_list_merged,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en_data</th>\n",
       "      <th>vi_data</th>\n",
       "      <th>en_tokenized</th>\n",
       "      <th>vi_tokenized</th>\n",
       "      <th>en_idized</th>\n",
       "      <th>vi_idized</th>\n",
       "      <th>en_len</th>\n",
       "      <th>vi_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rachel Pike : The science behind a climate hea...</td>\n",
       "      <td>Khoa_học đằng_sau một tiêu_đề về khí_hậu</td>\n",
       "      <td>[rachel, pike, :, the, science, behind, a, cli...</td>\n",
       "      <td>[khoa_học, đằng_sau, một, tiêu_đề, về, khí_hậu]</td>\n",
       "      <td>[8225, 2, 137, 4, 150, 573, 9, 15, 3717, 1]</td>\n",
       "      <td>[7, 1433, 5, 3054, 4, 13, 1]</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In 4 minutes , atmospheric chemist Rachel Pike...</td>\n",
       "      <td>Trong 4 phút , chuyên_gia hoá_học khí_quyển Ra...</td>\n",
       "      <td>[in, 4, minutes, ,, atmospheric, chemist, rach...</td>\n",
       "      <td>[trong, 4, phút, ,, chuyên_gia, hoá_học, khí_q...</td>\n",
       "      <td>[12, 5080, 264, 7, 20, 4552, 8225, 2, 3006, 9,...</td>\n",
       "      <td>[23, 812, 253, 8, 1043, 1342, 20, 8510, 2, 161...</td>\n",
       "      <td>51</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I &amp;apos;d like to talk to you today about the ...</td>\n",
       "      <td>Tôi muốn cho các bạn biết về sự to_lớn của nhữ...</td>\n",
       "      <td>[i, &amp;apos;d, like, to, talk, to, you, today, a...</td>\n",
       "      <td>[tôi, muốn, cho, các, bạn, biết, về, sự, to_lớ...</td>\n",
       "      <td>[23, 247, 16, 13, 272, 13, 33, 273, 100, 4, 14...</td>\n",
       "      <td>[26, 152, 47, 11, 38, 149, 4, 92, 787, 9, 15, ...</td>\n",
       "      <td>28</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Headlines that look like this when they have t...</td>\n",
       "      <td>Có những dòng trông như thế_này khi bàn về biế...</td>\n",
       "      <td>[headlines, that, look, like, this, when, they...</td>\n",
       "      <td>[có, những, dòng, trông, như, thế_này, khi, bà...</td>\n",
       "      <td>[6, 11, 24, 16, 21, 49, 35, 31, 13, 36, 51, 15...</td>\n",
       "      <td>[21, 15, 530, 419, 17, 193, 34, 488, 4, 1227, ...</td>\n",
       "      <td>32</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>They are both two branches of the same field o...</td>\n",
       "      <td>Cả hai đều là một nhánh của cùng một lĩnh_vực ...</td>\n",
       "      <td>[they, are, both, two, branches, of, the, same...</td>\n",
       "      <td>[cả, hai, đều, là, một, nhánh, của, cùng, một,...</td>\n",
       "      <td>[35, 55, 402, 118, 3364, 5, 4, 154, 260, 5, 20...</td>\n",
       "      <td>[41, 273, 70, 16, 5, 3618, 9, 178, 5, 198, 23,...</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             en_data  \\\n",
       "0  Rachel Pike : The science behind a climate hea...   \n",
       "1  In 4 minutes , atmospheric chemist Rachel Pike...   \n",
       "2  I &apos;d like to talk to you today about the ...   \n",
       "3  Headlines that look like this when they have t...   \n",
       "4  They are both two branches of the same field o...   \n",
       "\n",
       "                                             vi_data  \\\n",
       "0           Khoa_học đằng_sau một tiêu_đề về khí_hậu   \n",
       "1  Trong 4 phút , chuyên_gia hoá_học khí_quyển Ra...   \n",
       "2  Tôi muốn cho các bạn biết về sự to_lớn của nhữ...   \n",
       "3  Có những dòng trông như thế_này khi bàn về biế...   \n",
       "4  Cả hai đều là một nhánh của cùng một lĩnh_vực ...   \n",
       "\n",
       "                                        en_tokenized  \\\n",
       "0  [rachel, pike, :, the, science, behind, a, cli...   \n",
       "1  [in, 4, minutes, ,, atmospheric, chemist, rach...   \n",
       "2  [i, &apos;d, like, to, talk, to, you, today, a...   \n",
       "3  [headlines, that, look, like, this, when, they...   \n",
       "4  [they, are, both, two, branches, of, the, same...   \n",
       "\n",
       "                                        vi_tokenized  \\\n",
       "0    [khoa_học, đằng_sau, một, tiêu_đề, về, khí_hậu]   \n",
       "1  [trong, 4, phút, ,, chuyên_gia, hoá_học, khí_q...   \n",
       "2  [tôi, muốn, cho, các, bạn, biết, về, sự, to_lớ...   \n",
       "3  [có, những, dòng, trông, như, thế_này, khi, bà...   \n",
       "4  [cả, hai, đều, là, một, nhánh, của, cùng, một,...   \n",
       "\n",
       "                                           en_idized  \\\n",
       "0        [8225, 2, 137, 4, 150, 573, 9, 15, 3717, 1]   \n",
       "1  [12, 5080, 264, 7, 20, 4552, 8225, 2, 3006, 9,...   \n",
       "2  [23, 247, 16, 13, 272, 13, 33, 273, 100, 4, 14...   \n",
       "3  [6, 11, 24, 16, 21, 49, 35, 31, 13, 36, 51, 15...   \n",
       "4  [35, 55, 402, 118, 3364, 5, 4, 154, 260, 5, 20...   \n",
       "\n",
       "                                           vi_idized  en_len  vi_len  \n",
       "0                       [7, 1433, 5, 3054, 4, 13, 1]      10       7  \n",
       "1  [23, 812, 253, 8, 1043, 1342, 20, 8510, 2, 161...      51      57  \n",
       "2  [26, 152, 47, 11, 38, 149, 4, 92, 787, 9, 15, ...      28      26  \n",
       "3  [21, 15, 530, 419, 17, 193, 34, 488, 4, 1227, ...      32      25  \n",
       "4  [41, 273, 70, 16, 5, 3618, 9, 178, 5, 198, 23,...      14      16  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_dict = {'train':128,'validate':1, 'train_val':1,'val_train':128}\n",
    "# train_used = shuffle_sorted_batches(train_sorted, bs_dict['train'])\n",
    "# train_used = train.iloc[:50]\n",
    "train_used = train\n",
    "collate_fn_dict = {'train':vocab_collate_func, 'validate':vocab_collate_func_val,\\\n",
    "                   'train_val':vocab_collate_func_val,'val_train':vocab_collate_func}\n",
    "transformed_dataset = {'train': Vietnamese(train_used),\n",
    "                       'validate': Vietnamese(val, val = True),\n",
    "                       'train_val':Vietnamese(train.iloc[:50], val = True),\n",
    "                       'val_train':Vietnamese(val)\n",
    "                                               }\n",
    "\n",
    "dataloader = {x: DataLoader(transformed_dataset[x], batch_size=bs_dict[x], collate_fn=collate_fn_dict[x],\n",
    "                    shuffle=True, num_workers=0) for x in ['train', 'validate', 'train_val','val_train']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_sample = next(iter(dataloader['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 48])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_sample[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 48])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_sample[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12,  6, 33, 44, 16, 22, 42, 12, 48, 10])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_sample[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10,  8, 47, 37, 19, 27, 41, 17, 48,  9])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_sample[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: here in america , people actually love tv . EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "Viet: đây ở nước mỹ , mọi người thực_sự yêu_thích tivi . EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "English: three hundred UKN members of parliament . EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "Viet: nghị_viện có UKN thành_viên . EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "English: the first thing i did was i built a UKN puzzle in which these questions and answers are coded in the form of shapes , in the form of colors , and you have people putting these together and trying to understand how this works . EOS PAD\n",
      "Viet: đầu_tiên là xây_dựng một trò_chơi xếp hình trong đó , câu_hỏi và trả_lời được mã_hoá dưới dạng hình_khối và màu_sắc , mọi người phải sắp_xếp chúng lại với nhau và tìm_ra lời_giải EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "English: and if you had an injured organ , if you had a heart attack and we wanted to repair that injured area , do you want those robust , plentiful stem cells on the top ? EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "Viet: và nếu các bạn có_một cơ_quan nào_đó bị tổn_thương , nếu các bạn lên_cơn đau tim và chúng_tôi muốn hồi_phục những vùng bị tổn_thương đó , thì liệu các bạn có muốn những tế_bào mầm kia khoẻ_mạnh , và dồi_dào hơn không ? EOS PAD PAD PAD PAD\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "English: the language of technology is binary ; you &apos;ve probably heard that at some point in time . EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "Viet: ngôn_ngữ của công_nghệ là hệ nhị_phân ; bạn có_thể đã biết về điều này . EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "English: it is also important for the environment , because the world &apos;s energy requirements are expected to grow by 30 percent in the next decade . EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "Viet: nó cũng quan_trọng đối_với môi_trường bởi_vì nhu_cầu năng_lượng của toàn thế_giới dự_kiến sẽ tăng 30 % trong 10 năm tới . EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "English: so for example , in UKN paintings , it ranges from this , which is kind of old masters style , to really realistic UKN , to this type of painting where i &apos;m painting with a single hair . EOS PAD PAD PAD PAD PAD PAD PAD\n",
      "Viet: chẳng_hạn , trong các bức tranh theo chủ_nghĩa hiện_thực , nó trải dài trong phạm_vi từ kiểu phong_cách chủ_đạo cũ cho_đến tranh UKN đầy hiện_thực , cho đến thể_loại tranh mà tôi chi vẽ bằng một sợi cọ duy_nhất này . EOS PAD PAD PAD PAD PAD PAD\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "English: and interval research is kind of the living example of how that can be true . EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "Viet: và interval research là một bằng_chứng sống của sự_thật đó . EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "English: in a teaching -- called a &quot; UKN UKN &quot; in our tradition -- god says that , &quot; my servant , &quot; or &quot; my creature , my human creature , does not approach me by anything that is UKN to me than what i have asked\n",
      "Viet: trong một bài_giảng , gọi_là UKN UKN trong truyền_thống của chúng_tôi chúa nói rằng , \" người phục_vụ của ta \" hay là \" sinh_vật của ta , sinh_vật loài_người của ta đừng tiếp_cận ta bằng bất_cứ thứ gì thân_thiết đối_với ta hơn_là những gì ta bảo chúng\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "English: another example , a beautiful thing happened . EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "Viet: một ví_dụ khác , một điều tuyệt_vời đã xảy_ra EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n"
     ]
    }
   ],
   "source": [
    "for t in zip(train_data_sample[1],train_data_sample[0]):\n",
    "    en_sent = []\n",
    "    vi_sent = []\n",
    "    for i in t[0]:\n",
    "        en_sent.append(en_lang.index2word[i.item()])\n",
    "    for i in t[1]:\n",
    "        vi_sent.append(vi_lang.index2word[i.item()])\n",
    "    print('English:',(' ').join(en_sent))\n",
    "    print('Viet:',(' ').join(vi_sent))\n",
    "    print('-*'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13294"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vi_lang.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16031"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_lang.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class OldEncoderRNN(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size,bi):\n",
    "#         super(OldEncoderRNN, self).__init__()\n",
    "#         self.bi=bi\n",
    "#         if self.bi:\n",
    "#             self.mul=2\n",
    "#         else:\n",
    "#             self.mul=1\n",
    "#         self.hidden_size = hidden_size\n",
    "\n",
    "#         self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "#         self.gru = nn.GRU(hidden_size, hidden_size,batch_first=True,bidirectional=self.bi)\n",
    "\n",
    "#     def forward(self, input, hidden):\n",
    "#         embedded = self.embedding(input)\n",
    "#         output = embedded\n",
    "#         output, hidden = self.gru(output, hidden)\n",
    "#         last_layer = output[:,-1,:self.hidden_size]\n",
    "#         first_layer = output[:,0,self.hidden_size:]\n",
    "#         hidden = torch.cat([first_layer,last_layer],dim=1).unsqueeze(0)\n",
    "#         return output, hidden\n",
    "\n",
    "#     def initHidden(self,bs):\n",
    "#         return torch.zeros(self.mul, bs, self.hidden_size).to(device)\n",
    "    \n",
    "    \n",
    "# class OldAttentionDecoderRNN(nn.Module):\n",
    "#     def __init__(self, hidden_size, output_size,bi, MAX_LEN,n_layers = 1, attention_type = None):\n",
    "#         super(OldAttentionDecoderRNN, self).__init__()\n",
    "#         self.mul=2\n",
    "#         self.hidden_size = hidden_size\n",
    "\n",
    "#         self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "#         self.dropout = nn.Dropout(p=0.2)\n",
    "#         self.gru = nn.GRU(hidden_size*2 + hidden_size, hidden_size*2,batch_first=True, num_layers = n_layers)\n",
    "        \n",
    "#         self.attention_type = attention_type\n",
    "#         if self.attention_type is not None:\n",
    "#             self.attn = nn.Linear(self.hidden_size*2, self.hidden_size*2)\n",
    "#             self.attn_drop = nn.Dropout(p = 0.5)\n",
    "#         else:\n",
    "#             self.attn = nn.Linear(self.hidden_size * 2, MAX_LEN)\n",
    "        \n",
    "# #         self.attn_combine = nn.Linear(self.hidden_size * self.mul+self.hidden_size, self.hidden_size)\n",
    "        \n",
    "#         self.out = nn.Linear(self.mul*hidden_size, output_size)\n",
    "#         self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "#     def forward(self, input, hidden,encoder_outputs):\n",
    "#         bss = input.size(0)\n",
    "#         output = self.embedding(input)\n",
    "#         output = self.dropout(output)\n",
    "#         att_out = self.attn_drop(self.attn(hidden[-1])).unsqueeze(-1)\n",
    "#         if self.attention_type is not None:\n",
    "#             attn_wts = F.softmax(torch.bmm(encoder_outputs,att_out),dim = 1)\n",
    "#             attn_applied = torch.bmm(encoder_outputs.transpose(1,2),attn_wts).transpose(1,2)\n",
    "#         else:\n",
    "#             att_out = F.softmax(self.attn(cat),dim=1)\n",
    "#             attn_applied = torch.bmm(att_out,encoder_outputs)\n",
    "#         attn_cat = torch.cat((output, attn_applied), 2)\n",
    "#         output = F.relu(attn_cat)\n",
    "#         output, hidden = self.gru(output, hidden)\n",
    "#         output = self.out(output.squeeze(dim=1))\n",
    "#         output = self.softmax(output)\n",
    "\n",
    "#         return output, hidden\n",
    "\n",
    "#     def initHidden(self):\n",
    "#         return torch.zeros(self.mul, bs, self.hidden_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.from_numpy(np.array([2,4,5,6,3,2,8,3,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.rand(2, 9, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_idx = torch.sort(a,descending=True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 3, 2, 1, 4, 7, 0, 5, 8])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, sort_sort_idx = torch.sort(sort_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_a = a[torch.sort(-a)[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8, 6, 5, 4, 3, 3, 2, 2, 2])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 3, 2, 1, 4, 7, 0, 5, 8])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort_sort_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 4, 5, 6, 3, 2, 8, 3, 2])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_a[sort_sort_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8, 6, 5, 4, 3, 3, 2, 2, 2])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[torch.sort(-a)[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, embed_dim, hidden_size,n_layers, rnn_type = 'lstm'):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = Embedding(input_size, embed_dim, PAD_IDX)\n",
    "        self.rnn_type =  rnn_type\n",
    "        self.dropout_in = nn.Dropout(p = 0.1)\n",
    "        self.n_layers = n_layers\n",
    "        if rnn_type == 'gru':\n",
    "            self.rnn = nn.GRU(embed_dim, hidden_size,batch_first=True,bidirectional=True, num_layers = self.n_layers, dropout = 0.2)\n",
    "        elif rnn_type == 'lstm':\n",
    "            self.rnn = LSTM(embed_dim, hidden_size, batch_first=True,bidirectional=True, num_layers = n_layers,dropout = 0.2)\n",
    "\n",
    "    def forward(self, enc_inp, src_len):\n",
    "        sorted_idx = torch.sort(src_len, descending=True)[1]\n",
    "        orig_idx = torch.sort(sorted_idx)[1]\n",
    "        embedded = self.embedding(enc_inp)\n",
    "        bs = embedded.size(0)\n",
    "        output = self.dropout_in(embedded)\n",
    "#         print(\"encoded Input\", output.shape)\n",
    "#         output = torch.nn.utils.rnn.pack_padded_sequence(output, src_len, batch_first=True)\n",
    "        if self.rnn_type == 'gru':\n",
    "            hidden =  self.initHidden(bs)\n",
    "            output, hidden = self.rnn(output, hidden)\n",
    "#             output = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "        elif self.rnn_type == 'lstm':\n",
    "            hidden, c = self.initHidden(bs)\n",
    "            sorted_output = output[sorted_idx]\n",
    "            sorted_len = src_len[sorted_idx]\n",
    "            packed_output = nn.utils.rnn.pack_padded_sequence(sorted_output, sorted_len.data.tolist(), batch_first = True)\n",
    "            packed_outs, (hiddden, c) = self.rnn(packed_output,(hidden, c))\n",
    "            hidden = hidden[:,orig_idx,:]\n",
    "            c = c[:,orig_idx,:]\n",
    "            output, _ = nn.utils.rnn.pad_packed_sequence(packed_outs, padding_value=PAD_IDX, batch_first = True)\n",
    "            output = output[orig_idx]\n",
    "        hidden = hidden.view(self.n_layers, 2, bs, -1).transpose(1, 2).contiguous().view(self.n_layers, bs, -1)\n",
    "        c = c.view(self.n_layers, 2, bs, -1).transpose(1, 2).contiguous().view(self.n_layers, bs, -1)\n",
    "#         print(output.shape)\n",
    "#         print(hidden.shape)\n",
    "#         print(c.shape)\n",
    "#             output = torch.nn.utils.rnn.pad_packed_sequence(output,batch_first=True)\n",
    "#         print(hidden.shape)\n",
    "#         print(\"rnn output\", output.shape)\n",
    "#         print(\"rnn hidden\", hidden.shape)\n",
    "#         hidden = hidden.transpose(0,1).contiguous().view(bs,-1)\n",
    "#         hidden_1 = torch.tanh(self.linear1(hidden)).unsqueeze(0)\n",
    "# #         print(\"rnn hidden_1\", hidden_1.shape)\n",
    "#         hidden_2 = torch.tanh(self.linear2(hidden)).unsqueeze(0)\n",
    "# #         print(\"rnn hidden_2\", hidden_2.shape)\n",
    "#         hidden = torch.cat([hidden_1,hidden_2],dim = 0)\n",
    "#         print(\"rnn hidden_final\", hidden.shape)\n",
    "#             print(hidden.shape)\n",
    "#         last_layer = output[:,-1,:self.hidden_size]\n",
    "#         first_layer = output[:,0,self.hidden_size:]\n",
    "#         hidden = first_layer + last_layer\n",
    "        return output, hidden, c\n",
    "\n",
    "    def initHidden(self,bs):\n",
    "        if self.rnn_type == 'gru' :\n",
    "            return torch.zeros(self.n_layers*2, bs, self.hidden_size).to(device)\n",
    "        elif self.rnn_type == 'lstm':\n",
    "            return torch.zeros(self.n_layers*2,bs,self.hidden_size).to(device),torch.zeros(self.n_layers*2,bs,self.hidden_size).to(device)\n",
    "\n",
    "class Attention_Module(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim):\n",
    "        super(Attention_Module, self).__init__()\n",
    "        self.l1 = Linear(hidden_dim, output_dim, bias = False)\n",
    "        self.l2 = Linear(hidden_dim+output_dim, output_dim, bias =  False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outs, src_lens):\n",
    "        ''' hiddden: bsz x hidden_dim\n",
    "        encoder_outs: bsz x sq_len x encoder dim (output_dim)\n",
    "        src_lens: bsz\n",
    "        \n",
    "        x: bsz x output_dim\n",
    "        attn_score: bsz x sq_len'''\n",
    "        x = self.l1(hidden)\n",
    "        att_score = (encoder_outs.transpose(0,1) * x.unsqueeze(0)).sum(dim = 2)\n",
    "        seq_mask = sequence_mask(src_lens, max_len = max(src_lens).item()).transpose(0,1)\n",
    "        masked_att = seq_mask*att_score\n",
    "        masked_att[masked_att==0] = -1e10\n",
    "        attn_scores = F.softmax(masked_att, dim=0)\n",
    "        x = (attn_scores.unsqueeze(2) * encoder_outs.transpose(0,1)).sum(dim=0)\n",
    "        x = torch.tanh(self.l2(torch.cat((x, hidden), dim=1)))\n",
    "        return x, attn_scores\n",
    "        \n",
    "class AttentionDecoderRNN(nn.Module):\n",
    "    def __init__(self, output_size, embed_dim, hidden_size, n_layers = 1, attention = True):\n",
    "        super(AttentionDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        encoder_output_size = hidden_size\n",
    "        self.embedding = Embedding(output_size, embed_dim, PAD_IDX)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.n_layers = n_layers\n",
    "        self.att_layer = Attention_Module(self.hidden_size, encoder_output_size) if attention else None\n",
    "        self.layers = nn.ModuleList([\n",
    "            LSTMCell(\n",
    "                input_size=self.hidden_size + embed_dim if ((layer == 0) and attention) else embed_dim if layer == 0 else hidden_size,\n",
    "                hidden_size=hidden_size,\n",
    "            )\n",
    "            for layer in range(self.n_layers)\n",
    "        ])\n",
    "#         if self.rnn_type == 'gru':\n",
    "#             self.rnn = nn.GRU(self.hidden_size + embed_dim, hidden_size,batch_first=True, num_layers = self.n_layers, )\n",
    "#         elif self.rnn_type == 'lstm':\n",
    "#             self.rnn = nn.LSTM(self.hidden_size + embed_dim, hidden_size,batch_first=True, num_layers = self.n_layers)\n",
    "        self.fc_out = nn.Linear(self.hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, dec_input,context_vector, prev_hiddens,prev_cs,encoder_outputs,src_len):\n",
    "        bsz = dec_input.size(0)\n",
    "        output = self.embedding(dec_input)\n",
    "        output = self.dropout(output)\n",
    "#         print(\"decoder Input embedded\", output.shape)\n",
    "        if self.att_layer is not None:\n",
    "            cated_input = torch.cat([output.squeeze(1),context_vector], dim = 1)\n",
    "        else:\n",
    "            cated_input = output.squeeze(1)\n",
    "#         print(\"cated_input\",cated_input.shape)\n",
    "        new_hiddens = []\n",
    "        new_cs = []\n",
    "        for i, rnn in enumerate(self.layers):\n",
    "            hidden, c = rnn(cated_input, (prev_hiddens[i], prev_cs[i]))\n",
    "            cated_input = self.dropout(hidden)\n",
    "            new_hiddens.append(hidden.unsqueeze(0))\n",
    "            new_cs.append(c.unsqueeze(0))\n",
    "        new_hiddens = torch.cat(new_hiddens, dim = 0)\n",
    "        new_cs = torch.cat(new_cs, dim = 0)\n",
    "\n",
    "        # apply attention using the last layer's hidden state\n",
    "        if self.att_layer is not None:\n",
    "            out, attn_score = self.att_layer(hidden, encoder_outputs, src_len)\n",
    "        else:\n",
    "            out = hidden\n",
    "            attn_score = None\n",
    "        context_vec = out\n",
    "        out = self.dropout(out)\n",
    "        out_vocab = self.softmax(self.fc_out(out))\n",
    "\n",
    "        return out_vocab, context_vec, new_hiddens, new_cs, attn_score\n",
    "    \n",
    "#     def initHidden(self,bs):\n",
    "#         if self.rnn_type == 'gru' :\n",
    "#             return None\n",
    "#         elif self.rnn_type == 'lstm':\n",
    "#             return torch.zeros(self.n_layers,bs,self.hidden_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Embedding(num_embeddings, embedding_dim, padding_idx):\n",
    "    m = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_idx)\n",
    "    nn.init.uniform_(m.weight, -0.1, 0.1)\n",
    "    nn.init.constant_(m.weight[padding_idx], 0)\n",
    "    return m\n",
    "\n",
    "\n",
    "def LSTM(input_size, hidden_size, **kwargs):\n",
    "    m = nn.LSTM(input_size, hidden_size,**kwargs)\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name or 'bias' in name:\n",
    "            param.data.uniform_(-0.1, 0.1)\n",
    "    return m\n",
    "\n",
    "\n",
    "def LSTMCell(input_size, hidden_size, **kwargs):\n",
    "    m = nn.LSTMCell(input_size, hidden_size,**kwargs)\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name or 'bias' in name:\n",
    "            param.data.uniform_(-0.1, 0.1)\n",
    "    return m\n",
    "\n",
    "\n",
    "def Linear(in_features, out_features, bias=True, dropout=0):\n",
    "    \"\"\"Linear layer (input: N x T x C)\"\"\"\n",
    "    m = nn.Linear(in_features, out_features, bias=bias)\n",
    "    m.weight.data.uniform_(-0.1, 0.1)\n",
    "    if bias:\n",
    "        m.bias.data.uniform_(-0.1, 0.1)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_mask(sequence_length, max_len=None):\n",
    "    if max_len is None:\n",
    "        max_len = sequence_length.max().item()\n",
    "    batch_size = sequence_length.size(0)\n",
    "    seq_range = torch.arange(0, max_len).long()\n",
    "    seq_range_expand = seq_range.unsqueeze(0).repeat([batch_size,1])\n",
    "    seq_range_expand = seq_range_expand.to(device)\n",
    "    seq_length_expand = (sequence_length.unsqueeze(1)\n",
    "                         .expand_as(seq_range_expand))\n",
    "    return (seq_range_expand < seq_length_expand).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def encode_decode(encoder,decoder,data_en,data_de,src_len,tar_len,m_type,rand_num = 0.5):\n",
    "#     use_teacher_forcing = True if random.random() < rand_num else False\n",
    "# #     print(\"tar_len\",tar_len)\n",
    "#     bss = data_en.size(0)\n",
    "#     en_hid = encoder.initHidden(bss)\n",
    "#     en_out,en_hid = encoder(data_en, en_hid)\n",
    "#     max_tar_len_batch = max(tar_len).item()\n",
    "#     decoder_hidden = en_hid\n",
    "#     decoder_input = torch.tensor([[SOS_token]]*bss).to(device)\n",
    "# #     c = decoder.initHidden(bss)\n",
    "# #     print(\"max_tar_len_batch\",max_tar_len_batch)\n",
    "#     if use_teacher_forcing:\n",
    "#         d_out = []\n",
    "#         for i in range(max_tar_len_batch):\n",
    "#             if m_type==\"attention\":\n",
    "#                 decoder_output,decoder_hidden = decoder(decoder_input,decoder_hidden,en_out)   \n",
    "#             else:\n",
    "#                 decoder_output,decoder_hidden = decoder(decoder_input,decoder_hidden)\n",
    "#             d_out.append(decoder_output.unsqueeze(-1))\n",
    "#             decoder_input = data_de[:,i].view(-1,1)\n",
    "#         d_hid = decoder_hidden\n",
    "#         d_out = torch.cat(d_out,dim=-1)\n",
    "#     else:\n",
    "#         d_out = []\n",
    "#         for i in range(max_tar_len_batch):\n",
    "#             if m_type==\"attention\":\n",
    "#                 decoder_output,decoder_hidden = decoder(decoder_input,decoder_hidden,en_out)\n",
    "#             else:\n",
    "#                 decoder_output, decoder_hidden = decoder(decoder_input,decoder_hidden)\n",
    "#             d_out.append(decoder_output.unsqueeze(-1))\n",
    "#             topv, topi = decoder_output.topk(1)\n",
    "# #             decoder_input = topi.squeeze().detach().view(-1,1)\n",
    "#             decoder_input = topi.squeeze().view(-1,1)\n",
    "#         d_hid = decoder_hidden\n",
    "#         d_out = torch.cat(d_out,dim=-1)\n",
    "#     return d_out, d_hid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def encode_decode(encoder,decoder,data_en,data_de,src_len,tar_len,m_type,rand_num = 0.5):\n",
    "#     use_teacher_forcing = True if random.random() < rand_num else False\n",
    "# #     print(\"tar_len\",tar_len)\n",
    "#     bss = data_en.size(0)\n",
    "#     en_out,en_hid,c = encoder(data_en, src_len)\n",
    "#     max_tar_len_batch = max(tar_len).item()\n",
    "#     decoder_hidden = en_hid\n",
    "#     decoder_input = torch.tensor([[SOS_token]]*bss).to(device)\n",
    "# #     print(\"max_tar_len_batch\",max_tar_len_batch)\n",
    "#     if use_teacher_forcing:\n",
    "#         d_out = []\n",
    "#         for i in range(max_tar_len_batch):\n",
    "#             if m_type==\"attention\":\n",
    "#                 decoder_output,decoder_hidden, c = decoder(decoder_input,decoder_hidden,en_out,src_len,c)   \n",
    "#             else:\n",
    "#                 decoder_output,decoder_hidden = decoder(decoder_input,decoder_hidden)\n",
    "#             d_out.append(decoder_output.unsqueeze(-1))\n",
    "#             decoder_input = data_de[:,i].view(-1,1)\n",
    "#         d_hid = decoder_hidden\n",
    "#         d_out = torch.cat(d_out,dim=-1)\n",
    "# #         print(\"softmax mat\", d_out.shape)\n",
    "#     else:\n",
    "#         d_out = []\n",
    "#         for i in range(max_tar_len_batch):\n",
    "#             if m_type==\"attention\":\n",
    "#                 decoder_output,decoder_hidden = decoder(decoder_input,decoder_hidden,en_out,src_len,c)\n",
    "#             else:\n",
    "#                 decoder_output, decoder_hidden = decoder(decoder_input,decoder_hidden)\n",
    "#             d_out.append(decoder_output.unsqueeze(-1))\n",
    "#             topv, topi = decoder_output.topk(1)\n",
    "# #             decoder_input = topi.squeeze().detach().view(-1,1)\n",
    "#             decoder_input = topi.squeeze().view(-1,1)\n",
    "#         d_hid = decoder_hidden\n",
    "#         d_out = torch.cat(d_out,dim=-1)\n",
    "#     return d_out, d_hid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_decode(encoder,decoder,data_en,data_de,src_len,tar_len,rand_num = 0.95, val = False):\n",
    "    if not val:\n",
    "        use_teacher_forcing = True if random.random() < rand_num else False\n",
    "    #     print(\"tar_len\",tar_len)\n",
    "        bss = data_en.size(0)\n",
    "        en_out,en_hid,en_c = encoder(data_en, src_len)\n",
    "        max_src_len_batch = max(src_len).item()\n",
    "        max_tar_len_batch = max(tar_len).item()\n",
    "        prev_hiddens = en_hid\n",
    "        prev_cs = en_c\n",
    "        decoder_input = torch.tensor([[SOS_token]]*bss).to(device)\n",
    "        prev_output = torch.zeros((bss, en_out.size(-1))).to(device)\n",
    "        if use_teacher_forcing:\n",
    "            d_out = []\n",
    "            for i in range(max_tar_len_batch):\n",
    "                out_vocab, prev_output,prev_hiddens, prev_cs, attention_score = decoder(decoder_input,prev_output, \\\n",
    "                                                                                        prev_hiddens,prev_cs, en_out,\\\n",
    "                                                                                        src_len)\n",
    "                d_out.append(out_vocab.unsqueeze(-1))\n",
    "                decoder_input = data_de[:,i].view(-1,1)\n",
    "            d_out = torch.cat(d_out,dim=-1)\n",
    "    #         print(\"softmax mat\", d_out.shape)\n",
    "        else:\n",
    "            d_out = []\n",
    "            for i in range(max_tar_len_batch):\n",
    "                out_vocab, prev_output,prev_hiddens, prev_cs, attention_score = decoder(decoder_input,prev_output, \\\n",
    "                                                                                        prev_hiddens,prev_cs, en_out,\\\n",
    "                                                                                        src_len)\n",
    "                d_out.append(out_vocab.unsqueeze(-1))\n",
    "                topv, topi = out_vocab.topk(1)\n",
    "                decoder_input = topi.squeeze().detach().view(-1,1)\n",
    "    #             decoder_input = topi.squeeze().view(-1,1)\n",
    "            d_out = torch.cat(d_out,dim=-1)\n",
    "        return d_out\n",
    "    else:\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        bss = data_en.size(0)\n",
    "        en_out,en_hid,en_c = encoder(data_en, src_len)\n",
    "        max_src_len_batch = max(src_len).item()\n",
    "        max_tar_len_batch = max(tar_len).item()\n",
    "        prev_hiddens = en_hid\n",
    "        prev_cs = en_c\n",
    "        decoder_input = torch.tensor([[SOS_token]]*bss).to(device)\n",
    "        prev_output = torch.zeros((bss, en_out.size(-1))).to(device)\n",
    "        d_out = []\n",
    "        for i in range(max_tar_len_batch):\n",
    "            out_vocab, prev_output,prev_hiddens, prev_cs, attention_score = decoder(decoder_input,prev_output, \\\n",
    "                                                                                    prev_hiddens,prev_cs, en_out,\\\n",
    "                                                                                    src_len)\n",
    "            d_out.append(out_vocab.unsqueeze(-1))\n",
    "            topv, topi = out_vocab.topk(1)\n",
    "            decoder_input = topi.squeeze().detach().view(-1,1)\n",
    "#             decoder_input = topi.squeeze().view(-1,1)\n",
    "        d_out = torch.cat(d_out,dim=-1)\n",
    "        return d_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_cel_loss(input,target,nll):\n",
    "    input = input.transpose(1,2)\n",
    "    bs, sl = input.size()[:2]\n",
    "    return nll(input.contiguous().view(bs*sl,-1),target.contiguous().view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(encoder_optimizer,decoder_optimizer, encoder, decoder, loss_fun,m_type, dataloader, en_lang,\\\n",
    "                num_epochs=60, val_every = 1, train_bleu_every = 10,clip = 0.1, rm = 0.8, enc_scheduler = None,\\\n",
    "               dec_scheduler = None):\n",
    "    best_score = 0\n",
    "    best_bleu = 0\n",
    "    loss_hist = {'train': [], 'val_train': []}\n",
    "    bleu_hist = {'train': [], 'validate': []}\n",
    "    best_encoder_wts = None\n",
    "    best_decoder_wts = None\n",
    "#     train_sorted = train.sort_values('en_len', ascending = False)\n",
    "    phases = ['train','val_train']\n",
    "#     phases = ['train']\n",
    "    for epoch in range(num_epochs):\n",
    "#         bs_dict = {'train':128,'validate':1, 'train_val':1,'val_train':128}\n",
    "#         train_used = shuffle_sorted_batches(train_sorted, bs_dict['train'])\n",
    "# #         train_used = train.iloc[:50]\n",
    "# #         train_used = train\n",
    "#         collate_fn_dict = {'train':vocab_collate_func, 'validate':vocab_collate_func_val,\\\n",
    "#                            'train_val':vocab_collate_func_val,'val_train':vocab_collate_func}\n",
    "#         transformed_dataset = {'train': Vietnamese(train_used),\n",
    "#                                'validate': Vietnamese(val, val = True),\n",
    "#                                'train_val':Vietnamese(train.iloc[:50], val = True),\n",
    "#                                'val_train':Vietnamese(val)\n",
    "#                                                        }\n",
    "\n",
    "#         dataloader = {x: DataLoader(transformed_dataset[x], batch_size=bs_dict[x], collate_fn=collate_fn_dict[x],\n",
    "#                             shuffle=True, num_workers=0) for x in ['train', 'validate', 'train_val','val_train']}\n",
    "        for ex, phase in enumerate(phases):\n",
    "            start = time.time()\n",
    "            total = 0\n",
    "            top1_correct = 0\n",
    "            running_loss = 0\n",
    "            running_total = 0\n",
    "            if phase == 'train':\n",
    "                encoder.train()\n",
    "                decoder.train()\n",
    "            else:\n",
    "                encoder.eval()\n",
    "                decoder.eval()\n",
    "            for data in dataloader[phase]:\n",
    "                encoder_optimizer.zero_grad()\n",
    "                decoder_optimizer.zero_grad()\n",
    "\n",
    "                encoder_i = data[0].to(device)\n",
    "                decoder_i = data[1].to(device)\n",
    "                src_len = data[2].to(device)\n",
    "                tar_len = data[3].to(device)\n",
    "#                 print(encoder_i.shape)\n",
    "                if phase == 'val_train':                \n",
    "                    out = encode_decode(encoder,decoder,encoder_i,decoder_i,src_len,tar_len,rand_num=rm,val = True )\n",
    "                else:\n",
    "                    out = encode_decode(encoder,decoder,encoder_i,decoder_i,src_len,tar_len,rand_num=rm,val = False )\n",
    "                N = decoder_i.size(0)\n",
    "#                 out = out.view(-1,en_lang.n_words)\n",
    "#                 decoder_i = decoder_i.view(-1)\n",
    "#                 if loss_fun == 'masked_cel':\n",
    "#                     loss = masked_cross_entropy(out.float(), decoder_i.long(), tar_len)\n",
    "#                 else:\n",
    "#                     loss = flatten_cel_loss(out.float(), decoder_i.long(),loss_fun)\n",
    "                loss = loss_fun(out.float(), decoder_i.long())\n",
    "                running_loss += loss.item() * N\n",
    "                \n",
    "                total += N\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "                    torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "                    encoder_optimizer.step()\n",
    "                    decoder_optimizer.step()\n",
    "                    \n",
    "            epoch_loss = running_loss / total \n",
    "            loss_hist[phase].append(epoch_loss)\n",
    "            print(\"epoch {} {} loss = {}, time = {}\".format(epoch, phase, epoch_loss,\n",
    "                                                                           time.time() - start))\n",
    "#             if epoch%train_bleu_every ==0:\n",
    "#                 train_loss, train_bleu_score = validation(encoder,decoder, dataloader['train'],loss_fun, en_lang,max_len,m_type)\n",
    "#                 bleu_hist['train'].append(train_bleu_score)\n",
    "#                 print(\"Train BLEU = \", train_bleu_score)\n",
    "        if (enc_scheduler is not None) and (dec_scheduler is not None):\n",
    "            enc_scheduler.step(epoch_loss)\n",
    "            dec_scheduler.step(epoch_loss)\n",
    "        if epoch%val_every == 0:\n",
    "            val_bleu_score = validation_new(encoder,decoder, dataloader['validate'], en_lang, m_type)\n",
    "            bleu_hist['validate'].append(val_bleu_score)\n",
    "            print(\"validation BLEU = \", val_bleu_score)\n",
    "            if val_bleu_score > best_bleu:\n",
    "                best_bleu = val_bleu_score\n",
    "                best_encoder_wts = encoder.state_dict()\n",
    "                best_decoder_wts = decoder.state_dict()\n",
    "        print('='*50)\n",
    "    encoder.load_state_dict(best_encoder_wts)\n",
    "    decoder.load_state_dict(best_decoder_wts)\n",
    "    print(\"Training completed. Best BLEU is {}\".format(best_bleu))\n",
    "    return encoder,decoder,loss_hist,bleu_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def validation_new(encoder, decoder, val_dataloader, lang_en,m_type):\n",
    "#     encoder.train(False)\n",
    "#     decoder.train(False)\n",
    "#     pred_corpus = []\n",
    "#     true_corpus = []\n",
    "#     running_loss = 0\n",
    "#     running_total = 0\n",
    "#     bl = BLEU_SCORE()\n",
    "#     for data in val_dataloader:\n",
    "#         encoder_i = data[0].to(device)\n",
    "#         src_len = data[2].to(device)\n",
    "#         bs,sl = encoder_i.size()[:2]\n",
    "#         en_hid = encoder.initHidden(bs)\n",
    "#         en_out,en_hid = encoder(encoder_i, en_hid)\n",
    "#         decoder_hidden = en_hid\n",
    "#         decoder_input = torch.tensor([[SOS_token]]*bs).to(device)\n",
    "# #         c = decoder.initHidden(bs)\n",
    "#         d_out = []\n",
    "#         for i in range(sl+20):\n",
    "#             if m_type==\"attention\":\n",
    "#                 decoder_output,decoder_hidden = decoder(decoder_input,decoder_hidden,en_out)\n",
    "#             else:\n",
    "#                 decoder_output,decoder_hidden = decoder(decoder_input,decoder_hidden)\n",
    "#             topv, topi = decoder_output.topk(1)\n",
    "#             decoder_input = topi.squeeze().detach().view(-1,1)\n",
    "#             d_out.append(topi.item())\n",
    "#             if topi.item() == EOS_token:\n",
    "#                 break\n",
    "#         d_hid = decoder_hidden\n",
    "        \n",
    "#         true_corpus.append(data[-1])\n",
    "#         pred_sent = convert_id_list_2_sent(d_out,lang_en)\n",
    "#         pred_corpus.append(pred_sent)\n",
    "# #         print(\"True Sentence:\",data[-1])\n",
    "# #         print(\"Pred Sentence:\", pred_sent)\n",
    "# #         print('-*'*50)\n",
    "#     score = bl.corpus_bleu(pred_corpus,[true_corpus],lowercase=True)[0]\n",
    "#     return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def validation_new(encoder, decoder, val_dataloader, lang_en,m_type):\n",
    "#     encoder.train(False)\n",
    "#     decoder.train(False)\n",
    "#     pred_corpus = []\n",
    "#     true_corpus = []\n",
    "#     running_loss = 0\n",
    "#     running_total = 0\n",
    "#     bl = BLEU_SCORE()\n",
    "#     for data in val_dataloader:\n",
    "#         encoder_i = data[0].to(device)\n",
    "#         src_len = data[2].to(device)\n",
    "#         bs,sl = encoder_i.size()[:2]\n",
    "#         en_out,en_hid = encoder(encoder_i,src_len)\n",
    "#         decoder_hidden = en_hid\n",
    "#         decoder_input = torch.tensor([[SOS_token]]*bs).to(device)\n",
    "#         c = decoder.initHidden(bs)\n",
    "#         d_out = []\n",
    "#         for i in range(sl+20):\n",
    "#             if m_type==\"attention\":\n",
    "#                 decoder_output,decoder_hidden = decoder(decoder_input,decoder_hidden,en_out,src_len,c)\n",
    "#             else:\n",
    "#                 decoder_output,decoder_hidden = decoder(decoder_input,decoder_hidden)\n",
    "#             topv, topi = decoder_output.topk(1)\n",
    "#             decoder_input = topi.squeeze().detach().view(-1,1)\n",
    "#             d_out.append(topi.item())\n",
    "#             if topi.item() == EOS_token:\n",
    "#                 break\n",
    "#         d_hid = decoder_hidden\n",
    "        \n",
    "#         true_corpus.append(data[-1])\n",
    "#         pred_sent = convert_id_list_2_sent(d_out,lang_en)\n",
    "#         pred_corpus.append(pred_sent)\n",
    "# #         print(\"True Sentence:\",data[-1])\n",
    "# #         print(\"Pred Sentence:\", pred_sent)\n",
    "# #         print('-*'*50)\n",
    "#     score = bl.corpus_bleu(pred_corpus,[true_corpus],lowercase=True)[0]\n",
    "#     return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_new(encoder, decoder, val_dataloader, lang_en,m_type, verbose = False):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    pred_corpus = []\n",
    "    true_corpus = []\n",
    "    running_loss = 0\n",
    "    running_total = 0\n",
    "    bl = BLEU_SCORE()\n",
    "    for data in val_dataloader:\n",
    "        encoder_i = data[0].to(device)\n",
    "        src_len = data[2].to(device)\n",
    "        bs,sl = encoder_i.size()[:2]\n",
    "        en_out,en_hid,en_c = encoder(encoder_i,src_len)\n",
    "        max_src_len_batch = max(src_len).item()\n",
    "        prev_hiddens = en_hid\n",
    "        prev_cs = en_c\n",
    "        decoder_input = torch.tensor([[SOS_token]]*bs).to(device)\n",
    "        prev_output = torch.zeros((bs, en_out.size(-1))).to(device)\n",
    "        d_out = []\n",
    "        for i in range(sl*2):\n",
    "            out_vocab, prev_output,prev_hiddens, prev_cs, attention_score = decoder(decoder_input,prev_output, \\\n",
    "                                                                                    prev_hiddens,prev_cs, en_out,\\\n",
    "                                                                                    src_len)\n",
    "            topv, topi = out_vocab.topk(1)\n",
    "#             decoder_input = topi.squeeze().detach().view(-1,1)\n",
    "            d_out.append(topi.item())\n",
    "            decoder_input = topi.squeeze().detach().view(-1,1)\n",
    "            if topi.item() == EOS_token:\n",
    "                break\n",
    "        \n",
    "        true_corpus.append(data[-1])\n",
    "        pred_sent = convert_id_list_2_sent(d_out,lang_en)\n",
    "        pred_corpus.append(pred_sent)\n",
    "        if verbose:\n",
    "            print(\"True Sentence:\",data[-1])\n",
    "            print(\"Pred Sentence:\", pred_sent)\n",
    "            print('-*'*50)\n",
    "    score = bl.corpus_bleu(pred_corpus,[true_corpus],lowercase=True)[0]\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_cross_entropy(logits, target, length):\n",
    "    logits = logits.transpose(1,2)\n",
    "#     print(\"logits\",logits.shape)\n",
    "#     print(\"target\",target.shape)\n",
    "    # logits_flat: (batch * max_len, num_classes)\n",
    "    logits_flat = logits.contiguous().view(-1, logits.size(-1))\n",
    "    # log_probs_flat: (batch * max_len, num_classes)\n",
    "#     log_probs_flat = F.log_softmax(logits_flat, dim = 1)\n",
    "    # target_flat: (batch * max_len, 1)\n",
    "    target_flat = target.view(-1, 1)\n",
    "#     print(\"tar_flat\", target_flat.shape)\n",
    "    # losses_flat: (batch * max_len, 1)\n",
    "    losses_flat = -torch.gather(logits_flat, dim=1, index=target_flat)\n",
    "    # losses: (batch, max_len)\n",
    "    losses = losses_flat.view(*target.size())\n",
    "    # mask: (batch, max_len)\n",
    "    mask = sequence_mask(sequence_length=length, max_len=target.size(1))\n",
    "    losses = losses * mask\n",
    "    loss = losses.sum() / length.float().sum()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = 'masked_cel'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Old Interesting performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(vi_lang.n_words,512,512, 2).to(device)\n",
    "decoder = AttentionDecoderRNN(en_lang.n_words,512,1024,n_layers=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# encoder = OldEncoderRNN(vi_lang.n_words,512,bi = True).to(device)\n",
    "# decoder = OldAttentionDecoderRNN(512,en_lang.n_words, True, MAX_LEN, attention_type='t2').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(512, 512, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=1e-4)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 train loss = 5.654132865225413, time = 341.1948525905609\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 1 train loss = 5.45065745284289, time = 341.9193711280823\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 2 train loss = 5.210996340336155, time = 342.40592312812805\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 4 train loss = 4.809566239472573, time = 341.6780881881714\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 5 train loss = 4.63142532622266, time = 341.56193590164185\n",
      "validation BLEU =  0.0\n",
      "==================================================\n",
      "epoch 6 train loss = 4.520703522670287, time = 341.8052592277527\n",
      "validation BLEU =  0.007257456178771249\n",
      "==================================================\n",
      "epoch 7 train loss = 4.4197971115632395, time = 341.1301064491272\n",
      "validation BLEU =  0.00778988176186667\n",
      "==================================================\n",
      "epoch 8 train loss = 4.318348191080374, time = 341.3023076057434\n",
      "validation BLEU =  0.00873574484162468\n",
      "==================================================\n",
      "epoch 9 train loss = 4.237790871660339, time = 341.7362952232361\n",
      "validation BLEU =  0.01489571311812371\n",
      "==================================================\n",
      "epoch 10 train loss = 4.151050939980724, time = 341.4068603515625\n",
      "validation BLEU =  0.019931957373692067\n",
      "==================================================\n",
      "epoch 11 train loss = 4.091922725855361, time = 341.15118384361267\n",
      "validation BLEU =  0.029661319317699324\n",
      "==================================================\n",
      "epoch 12 train loss = 3.9871769643770034, time = 341.45858216285706\n",
      "validation BLEU =  0.026448033178258345\n",
      "==================================================\n",
      "epoch 13 train loss = 3.913650239591969, time = 341.27526092529297\n",
      "validation BLEU =  0.03262531539161065\n",
      "==================================================\n",
      "epoch 14 train loss = 3.8498835832313856, time = 340.95101499557495\n",
      "validation BLEU =  0.044428241859491846\n",
      "==================================================\n",
      "epoch 15 train loss = 3.7791828754784245, time = 340.5919189453125\n",
      "validation BLEU =  0.03321728976507814\n",
      "==================================================\n",
      "epoch 16 train loss = 3.73131796845992, time = 341.2179868221283\n",
      "validation BLEU =  0.038820169262083856\n",
      "==================================================\n",
      "epoch 17 train loss = 3.690869427688029, time = 340.93681621551514\n",
      "validation BLEU =  0.05502212762362031\n",
      "==================================================\n",
      "epoch 18 train loss = 3.6422906667641395, time = 341.4393320083618\n",
      "validation BLEU =  0.058218193395599985\n",
      "==================================================\n",
      "epoch 19 train loss = 3.6025976965133, time = 342.0387489795685\n",
      "validation BLEU =  0.06265281052708205\n",
      "==================================================\n",
      "epoch 20 train loss = 3.568165948672502, time = 341.7699897289276\n",
      "validation BLEU =  0.06432711461627151\n",
      "==================================================\n",
      "epoch 21 train loss = 3.467597369658819, time = 341.9801962375641\n",
      "validation BLEU =  0.07283668274696219\n",
      "==================================================\n",
      "epoch 22 train loss = 3.4127586705564568, time = 341.50896525382996\n",
      "validation BLEU =  0.09124435261111917\n",
      "==================================================\n",
      "epoch 23 train loss = 4.11731507090061, time = 662.7810173034668\n",
      "validation BLEU =  3.7549343800802806\n",
      "==================================================\n",
      "epoch 24 train loss = 3.281447067662696, time = 341.5275800228119\n",
      "validation BLEU =  0.08469296828050618\n",
      "==================================================\n",
      "epoch 25 train loss = 3.271404259174648, time = 341.82329988479614\n",
      "validation BLEU =  0.15599388189696986\n",
      "==================================================\n",
      "epoch 26 train loss = 3.2869543531108163, time = 341.4335904121399\n",
      "validation BLEU =  0.1577493999531032\n",
      "==================================================\n",
      "epoch 27 train loss = 3.230972457758133, time = 341.5713469982147\n",
      "validation BLEU =  0.19278032635104744\n",
      "==================================================\n",
      "epoch 28 train loss = 3.19142589813571, time = 341.66050362586975\n",
      "validation BLEU =  0.1841768342892874\n",
      "==================================================\n",
      "epoch 29 train loss = 3.14125059641754, time = 341.6516077518463\n",
      "validation BLEU =  0.3033989533197111\n",
      "==================================================\n",
      "epoch 30 train loss = 3.115007367013696, time = 341.53219294548035\n",
      "validation BLEU =  0.3382457234701169\n",
      "==================================================\n",
      "epoch 31 train loss = 3.0665265867784455, time = 341.37299036979675\n",
      "validation BLEU =  0.5253105968823382\n",
      "==================================================\n",
      "epoch 32 train loss = 3.024415717351258, time = 342.05147337913513\n",
      "validation BLEU =  0.62985109085486\n",
      "==================================================\n",
      "epoch 33 train loss = 2.9992271762819116, time = 341.3521366119385\n",
      "validation BLEU =  0.7111174465433215\n",
      "==================================================\n",
      "epoch 34 train loss = 2.973018437561273, time = 341.3219199180603\n",
      "validation BLEU =  0.7373088348879125\n",
      "==================================================\n",
      "epoch 35 train loss = 2.9244212736258652, time = 341.13356590270996\n",
      "validation BLEU =  0.6783435354723314\n",
      "==================================================\n",
      "epoch 36 train loss = 2.887372524305964, time = 341.86699175834656\n",
      "validation BLEU =  0.7934873634499011\n",
      "==================================================\n",
      "epoch 37 train loss = 3.7282196400737244, time = 663.0309255123138\n",
      "validation BLEU =  5.422935553126234\n",
      "==================================================\n",
      "epoch 38 train loss = 2.7775619442239425, time = 341.5769703388214\n",
      "validation BLEU =  0.7515482635411938\n",
      "==================================================\n",
      "epoch 39 train loss = 2.822405433327482, time = 341.5337553024292\n",
      "validation BLEU =  0.8095145218786286\n",
      "==================================================\n",
      "epoch 40 train loss = 2.756222195607683, time = 341.53091645240784\n",
      "validation BLEU =  0.9110316293259347\n",
      "==================================================\n",
      "epoch 41 train loss = 2.721858608548948, time = 340.9498107433319\n",
      "validation BLEU =  0.918893890140245\n",
      "==================================================\n",
      "epoch 42 train loss = 2.7066624271356634, time = 341.8966417312622\n",
      "validation BLEU =  1.033976401522409\n",
      "==================================================\n",
      "epoch 43 train loss = 2.68261871081093, time = 341.18897104263306\n",
      "validation BLEU =  1.0486068458813393\n",
      "==================================================\n",
      "epoch 44 train loss = 3.5252452283543056, time = 662.3115556240082\n",
      "validation BLEU =  5.361300296846215\n",
      "==================================================\n",
      "epoch 45 train loss = 2.6183942231628192, time = 341.8273367881775\n",
      "validation BLEU =  1.0240641803494395\n",
      "==================================================\n",
      "epoch 46 train loss = 2.5875116109712106, time = 341.2915151119232\n",
      "validation BLEU =  1.3144701708752409\n",
      "==================================================\n",
      "epoch 47 train loss = 2.5632219784778774, time = 341.41440892219543\n",
      "validation BLEU =  1.3706422127766096\n",
      "==================================================\n",
      "epoch 48 train loss = 2.550861967960378, time = 341.4405016899109\n",
      "validation BLEU =  1.3821948611330461\n",
      "==================================================\n",
      "epoch 49 train loss = 3.414882306602232, time = 662.9478738307953\n",
      "validation BLEU =  5.737080525314268\n",
      "==================================================\n",
      "Training completed. Best BLEU is 5.737080525314268\n"
     ]
    }
   ],
   "source": [
    "enc, dec, loss_hist, acc_hist = train_model(encoder_optimizer, decoder_optimizer, encoder, decoder, criterion,\\\n",
    "                                            \"attention\", dataloader,en_lang, num_epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Sentence: Rachel Pike : The science behind a climate headline\n",
      "Pred Sentence: UKN UKN revolution .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: In 4 minutes , atmospheric chemist Rachel Pike provides a glimpse of the massive scientific effort behind the bold headlines on climate change , with her team -- one of thousands who contributed -- taking a risky flight over the rainforest in pursuit of data on a key molecule .\n",
      "Pred Sentence: In four minutes , the UKN UKN UKN UKN UKN of of of the the the the the the the , , , , the the the , , , , , , , , , , the the the the the the the the the the the\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: I &apos;d like to talk to you today about the scale of the scientific effort that goes into making the headlines you see in the paper .\n",
      "Pred Sentence: I want you to know about the great challenges of the challenges that you &apos;ve been to to .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: Headlines that look like this when they have to do with climate change , and headlines that look like this when they have to do with air quality or smog .\n",
      "Pred Sentence: These are things like like like to look to the , , , , , , , , , UKN UKN UKN\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: They are both two branches of the same field of atmospheric science .\n",
      "Pred Sentence: Both are both UKN of a of a of . .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: Recently the headlines looked like this when the Intergovernmental Panel on Climate Change , or IPCC , put out their report on the state of understanding of the atmospheric system .\n",
      "Pred Sentence: The recent headlines like this as the UKN UKN UKN , which if they UKN the UKN to the the the the\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: That report was written by 620 scientists from 40 countries .\n",
      "Pred Sentence: UKN was written by the UKN UKN UKN UKN .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: They wrote almost a thousand pages on the topic .\n",
      "Pred Sentence: They wrote a thousand pages on this video .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: And all of those pages were reviewed by another 400-plus scientists and reviewers , from 113 countries .\n",
      "Pred Sentence: And all all of are by by by by UKN of of UKN UKN and .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: It &apos;s a big community . It &apos;s such a big community , in fact , that our annual gathering is the largest scientific meeting in the world .\n",
      "Pred Sentence: It &apos;s a huge community , and , , , , the the the the the the the the the the the the the the the . . .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: Over 15,000 scientists go to San Francisco every year for that .\n",
      "Pred Sentence: More , we invited to San Francisco at San Francisco .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: And every one of those scientists is in a research group , and every research group studies a wide variety of topics .\n",
      "Pred Sentence: Every human group is a group group , a group of students , and each of them .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: For us at Cambridge , it &apos;s as varied as the El Niño oscillation , which affects weather and climate , to the assimilation of satellite data , to emissions from crops that produce biofuels , which is what I happen to study .\n",
      "Pred Sentence: For us , , , , the the the the of of of of , , , , , , , , , , , , , , , , , , , , , , , , the the the the the the . . .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: And in each one of these research areas , of which there are even more , there are PhD students , like me , and we study incredibly narrow topics , things as narrow as a few processes or a few molecules .\n",
      "Pred Sentence: Each of these projects projects more , , , , , , , , , , , I , , , , , , , , , , , , some some . . .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: And one of the molecules I study is called isoprene , which is here . It &apos;s a small organic molecule . You &apos;ve probably never heard of it .\n",
      "Pred Sentence: One of the UKN of this is called called UKN . UKN . It &apos;s a . a . a , , , , you . . . . .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: The weight of a paper clip is approximately equal to 900 zeta-illion -- 10 to the 21st -- molecules of isoprene .\n",
      "Pred Sentence: A little of UKN UKN UKN UKN UKN UKN UKN -- -- -- -- -- -- -- -- .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: But despite its very small weight , enough of it is emitted into the atmosphere every year to equal the weight of all the people on the planet .\n",
      "Pred Sentence: UKN small , very small , but the amount of energy is actually in the atmosphere of the the of .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: It &apos;s a huge amount of stuff . It &apos;s equal to the weight of methane .\n",
      "Pred Sentence: It &apos;s a huge amount of carbon emissions .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: And because it &apos;s so much stuff , it &apos;s really important for the atmospheric system .\n",
      "Pred Sentence: It &apos;s because huge huge huge amount of water , it &apos;s important .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: Because it &apos;s important to the atmospheric system , we go to all lengths to study this thing .\n",
      "Pred Sentence: It &apos;s because it &apos;s because the environment , the system that we &apos;re going to to the we the the .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: We blow it up and look at the pieces .\n",
      "Pred Sentence: We made it and and and we a little . .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: This is the EUPHORE Smog Chamber in Spain .\n",
      "Pred Sentence: This is UKN UKN UKN in Spain .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: Atmospheric explosions , or full combustion , takes about 15,000 times longer than what happens in your car .\n",
      "Pred Sentence: UKN explosions , UKN UKN UKN , UKN times times times the of of\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: But still , we look at the pieces .\n",
      "Pred Sentence: And sometimes we &apos;re still small .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: We run enormous models on supercomputers ; this is what I happen to do .\n",
      "Pred Sentence: We created the models model on computers .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: Our models have hundreds of thousands of grid boxes calculating hundreds of variables each , on minute timescales .\n",
      "Pred Sentence: Our data has thousands of thousands of UKN UKN UKN hundreds of hundreds of .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Sentence: And it takes weeks to perform our integrations .\n",
      "Pred Sentence: It &apos;s takes a week of weeks .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: And we perform dozens of integrations in order to understand what &apos;s happening .\n",
      "Pred Sentence: We need hundreds of these kinds of algorithms to do that .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: We also fly all over the world looking for this thing .\n",
      "Pred Sentence: We &apos;re around around the world to to .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: I recently joined a field campaign in Malaysia . There are others .\n",
      "Pred Sentence: I recently joined a UKN in the UKN . . . . . . .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: We found a global atmospheric watchtower there , in the middle of the rainforest , and hung hundreds of thousands of dollars worth of scientific equipment off this tower , to look for isoprene , and of course , other things while we were there .\n",
      "Pred Sentence: We found a global atmospheric global in the , , , and and and and we and to to to thousands thousands thousands of thousands of , , , , , , , , , , and , . . . . . . .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: This is the tower in the middle of the rainforest , from above .\n",
      "Pred Sentence: This is the UKN UKN in the middle of the the .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: And this is the tower from below .\n",
      "Pred Sentence: \n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: And on part of that field campaign we even brought an aircraft with us .\n",
      "Pred Sentence: We have we we we .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: And this plane , the model , BA146 , which was run by FAAM , normally flies 120 to 130 people .\n",
      "Pred Sentence: This slide , UKN , UKN , which is UKN UKN , UKN UKN .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: So maybe you took a similar aircraft to get here today .\n",
      "Pred Sentence: Very probably you have on a new future .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: But we didn &apos;t just fly it . We were flying at 100 meters above the top of the canopy to measure this molecule -- incredibly dangerous stuff .\n",
      "Pred Sentence: We didn &apos;t fly just how fast it was to fly to the fly to the surface of the surface of the reef , and it &apos;s very pretty cheap .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: We have to fly at a special incline in order to make the measurements .\n",
      "Pred Sentence: We have to UKN UKN UKN UKN ,\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: We hire military and test pilots to do the maneuvering .\n",
      "Pred Sentence: We hire military and pilots to choose the police .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: We have to get special flight clearance .\n",
      "Pred Sentence: UKN to .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: And as you come around the banks in these valleys , the forces can get up to two Gs .\n",
      "Pred Sentence: When you go around the the the Valley Valley , the , the , that are .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: And the scientists have to be completely harnessed in in order to make measurements while they &apos;re on board .\n",
      "Pred Sentence: UKN have been to to to to to to to to to to .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: So , as you can imagine , the inside of this aircraft doesn &apos;t look like any plane you would take on vacation .\n",
      "Pred Sentence: So you can imagine , in the way , the right thing is not going to be any any any .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: It &apos;s a flying laboratory that we took to make measurements in the region of this molecule .\n",
      "Pred Sentence: It &apos;s a laboratory laboratory that we can try to do .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: We do all of this to understand the chemistry of one molecule .\n",
      "Pred Sentence: We do it to to to to the molecular molecular molecule .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: And when one student like me has some sort of inclination or understanding about that molecule , they write one scientific paper on the subject .\n",
      "Pred Sentence: When my dad was a a a about or or or , , , , , , , , a a . a . . .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: And out of that field campaign we &apos;ll probably get a few dozen papers on a few dozen processes or molecules .\n",
      "Pred Sentence: And the results of that that we have able to a a a a a a of of\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: And as a body of knowledge builds up , it will form one subsection , or one sub-subsection of an assessment like the IPCC , although we have others .\n",
      "Pred Sentence: When a of of of this it it it , , a a , , a a a a or or or , , , , , ,\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: And each one of the 11 chapters of the IPCC has six to ten subsections .\n",
      "Pred Sentence: Every single chapter of the UKN of the IPCC has about 10 words .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: So you can imagine the scale of the effort .\n",
      "Pred Sentence: So you can see how this great movement of this work is going to be .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.89490079655882"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_new(enc,dec,dataloader['train_val'],en_lang, 'attention', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second with shuffled batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(vi_lang.n_words,512,512, 2).to(device)\n",
    "decoder = AttentionDecoderRNN(en_lang.n_words,512,1024,n_layers=2, attention = True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = OldEncoderRNN(vi_lang.n_words,512,bi = True).to(device)\n",
    "# decoder = OldAttentionDecoderRNN(512,en_lang.n_words, True, MAX_LEN, attention_type='t2').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr = 5e-3)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr = 5e-3)\n",
    "# encoder_optimizer = optim.SGD(encoder.parameters(), lr=0.25,nesterov=True, momentum = 0.99)\n",
    "enc_scheduler = ReduceLROnPlateau(encoder_optimizer, factor = 0.5, min_lr=5e-5,  patience=0)\n",
    "# decoder_optimizer = optim.SGD(decoder.parameters(), lr=0.25,nesterov=True, momentum = 0.99)\n",
    "dec_scheduler = ReduceLROnPlateau(decoder_optimizer, factor = 0.5, min_lr=5e-5,  patience=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 train loss = 6.663351734576734, time = 1114.1865501403809\n",
      "epoch 0 val_train loss = 8.06595925051331, time = 6.069099426269531\n",
      "validation BLEU =  1.3829197773400932\n",
      "==================================================\n",
      "epoch 1 train loss = 6.219018471040969, time = 1113.4414727687836\n",
      "epoch 1 val_train loss = 8.016192072197466, time = 6.1017372608184814\n",
      "validation BLEU =  1.1039038541592752\n",
      "==================================================\n",
      "epoch 2 train loss = 5.939835129506533, time = 1111.5073668956757\n",
      "epoch 2 val_train loss = 7.930288785639622, time = 6.0234386920928955\n",
      "validation BLEU =  1.9728202797441594\n",
      "==================================================\n",
      "epoch 3 train loss = 5.336882101169395, time = 1106.1388936042786\n",
      "epoch 3 val_train loss = 7.170681196808439, time = 6.007143020629883\n",
      "validation BLEU =  2.5507477701856334\n",
      "==================================================\n",
      "epoch 4 train loss = 4.53218613214774, time = 1105.0649061203003\n",
      "epoch 4 val_train loss = 6.974173709797182, time = 6.019273042678833\n",
      "validation BLEU =  3.7938947592150014\n",
      "==================================================\n",
      "epoch 5 train loss = 4.203932680514588, time = 1105.1447207927704\n",
      "epoch 5 val_train loss = 6.563302664350636, time = 6.0073864459991455\n",
      "validation BLEU =  4.755736464200618\n",
      "==================================================\n",
      "epoch 6 train loss = 3.974879565105863, time = 1104.7098472118378\n",
      "epoch 6 val_train loss = 6.3796891534366065, time = 6.006208419799805\n",
      "validation BLEU =  5.816073613473006\n",
      "==================================================\n",
      "epoch 7 train loss = 3.801886187122862, time = 1105.2348370552063\n",
      "epoch 7 val_train loss = 6.222200649393846, time = 5.998500347137451\n",
      "validation BLEU =  6.724281151417688\n",
      "==================================================\n",
      "epoch 8 train loss = 3.6411358793761655, time = 1105.0167949199677\n",
      "epoch 8 val_train loss = 6.088387707430481, time = 6.0096755027771\n",
      "validation BLEU =  6.327152158227848\n",
      "==================================================\n",
      "epoch 9 train loss = 3.5146704326686065, time = 1112.1327111721039\n",
      "epoch 9 val_train loss = 6.586871551790448, time = 6.0776989459991455\n",
      "validation BLEU =  6.884157529487406\n",
      "==================================================\n",
      "epoch 10 train loss = 3.257167222675697, time = 1111.7208666801453\n",
      "epoch 10 val_train loss = 6.308031389013826, time = 6.0903778076171875\n",
      "validation BLEU =  8.78825560642871\n",
      "==================================================\n",
      "epoch 11 train loss = 3.0646443947451885, time = 1111.157721042633\n",
      "epoch 11 val_train loss = 6.2458623200184915, time = 6.14197564125061\n",
      "validation BLEU =  9.280485347908634\n",
      "==================================================\n",
      "epoch 12 train loss = 2.892303344378376, time = 1111.2959926128387\n",
      "epoch 12 val_train loss = 6.21564023953507, time = 6.1025495529174805\n",
      "validation BLEU =  9.64670139749804\n",
      "==================================================\n",
      "epoch 13 train loss = 2.8429988638036345, time = 1112.0345907211304\n",
      "epoch 13 val_train loss = 6.166728167127736, time = 6.065191984176636\n",
      "validation BLEU =  9.96375722884997\n",
      "==================================================\n",
      "epoch 14 train loss = 2.7853312226978217, time = 1111.6859107017517\n",
      "epoch 14 val_train loss = 6.252849223861935, time = 6.075963973999023\n",
      "validation BLEU =  9.898069796903714\n",
      "==================================================\n",
      "Training completed. Best BLEU is 9.96375722884997\n"
     ]
    }
   ],
   "source": [
    "enc, dec, loss_hist, acc_hist = train_model(encoder_optimizer, decoder_optimizer, encoder, decoder, criterion,\\\n",
    "                                            \"attention\", dataloader,en_lang, num_epochs = 15, rm = 0.95,\\\n",
    "                                           enc_scheduler = enc_scheduler, dec_scheduler = dec_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Sentence: we run enormous models on supercomputers ; this is what i happen to do .\n",
      "Pred Sentence: we published the code of the code ; it &apos;s the most beautiful thing .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: and as you come around the banks in these valleys , the forces can get up to two gs .\n",
      "Pred Sentence: when the light is flowing down , the river is able to move the UKN to the UKN .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: because it &apos;s important to the atmospheric system , we go to all lengths to study this thing .\n",
      "Pred Sentence: it &apos;s because it &apos;s important to the system , the system , we also have this system that we also have .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: and one of the molecules i study is called isoprene , which is here . it &apos;s a small organic molecule . you &apos;ve probably never heard of it .\n",
      "Pred Sentence: one of the molecules i called the called the UKN . this is a UKN . it &apos;s a UKN . you can &apos;t have a name .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: and as a body of knowledge builds up , it will form one subsection , or one sub-subsection of an assessment like the ipcc , although we have others .\n",
      "Pred Sentence: when a trip to a project , it will create a UKN , a UKN , a UKN , a UKN , a UKN , same project .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: so you can imagine the scale of the effort .\n",
      "Pred Sentence: so let me demonstrate you imagine the potential of these animals .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: and because it &apos;s so much stuff , it &apos;s really important for the atmospheric system .\n",
      "Pred Sentence: so because the emissions are very low , it &apos;s very important to the atmospheric system .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: we have to fly at a special incline in order to make the measurements .\n",
      "Pred Sentence: we have to fly to the surface of the UKN .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: the weight of a paper clip is approximately equal to 900 zeta-illion -- 10 to the 21st -- molecules of isoprene .\n",
      "Pred Sentence: the UKN of a UKN UKN -- UKN -- UKN -- UKN -- UKN -- UKN -- UKN .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: we also fly all over the world looking for this thing .\n",
      "Pred Sentence: we &apos;re flying around the world to see this molecule .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: and we perform dozens of integrations in order to understand what &apos;s happening .\n",
      "Pred Sentence: we need to produce the big challenge to understand what &apos;s happening .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: so , as you can imagine , the inside of this aircraft doesn &apos;t look like any plane you would take on vacation .\n",
      "Pred Sentence: so , you can imagine , there &apos;s a little bit of UKN , that &apos;s not the same .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: it &apos;s a flying laboratory that we took to make measurements in the region of this molecule .\n",
      "Pred Sentence: it &apos;s a UKN laboratory to do the simulations .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: this is the tower in the middle of the rainforest , from above .\n",
      "Pred Sentence: this is the sea between the ocean , the bottom of the bottom .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: and this plane , the model , ba146 , which was run by faam , normally flies 120 to 130 people .\n",
      "Pred Sentence: this is the UKN UKN from UKN , from the UKN of the people .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: it &apos;s a huge amount of stuff . it &apos;s equal to the weight of methane .\n",
      "Pred Sentence: it &apos;s a huge amount of carbon footprint , the size of the UKN .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: i &apos;d like to talk to you today about the scale of the scientific effort that goes into making the headlines you see in the paper .\n",
      "Pred Sentence: i want to show you about the psychology of the scientific studies that you see in the past , you see the results in the past .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: and each one of the 11 chapters of the ipcc has six to ten subsections .\n",
      "Pred Sentence: each of the UKN of the UKN of the UKN of the UKN of the UKN .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: but still , we look at the pieces .\n",
      "Pred Sentence: yet , we looked at the little little bit of UKN .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: but we didn &apos;t just fly it . we were flying at 100 meters above the top of the canopy to measure this molecule -- incredibly dangerous stuff .\n",
      "Pred Sentence: we &apos;re not only 10 feet away from the skin , and we found how to fly the surface of this .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: in 4 minutes , atmospheric chemist rachel pike provides a glimpse of the massive scientific effort behind the bold headlines on climate change , with her team -- one of thousands who contributed -- taking a risky flight over the rainforest in pursuit of data on a key molecule .\n",
      "Pred Sentence: in 4 minutes , the UKN UKN , the UKN UKN , the UKN of the UKN UKN , the UKN of the UKN UKN , the UKN of the UKN of the UKN , the UKN of the UKN of the UKN of the UKN of the UKN to the UKN of the UKN .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: and every one of those scientists is in a research group , and every research group studies a wide variety of topics .\n",
      "Pred Sentence: every scientific research requires a large web , and each of these are different kinds of institutions .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: and all of those pages were reviewed by another 400-plus scientists and reviewers , from 113 countries .\n",
      "Pred Sentence: and all of the words that are all about the science of science and science and UKN .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: atmospheric explosions , or full combustion , takes about 15,000 times longer than what happens in your car .\n",
      "Pred Sentence: atmospheric explosions in the air is slightly UKN to the UKN of the UKN .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: headlines that look like this when they have to do with climate change , and headlines that look like this when they have to do with air quality or smog .\n",
      "Pred Sentence: headlines like the UKN , and when the weather is , and when the weather is the same as the quality of the car .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: but despite its very small weight , enough of it is emitted into the atmosphere every year to equal the weight of all the people on the planet .\n",
      "Pred Sentence: whatever the higgs size is , 30 , 30 , 30 , 30 degrees celsius are UKN by the atmosphere .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: our models have hundreds of thousands of grid boxes calculating hundreds of variables each , on minute timescales .\n",
      "Pred Sentence: our models of thousands of thousands of UKN UKN in the UKN of the UKN .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Sentence: we blow it up and look at the pieces .\n",
      "Pred Sentence: we showed it the same thing and the UKN .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: that report was written by 620 scientists from 40 countries .\n",
      "Pred Sentence: the study was written by the UKN from the national academy .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: and on part of that field campaign we even brought an aircraft with us .\n",
      "Pred Sentence: there are a thousand UKN .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: and when one student like me has some sort of inclination or understanding about that molecule , they write one scientific paper on the subject .\n",
      "Pred Sentence: when i got the idea of the library , that &apos;s what that synthesis was , that they &apos;re going to be able to study that same thing with that same thing .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: so maybe you took a similar aircraft to get here today .\n",
      "Pred Sentence: it may be you on a long time , which is a very typical thing .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: i recently joined a field campaign in malaysia . there are others .\n",
      "Pred Sentence: i found a survey in the UKN of the other .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: we found a global atmospheric watchtower there , in the middle of the rainforest , and hung hundreds of thousands of dollars worth of scientific equipment off this tower , to look for isoprene , and of course , other things while we were there .\n",
      "Pred Sentence: we found that a UKN of the UKN , the UKN of the water , and we &apos;ve been UKN in the process of the thousands of thousands of dollars to see the UKN of the UKN , and the UKN of the UKN .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: and out of that field campaign we &apos;ll probably get a few dozen papers on a few dozen processes or molecules .\n",
      "Pred Sentence: and that video we &apos;re going to take hundreds of millions of producers that are helping hundreds of different machines .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: recently the headlines looked like this when the intergovernmental panel on climate change , or ipcc , put out their report on the state of understanding of the atmospheric system .\n",
      "Pred Sentence: the tunnels recently happened when the earthquake was UKN , like UKN , UKN , UKN , the UKN of the UKN of the united states .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: and it takes weeks to perform our integrations .\n",
      "Pred Sentence: it &apos;s just a new way of the UKN .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: we have to get special flight clearance .\n",
      "Pred Sentence: it &apos;s like a UKN device .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: and this is the tower from below .\n",
      "Pred Sentence: and from the UKN .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: over 15,000 scientists go to san francisco every year for that .\n",
      "Pred Sentence: over 15,000 years , more than the science of san francisco to go to this website .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: they wrote almost a thousand pages on the topic .\n",
      "Pred Sentence: they were familiar with this conference .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: and the scientists have to be completely harnessed in in order to make measurements while they &apos;re on board .\n",
      "Pred Sentence: the scientists have to get into the hurtigruten to do the hurtigruten to do the hurtigruten .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: for us at cambridge , it &apos;s as varied as the el niño oscillation , which affects weather and climate , to the assimilation of satellite data , to emissions from crops that produce biofuels , which is what i happen to study .\n",
      "Pred Sentence: for us , in the west , the UKN of the UKN , the UKN of the UKN , the UKN , the UKN , the UKN , the UKN , the UKN , the UKN , the UKN , the UKN , the extracellular matrix .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: and in each one of these research areas , of which there are even more , there are phd students , like me , and we study incredibly narrow topics , things as narrow as a few processes or a few molecules .\n",
      "Pred Sentence: each research is more UKN , and the UKN , and the UKN , i have to have a few different kinds of UKN , like a few different kinds of maps .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: it &apos;s a big community . it &apos;s such a big community , in fact , that our annual gathering is the largest scientific meeting in the world .\n",
      "Pred Sentence: it &apos;s a huge community , the big community of the UKN of our UKN UKN , the largest UKN of the world .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: we hire military and test pilots to do the maneuvering .\n",
      "Pred Sentence: it &apos;s 27 miles and UKN , and UKN to fly .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: they are both two branches of the same field of atmospheric science .\n",
      "Pred Sentence: both both of the entire entire network of the future in the future .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: this is the euphore smog chamber in spain .\n",
      "Pred Sentence: this is the UKN smog that UKN the UKN in spain .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: rachel pike : the science behind a climate headline\n",
      "Pred Sentence: rachel science : a climate change .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: we do all of this to understand the chemistry of one molecule .\n",
      "Pred Sentence: we do all the data to make a molecule of molecular molecules .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.350793619403419"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_new(enc,dec,dataloader['train_val'],en_lang, 'attention', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(vi_lang.n_words,512,512, 2).to(device)\n",
    "decoder = AttentionDecoderRNN(en_lang.n_words,512,1024,n_layers=2, attention = True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = OldEncoderRNN(vi_lang.n_words,512,bi = True).to(device)\n",
    "# decoder = OldAttentionDecoderRNN(512,en_lang.n_words, True, MAX_LEN, attention_type='t2').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_optimizer = optim.Adam(encoder.parameters(), lr = 5e-3)\n",
    "# decoder_optimizer = optim.Adam(decoder.parameters(), lr = 5e-3)\n",
    "encoder_optimizer = optim.SGD(encoder.parameters(), lr=0.25,nesterov=True, momentum = 0.99)\n",
    "enc_scheduler = ReduceLROnPlateau(encoder_optimizer, min_lr=1e-4,  patience=0)\n",
    "decoder_optimizer = optim.SGD(decoder.parameters(), lr=0.25,nesterov=True, momentum = 0.99)\n",
    "dec_scheduler = ReduceLROnPlateau(decoder_optimizer, min_lr=1e-4,  patience=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 train loss = 5.393191720143928, time = 1110.3711938858032\n",
      "epoch 0 val_train loss = 6.350985495449993, time = 6.045475959777832\n",
      "validation BLEU =  6.097490864225506\n",
      "==================================================\n",
      "epoch 1 train loss = 3.7876384918800445, time = 1109.0049946308136\n",
      "epoch 1 val_train loss = 6.497099796680246, time = 6.041832447052002\n",
      "validation BLEU =  14.827929089259817\n",
      "==================================================\n",
      "epoch 2 train loss = 3.2587029963508893, time = 1110.3191792964935\n",
      "epoch 2 val_train loss = 6.220447167237098, time = 6.061037063598633\n",
      "validation BLEU =  16.523216268913465\n",
      "==================================================\n",
      "epoch 3 train loss = 3.1221366043516166, time = 1110.687434911728\n",
      "epoch 3 val_train loss = 6.181964690752962, time = 6.036092281341553\n",
      "validation BLEU =  17.562226160158012\n",
      "==================================================\n",
      "epoch 4 train loss = 3.023102364547647, time = 1109.6038773059845\n",
      "epoch 4 val_train loss = 6.166984209126855, time = 6.062610387802124\n",
      "validation BLEU =  18.338058140978138\n",
      "==================================================\n",
      "epoch 5 train loss = 3.008821606100434, time = 1111.0691175460815\n",
      "epoch 5 val_train loss = 5.993662122672289, time = 6.037784814834595\n",
      "validation BLEU =  19.103652429072163\n",
      "==================================================\n",
      "epoch 6 train loss = 2.874202186123477, time = 1110.6227493286133\n",
      "epoch 6 val_train loss = 6.137065851349936, time = 6.050750732421875\n",
      "validation BLEU =  19.761342219527542\n",
      "==================================================\n",
      "epoch 7 train loss = 2.813067223088859, time = 1110.6979339122772\n",
      "epoch 7 val_train loss = 6.093534018339043, time = 6.0591864585876465\n",
      "validation BLEU =  20.121189849660183\n",
      "==================================================\n",
      "epoch 8 train loss = 2.778007769692494, time = 1110.9508564472198\n",
      "epoch 8 val_train loss = 6.084373299631786, time = 6.041238784790039\n",
      "validation BLEU =  20.08458956829022\n",
      "==================================================\n",
      "epoch 9 train loss = 2.8003427863154116, time = 1110.972732782364\n",
      "epoch 9 val_train loss = 6.079099006833339, time = 6.124432563781738\n",
      "validation BLEU =  20.046320437336377\n",
      "==================================================\n",
      "epoch 10 train loss = 2.8004198814363535, time = 1111.1951942443848\n",
      "epoch 10 val_train loss = 6.067473589057802, time = 6.0821568965911865\n",
      "validation BLEU =  20.042196261705072\n",
      "==================================================\n",
      "epoch 11 train loss = 2.7765031623188654, time = 1110.4013135433197\n",
      "epoch 11 val_train loss = 6.0796619845489595, time = 6.123585224151611\n",
      "validation BLEU =  20.108562744714874\n",
      "==================================================\n",
      "epoch 12 train loss = 2.809542427973835, time = 1110.2485928535461\n",
      "epoch 12 val_train loss = 6.0691356779273, time = 6.099574327468872\n",
      "validation BLEU =  20.079145745298263\n",
      "==================================================\n",
      "epoch 13 train loss = 2.7845728380737653, time = 1110.8955845832825\n",
      "epoch 13 val_train loss = 6.072497662685647, time = 6.073423624038696\n",
      "validation BLEU =  20.033281668548003\n",
      "==================================================\n",
      "epoch 14 train loss = 2.7938509307882704, time = 1111.2742054462433\n",
      "epoch 14 val_train loss = 6.07371631405707, time = 6.063115835189819\n",
      "validation BLEU =  20.04215951917312\n",
      "==================================================\n",
      "Training completed. Best BLEU is 20.121189849660183\n"
     ]
    }
   ],
   "source": [
    "enc, dec, loss_hist, acc_hist = train_model(encoder_optimizer, decoder_optimizer, encoder, decoder, criterion,\\\n",
    "                                            \"attention\", dataloader,en_lang, num_epochs = 15, rm = 0.95,\\\n",
    "                                           enc_scheduler = enc_scheduler, dec_scheduler = dec_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Sentence: so maybe you took a similar aircraft to get here today .\n",
      "Pred Sentence: you can have on a future where you come here .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: and the scientists have to be completely harnessed in in order to make measurements while they &apos;re on board .\n",
      "Pred Sentence: scientists have to be UKN completely on the chair to make the UKN on the airplane .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: we found a global atmospheric watchtower there , in the middle of the rainforest , and hung hundreds of thousands of dollars worth of scientific equipment off this tower , to look for isoprene , and of course , other things while we were there .\n",
      "Pred Sentence: we found a global climate in which , right in the middle of the middle , and we &apos;ve UKN hundreds of thousands of dollars from the UKN to find UKN , and of course , other things that are in the time there .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: our models have hundreds of thousands of grid boxes calculating hundreds of variables each , on minute timescales .\n",
      "Pred Sentence: our model of thousands of thousands of UKN UKN with hundreds of UKN in time .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: we have to fly at a special incline in order to make the measurements .\n",
      "Pred Sentence: we have to fly with a special way to do the UKN .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: and when one student like me has some sort of inclination or understanding about that molecule , they write one scientific paper on the subject .\n",
      "Pred Sentence: when i was as a UKN or a sense of that molecule , like , they would write a scientific study of that .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: and it takes weeks to perform our integrations .\n",
      "Pred Sentence: it &apos;s still going to have new weeks to do that we can do .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: headlines that look like this when they have to do with climate change , and headlines that look like this when they have to do with air quality or smog .\n",
      "Pred Sentence: there are these things like this when the table of climate change , and like this as a talk about the air quality or UKN .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: this is the euphore smog chamber in spain .\n",
      "Pred Sentence: this is the UKN UKN lab in the west\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: this is the tower in the middle of the rainforest , from above .\n",
      "Pred Sentence: this is the UKN between the middle of the UKN , look from the top .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: recently the headlines looked like this when the intergovernmental panel on climate change , or ipcc , put out their report on the state of understanding of the atmospheric system .\n",
      "Pred Sentence: the recent ones look like this when the UKN climate change , called the UKN , is the UKN to put their research on the atmosphere .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: but despite its very small weight , enough of it is emitted into the atmosphere every year to equal the weight of all the people on the planet .\n",
      "Pred Sentence: although that &apos;s very small , the amount of UKN is UKN in the atmosphere of the past the\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: but still , we look at the pieces .\n",
      "Pred Sentence: and yet , we &apos;re looking at a little bit of little .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: they wrote almost a thousand pages on the topic .\n",
      "Pred Sentence: they wrote about 1,000 pages of this .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: and all of those pages were reviewed by another 400-plus scientists and reviewers , from 113 countries .\n",
      "Pred Sentence: and all the pages are looking at by 400 science and another UKN from UKN countries .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: atmospheric explosions , or full combustion , takes about 15,000 times longer than what happens in your car .\n",
      "Pred Sentence: it &apos;s in the air or UKN , completely UKN than UKN times in the car .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: and in each one of these research areas , of which there are even more , there are phd students , like me , and we study incredibly narrow topics , things as narrow as a few processes or a few molecules .\n",
      "Pred Sentence: each of the study of studies is UKN out of smaller areas , and the students have in a UKN , like me , to study very specific , specific , specific , specific objects .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: and as a body of knowledge builds up , it will form one subsection , or one sub-subsection of an assessment like the ipcc , although we have others .\n",
      "Pred Sentence: when a whole part of knowledge is UKN , it &apos;s going to make a UKN , or a UKN in a UKN , even though there &apos;s many other .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: we hire military and test pilots to do the maneuvering .\n",
      "Pred Sentence: it has to be UKN and UKN UKN to control the airplane .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: and each one of the 11 chapters of the ipcc has six to ten subsections .\n",
      "Pred Sentence: every single one of the UKN of the UKN has from six to 10 .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: we have to get special flight clearance .\n",
      "Pred Sentence: so , you have to be a special UKN that allows you .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: it &apos;s a flying laboratory that we took to make measurements in the region of this molecule .\n",
      "Pred Sentence: it &apos;s a mobile lab that we can help us do the UKN .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: and this plane , the model , ba146 , which was run by faam , normally flies 120 to 130 people .\n",
      "Pred Sentence: this is , the UKN model , UKN UKN , can UKN from UKN .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: that report was written by 620 scientists from 40 countries .\n",
      "Pred Sentence: research was written by UKN scientists from 40 different countries .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: so you can imagine the scale of the effort .\n",
      "Pred Sentence: so , you can imagine that the scale of these effort .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: but we didn &apos;t just fly it . we were flying at 100 meters above the top of the canopy to measure this molecule -- incredibly dangerous stuff .\n",
      "Pred Sentence: we &apos;re not just fly . we fly the UKN of the 100 meters to measure this molecule -- it &apos;s very dangerous .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: i recently joined a field campaign in malaysia . there are others .\n",
      "Pred Sentence: i was in a UKN study in UKN . there &apos;s a lot of other .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: rachel pike : the science behind a climate headline\n",
      "Pred Sentence: science behind a UKN of climate .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: we blow it up and look at the pieces .\n",
      "Pred Sentence: we put it out and look at a little bit of little .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Sentence: we run enormous models on supercomputers ; this is what i happen to do .\n",
      "Pred Sentence: we run huge models of these computers ; this is my work .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: and because it &apos;s so much stuff , it &apos;s really important for the atmospheric system .\n",
      "Pred Sentence: because the amount of carbon is large , it means important to the atmosphere system .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: i &apos;d like to talk to you today about the scale of the scientific effort that goes into making the headlines you see in the paper .\n",
      "Pred Sentence: i want to show you about the great UKN of science that have been used to make the UKN you see on the page .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: we also fly all over the world looking for this thing .\n",
      "Pred Sentence: we &apos;ve been flying around the world to find this molecule .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: it &apos;s a huge amount of stuff . it &apos;s equal to the weight of methane .\n",
      "Pred Sentence: it &apos;s a huge amount of carbon emissions , by the UKN of UKN .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: we do all of this to understand the chemistry of one molecule .\n",
      "Pred Sentence: we do all of the way to understand the chemical of a molecule .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: in 4 minutes , atmospheric chemist rachel pike provides a glimpse of the massive scientific effort behind the bold headlines on climate change , with her team -- one of thousands who contributed -- taking a risky flight over the rainforest in pursuit of data on a key molecule .\n",
      "Pred Sentence: in four minutes , the chemical chemical UKN UKN shows the UKN of the scientific UKN of the climate climate , with the with his research -- thousands of people who have been UKN for this project -- a UKN UKN UKN to search of the information of a unique molecule .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: and one of the molecules i study is called isoprene , which is here . it &apos;s a small organic molecule . you &apos;ve probably never heard of it .\n",
      "Pred Sentence: one of the molecules i &apos;ve been working called is UKN . this is a little UKN molecule . you can never heard the name .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: because it &apos;s important to the atmospheric system , we go to all lengths to study this thing .\n",
      "Pred Sentence: it means that important meaning with the atmosphere system , which is the way we &apos;ve been doing this research .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: they are both two branches of the same field of atmospheric science .\n",
      "Pred Sentence: both of them are a UKN of the same area in the atmosphere industry .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: it &apos;s a big community . it &apos;s such a big community , in fact , that our annual gathering is the largest scientific meeting in the world .\n",
      "Pred Sentence: it &apos;s a big community , which is actually in fact that our year of our year is the largest UKN of the world &apos;s largest .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: and on part of that field campaign we even brought an aircraft with us .\n",
      "Pred Sentence: there &apos;s a stage we &apos;ve got the airplane .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: and as you come around the banks in these valleys , the forces can get up to two gs .\n",
      "Pred Sentence: when we fly around the river of the ocean , the forces that can be on the UKN .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: and we perform dozens of integrations in order to understand what &apos;s happening .\n",
      "Pred Sentence: we need to take a little bit of UKN that we have to understand what &apos;s going on .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: and every one of those scientists is in a research group , and every research group studies a wide variety of topics .\n",
      "Pred Sentence: every single science of the research , and each of each groups have been studying a lot of diversity .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: for us at cambridge , it &apos;s as varied as the el niño oscillation , which affects weather and climate , to the assimilation of satellite data , to emissions from crops that produce biofuels , which is what i happen to study .\n",
      "Pred Sentence: for us , in cambridge , the main UKN of the UKN of UKN , the the climate , the climate , the UKN , the information from the , the of the UKN from the of the , the of the of the , the\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: the weight of a paper clip is approximately equal to 900 zeta-illion -- 10 to the 21st -- molecules of isoprene .\n",
      "Pred Sentence: the weight of a paper is about UKN UKN -- 10 UKN -- the molecule UKN .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: and this is the tower from below .\n",
      "Pred Sentence: and from the ground .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: and out of that field campaign we &apos;ll probably get a few dozen papers on a few dozen processes or molecules .\n",
      "Pred Sentence: and that &apos;s UKN we &apos;ll have a bunch of studies about hundreds of UKN and molecules .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: over 15,000 scientists go to san francisco every year for that .\n",
      "Pred Sentence: every year , more than the UKN of san francisco to this conference .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "True Sentence: so , as you can imagine , the inside of this aircraft doesn &apos;t look like any plane you would take on vacation .\n",
      "Pred Sentence: so , you can imagine , inside it is completely not the same with any other UKN .\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12.301247761128733"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_new(enc,dec,dataloader['train_val'],en_lang, 'attention', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(enc.state_dict(), 'best_att_encoder.pth')\n",
    "torch.save(dec.state_dict(), 'best_att_decoder.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_wo_att = EncoderRNN(vi_lang.n_words,512,512, 2).to(device)\n",
    "decoder_wo_att = AttentionDecoderRNN(en_lang.n_words,512,1024,n_layers=2, attention = False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = OldEncoderRNN(vi_lang.n_words,512,bi = True).to(device)\n",
    "# decoder = OldAttentionDecoderRNN(512,en_lang.n_words, True, MAX_LEN, attention_type='t2').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr = 5e-4)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr = 5e-4)\n",
    "# encoder_optimizer = optim.SGD(encoder_wo_att.parameters(), lr=0.25,nesterov=True, momentum = 0.99)\n",
    "enc_scheduler = ReduceLROnPlateau(encoder_optimizer, min_lr=1e-5,factor = 0.5,  patience=0)\n",
    "# decoder_optimizer = optim.SGD(decoder_wo_att.parameters(), lr=0.25,nesterov=True, momentum = 0.99)\n",
    "dec_scheduler = ReduceLROnPlateau(decoder_optimizer, min_lr=1e-5,factor = 0.5,  patience=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 train loss = 3.118959676665268, time = 637.1421394348145\n",
      "epoch 0 val_train loss = 5.432344403552708, time = 3.092496395111084\n",
      "validation BLEU =  7.919194767575914\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "encoder_wo_att, decoder_wo_att, loss_hist, acc_hist = train_model(encoder_optimizer, decoder_optimizer, encoder_wo_att, decoder_wo_att, criterion,\\\n",
    "                                            \"attention\", dataloader,en_lang, num_epochs = 15, rm = 0.95,\\\n",
    "                                           enc_scheduler = enc_scheduler, dec_scheduler = dec_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_new(encoder_wo_att, decoder_wo_att,dataloader['train_val'],en_lang, 'attention', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
