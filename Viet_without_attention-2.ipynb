{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pdb\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IDX = 2\n",
    "PAD_IDX = 3\n",
    "SOS_token = 0\n",
    "EOS_token = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(file):\n",
    "    f = open(file)\n",
    "    list_l = []\n",
    "    for line in f:\n",
    "        list_l.append(line.strip())\n",
    "    df = pd.DataFrame()\n",
    "    df['data'] = list_l\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_train = read_dataset(\"./iwslt-vi-en/train.tok.en\")\n",
    "en_val = read_dataset(\"./iwslt-vi-en/dev.tok.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_train = read_dataset(\"./iwslt-vi-en/train.tok.vi\")\n",
    "vi_val = read_dataset(\"./iwslt-vi-en/dev.tok.vi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(133317, 1) (133317, 1)\n",
      "(1268, 1) (1268, 1)\n"
     ]
    }
   ],
   "source": [
    "print(en_train.shape,vi_train.shape)\n",
    "print(en_val.shape,vi_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame()\n",
    "train['en_data'] = en_train['data']\n",
    "train['vi_data'] = vi_train['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "en_data                  \n",
       "vi_data    \" tán_thưởng \"\n",
       "Name: 38600, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.ix[38600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = pd.DataFrame()\n",
    "val['en_data'] = en_val['data']\n",
    "val['vi_data'] = vi_val['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(133317, 2) (1268, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape,val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2:\"UKN\",3:\"PAD\"}\n",
    "        self.n_words = 4  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word.lower())\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['en_data'] = train['en_data'].apply(lambda x: x.lower())\n",
    "# train['vi_data'] = train['vi_data'].apply(lambda x:x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_lang = Lang(\"en\")\n",
    "for ex in train['en_data']:\n",
    "#     ex = unicodeToAscii(ex)\n",
    "#     ex = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", ex)\n",
    "    en_lang.addSentence(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_lang = Lang(\"vi\")\n",
    "for ex in train['vi_data']:\n",
    "#     ex = unicodeToAscii(ex)\n",
    "#     ex = re.sub(r\"[^a-zA-Z.!?_]+\", r\" \", ex)\n",
    "    vi_lang.addSentence(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vi_lang.word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(df):\n",
    "    df['en_tokenized'] = df[\"en_data\"].apply(lambda x:x.lower().split( ))\n",
    "    df['vi_tokenized'] = df['vi_data'].apply(lambda x:x.lower().split( ))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = split(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = split(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en_data</th>\n",
       "      <th>vi_data</th>\n",
       "      <th>en_tokenized</th>\n",
       "      <th>vi_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rachel Pike : The science behind a climate hea...</td>\n",
       "      <td>Khoa_học đằng_sau một tiêu_đề về khí_hậu</td>\n",
       "      <td>[rachel, pike, :, the, science, behind, a, cli...</td>\n",
       "      <td>[khoa_học, đằng_sau, một, tiêu_đề, về, khí_hậu]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In 4 minutes , atmospheric chemist Rachel Pike...</td>\n",
       "      <td>Trong 4 phút , chuyên_gia hoá_học khí_quyển Ra...</td>\n",
       "      <td>[in, 4, minutes, ,, atmospheric, chemist, rach...</td>\n",
       "      <td>[trong, 4, phút, ,, chuyên_gia, hoá_học, khí_q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I &amp;apos;d like to talk to you today about the ...</td>\n",
       "      <td>Tôi muốn cho các bạn biết về sự to_lớn của nhữ...</td>\n",
       "      <td>[i, &amp;apos;d, like, to, talk, to, you, today, a...</td>\n",
       "      <td>[tôi, muốn, cho, các, bạn, biết, về, sự, to_lớ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Headlines that look like this when they have t...</td>\n",
       "      <td>Có những dòng trông như thế_này khi bàn về biế...</td>\n",
       "      <td>[headlines, that, look, like, this, when, they...</td>\n",
       "      <td>[có, những, dòng, trông, như, thế_này, khi, bà...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>They are both two branches of the same field o...</td>\n",
       "      <td>Cả hai đều là một nhánh của cùng một lĩnh_vực ...</td>\n",
       "      <td>[they, are, both, two, branches, of, the, same...</td>\n",
       "      <td>[cả, hai, đều, là, một, nhánh, của, cùng, một,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             en_data  \\\n",
       "0  Rachel Pike : The science behind a climate hea...   \n",
       "1  In 4 minutes , atmospheric chemist Rachel Pike...   \n",
       "2  I &apos;d like to talk to you today about the ...   \n",
       "3  Headlines that look like this when they have t...   \n",
       "4  They are both two branches of the same field o...   \n",
       "\n",
       "                                             vi_data  \\\n",
       "0           Khoa_học đằng_sau một tiêu_đề về khí_hậu   \n",
       "1  Trong 4 phút , chuyên_gia hoá_học khí_quyển Ra...   \n",
       "2  Tôi muốn cho các bạn biết về sự to_lớn của nhữ...   \n",
       "3  Có những dòng trông như thế_này khi bàn về biế...   \n",
       "4  Cả hai đều là một nhánh của cùng một lĩnh_vực ...   \n",
       "\n",
       "                                        en_tokenized  \\\n",
       "0  [rachel, pike, :, the, science, behind, a, cli...   \n",
       "1  [in, 4, minutes, ,, atmospheric, chemist, rach...   \n",
       "2  [i, &apos;d, like, to, talk, to, you, today, a...   \n",
       "3  [headlines, that, look, like, this, when, they...   \n",
       "4  [they, are, both, two, branches, of, the, same...   \n",
       "\n",
       "                                        vi_tokenized  \n",
       "0    [khoa_học, đằng_sau, một, tiêu_đề, về, khí_hậu]  \n",
       "1  [trong, 4, phút, ,, chuyên_gia, hoá_học, khí_q...  \n",
       "2  [tôi, muốn, cho, các, bạn, biết, về, sự, to_lớ...  \n",
       "3  [có, những, dòng, trông, như, thế_này, khi, bà...  \n",
       "4  [cả, hai, đều, là, một, nhánh, của, cùng, một,...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train[train['vi_len']==713]['en_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vi_lang.word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token2index_dataset(df):\n",
    "    for lan in ['en','vi']:\n",
    "        indices_data = []\n",
    "        if lan=='en':\n",
    "            lang_obj = en_lang\n",
    "        else:\n",
    "            lang_obj = vi_lang\n",
    "        for tokens in df[lan+'_tokenized']:\n",
    "            index_list = [lang_obj.word2index[token] if token in lang_obj.word2index else UNK_IDX for token in tokens]\n",
    "            index_list.append(EOS_token)\n",
    "#             index_list.insert(0,SOS_token)\n",
    "            indices_data.append(index_list)\n",
    "        df[lan+'_idized'] = indices_data\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = token2index_dataset(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = token2index_dataset(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vietnamese(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        english = self.df.iloc[idx]['en_idized']\n",
    "        viet = self.df.iloc[idx]['vi_idized']\n",
    "        en_len = self.df.iloc[idx]['en_len']\n",
    "        vi_len = self.df.iloc[idx]['vi_len']\n",
    "        return [english,viet,en_len,vi_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en_data</th>\n",
       "      <th>vi_data</th>\n",
       "      <th>en_tokenized</th>\n",
       "      <th>vi_tokenized</th>\n",
       "      <th>en_idized</th>\n",
       "      <th>vi_idized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rachel Pike : The science behind a climate hea...</td>\n",
       "      <td>Khoa_học đằng_sau một tiêu_đề về khí_hậu</td>\n",
       "      <td>[rachel, pike, :, the, science, behind, a, cli...</td>\n",
       "      <td>[khoa_học, đằng_sau, một, tiêu_đề, về, khí_hậu]</td>\n",
       "      <td>[4, 5, 6, 7, 8, 9, 10, 11, 12, 1]</td>\n",
       "      <td>[4, 5, 6, 7, 8, 9, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In 4 minutes , atmospheric chemist Rachel Pike...</td>\n",
       "      <td>Trong 4 phút , chuyên_gia hoá_học khí_quyển Ra...</td>\n",
       "      <td>[in, 4, minutes, ,, atmospheric, chemist, rach...</td>\n",
       "      <td>[trong, 4, phút, ,, chuyên_gia, hoá_học, khí_q...</td>\n",
       "      <td>[13, 14, 15, 16, 17, 18, 4, 5, 19, 10, 20, 21,...</td>\n",
       "      <td>[10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I &amp;apos;d like to talk to you today about the ...</td>\n",
       "      <td>Tôi muốn cho các bạn biết về sự to_lớn của nhữ...</td>\n",
       "      <td>[i, &amp;apos;d, like, to, talk, to, you, today, a...</td>\n",
       "      <td>[tôi, muốn, cho, các, bạn, biết, về, sự, to_lớ...</td>\n",
       "      <td>[47, 48, 49, 50, 51, 50, 52, 53, 54, 7, 55, 21...</td>\n",
       "      <td>[52, 53, 38, 54, 55, 56, 8, 57, 58, 30, 21, 22...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Headlines that look like this when they have t...</td>\n",
       "      <td>Có những dòng trông như thế_này khi bàn về biế...</td>\n",
       "      <td>[headlines, that, look, like, this, when, they...</td>\n",
       "      <td>[có, những, dòng, trông, như, thế_này, khi, bà...</td>\n",
       "      <td>[26, 56, 62, 49, 63, 64, 65, 66, 50, 67, 29, 1...</td>\n",
       "      <td>[67, 21, 61, 68, 69, 70, 71, 72, 8, 25, 9, 13,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>They are both two branches of the same field o...</td>\n",
       "      <td>Cả hai đều là một nhánh của cùng một lĩnh_vực ...</td>\n",
       "      <td>[they, are, both, two, branches, of, the, same...</td>\n",
       "      <td>[cả, hai, đều, là, một, nhánh, của, cùng, một,...</td>\n",
       "      <td>[65, 73, 74, 75, 76, 21, 7, 77, 78, 21, 17, 8,...</td>\n",
       "      <td>[80, 81, 82, 83, 6, 84, 30, 26, 6, 85, 10, 86,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             en_data  \\\n",
       "0  Rachel Pike : The science behind a climate hea...   \n",
       "1  In 4 minutes , atmospheric chemist Rachel Pike...   \n",
       "2  I &apos;d like to talk to you today about the ...   \n",
       "3  Headlines that look like this when they have t...   \n",
       "4  They are both two branches of the same field o...   \n",
       "\n",
       "                                             vi_data  \\\n",
       "0           Khoa_học đằng_sau một tiêu_đề về khí_hậu   \n",
       "1  Trong 4 phút , chuyên_gia hoá_học khí_quyển Ra...   \n",
       "2  Tôi muốn cho các bạn biết về sự to_lớn của nhữ...   \n",
       "3  Có những dòng trông như thế_này khi bàn về biế...   \n",
       "4  Cả hai đều là một nhánh của cùng một lĩnh_vực ...   \n",
       "\n",
       "                                        en_tokenized  \\\n",
       "0  [rachel, pike, :, the, science, behind, a, cli...   \n",
       "1  [in, 4, minutes, ,, atmospheric, chemist, rach...   \n",
       "2  [i, &apos;d, like, to, talk, to, you, today, a...   \n",
       "3  [headlines, that, look, like, this, when, they...   \n",
       "4  [they, are, both, two, branches, of, the, same...   \n",
       "\n",
       "                                        vi_tokenized  \\\n",
       "0    [khoa_học, đằng_sau, một, tiêu_đề, về, khí_hậu]   \n",
       "1  [trong, 4, phút, ,, chuyên_gia, hoá_học, khí_q...   \n",
       "2  [tôi, muốn, cho, các, bạn, biết, về, sự, to_lớ...   \n",
       "3  [có, những, dòng, trông, như, thế_này, khi, bà...   \n",
       "4  [cả, hai, đều, là, một, nhánh, của, cùng, một,...   \n",
       "\n",
       "                                           en_idized  \\\n",
       "0                  [4, 5, 6, 7, 8, 9, 10, 11, 12, 1]   \n",
       "1  [13, 14, 15, 16, 17, 18, 4, 5, 19, 10, 20, 21,...   \n",
       "2  [47, 48, 49, 50, 51, 50, 52, 53, 54, 7, 55, 21...   \n",
       "3  [26, 56, 62, 49, 63, 64, 65, 66, 50, 67, 29, 1...   \n",
       "4  [65, 73, 74, 75, 76, 21, 7, 77, 78, 21, 17, 8,...   \n",
       "\n",
       "                                           vi_idized  \n",
       "0                              [4, 5, 6, 7, 8, 9, 1]  \n",
       "1  [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 8...  \n",
       "2  [52, 53, 38, 54, 55, 56, 8, 57, 58, 30, 21, 22...  \n",
       "3  [67, 21, 61, 68, 69, 70, 71, 72, 8, 25, 9, 13,...  \n",
       "4  [80, 81, 82, 83, 6, 84, 30, 26, 6, 85, 10, 86,...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['en_len'] = train['en_idized'].apply(lambda x: len(x))\n",
    "train['vi_len'] = train['vi_idized'].apply(lambda x:len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "val['en_len'] = val['en_idized'].apply(lambda x: len(x))\n",
    "val['vi_len'] = val['vi_idized'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    133317.000000\n",
       "mean         21.299399\n",
       "std          15.035268\n",
       "min           1.000000\n",
       "25%          11.000000\n",
       "50%          17.000000\n",
       "75%          27.000000\n",
       "max         629.000000\n",
       "Name: en_len, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['en_len'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[np.logical_and(train['en_len']>=2,train['vi_len']>=2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train['en_len']<=30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = val[np.logical_and(val['en_len']>=2,val['vi_len']>=2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = val[val['en_len']<=30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 31\n",
    "def vocab_collate_func(batch):\n",
    "    en_data = []\n",
    "    vi_data = []\n",
    "    en_len = []\n",
    "    vi_len = []\n",
    "\n",
    "    for datum in batch:\n",
    "        en_len.append(datum[2])\n",
    "        vi_len.append(datum[3])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        if datum[2]>MAX_LEN:\n",
    "            padded_vec_s1 = np.array(datum[0])[:MAX_LEN]\n",
    "        else:\n",
    "            padded_vec_s1 = np.pad(np.array(datum[0]),\n",
    "                                pad_width=((0,MAX_LEN - datum[2])),\n",
    "                                mode=\"constant\", constant_values=PAD_IDX)\n",
    "        if datum[3]>MAX_LEN:\n",
    "            padded_vec_s2 = np.array(datum[1])[:MAX_LEN]\n",
    "        else:\n",
    "            padded_vec_s2 = np.pad(np.array(datum[1]),\n",
    "                                pad_width=((0,MAX_LEN - datum[3])),\n",
    "                                mode=\"constant\", constant_values=PAD_IDX)\n",
    "        en_data.append(padded_vec_s1)\n",
    "        vi_data.append(padded_vec_s2)\n",
    "        \n",
    "    return [torch.from_numpy(np.array(en_data)), torch.from_numpy(np.array(vi_data)),\n",
    "            torch.from_numpy(np.array(en_len)), torch.from_numpy(np.array(vi_len))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 10\n",
    "transformed_dataset = {'train': Vietnamese(train),\n",
    "                       'validate': Vietnamese(val)\n",
    "                                               }\n",
    "\n",
    "dataloader = {x: DataLoader(transformed_dataset[x], batch_size=bs, collate_fn=vocab_collate_func,\n",
    "                        shuffle=True, num_workers=0) for x in ['train', 'validate']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(dataloader['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 31])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size,batch_first=True)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)\n",
    "#         bss = input.size(0)\n",
    "        output = embedded\n",
    "#         print(\"emb size\",output.size())\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "#         print(\"rnn out\",output.size())\n",
    "#         print(\"rnn hid\",hidden.size())\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self,bs):\n",
    "        return torch.zeros(1, bs, self.hidden_size).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size,batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "#         bss = input.size(0)\n",
    "#         print(input.size(),bss)\n",
    "#         print(bss)\n",
    "#         print(\"in\",input.size())\n",
    "#         print(\"hid\",hidden.size())\n",
    "        \n",
    "        output = self.embedding(input)\n",
    "#         .view(bss,-1,self.hidden_size)\n",
    "#         print(\"ou\",output.size())\n",
    "        output = F.relu(output)\n",
    "#         print(\"ou rel\",output.size())\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "#         print(\"out\",output.size(),\"hid\",hidden.size())\n",
    "        output = self.softmax(self.out(output))\n",
    "#         print(\"sm\",output.size())\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, bs, self.hidden_size).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def encode_decode(encoder,decoder,data_en,data_de):\n",
    "    use_teacher_forcing = True if random.random() < 0.5 else False\n",
    "#     use_teacher_forcing = True\n",
    "    bss = data_en.size(0)\n",
    "#     print(\"data de\",data_de.size())\n",
    "    en_h = encoder.initHidden(bss)\n",
    "    en_out,en_hid = encoder(data_en,en_h)\n",
    "    \n",
    "    decoder_hidden = en_hid\n",
    "    decoder_input = torch.tensor([[SOS_token]]*bss).cuda()\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        d_out = []\n",
    "        for i in range(31):\n",
    "            decoder_output,decoder_hidden = decoder(decoder_input,decoder_hidden)\n",
    "            d_out.append(decoder_output)\n",
    "            decoder_input = data_de[:,i].view(-1,1)\n",
    "#             print(decoder_input.size())\n",
    "        d_hid = decoder_hidden\n",
    "        d_out = torch.cat(d_out,dim=1)\n",
    "#         d_out,d_hid = decoder(decoder_input,decoder_hidden)  \n",
    "#         d_out,d_hid = decoder(data_de,d_hid)\n",
    "    else:\n",
    "        d_out = []\n",
    "        for i in range(31):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input,decoder_hidden)\n",
    "            d_out.append(decoder_output)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach().view(-1,1)\n",
    "        d_hid = decoder_hidden\n",
    "        d_out = torch.cat(d_out,dim=1)\n",
    "#         print(len(d_out),d_out[0].size(),torch.cat(d_out,dim=1).size())\n",
    "    return d_out, d_hid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(encoder_optimizer,decoder_optimizer, encoder, decoder, loss_fun, num_epochs=60):\n",
    "    best_score = 0\n",
    "    best_au = 0\n",
    "    loss_hist = {'train': [], 'validate': []}\n",
    "    acc_hist = {'train': [], 'validate': []}\n",
    "    for epoch in range(num_epochs):\n",
    "        for ex, phase in enumerate(['train', 'validate']):\n",
    "            start = time.time()\n",
    "            total = 0\n",
    "            top1_correct = 0\n",
    "            running_loss = 0\n",
    "            running_total = 0\n",
    "            if phase == 'train':\n",
    "                encoder.train(True)\n",
    "                decoder.train(True)\n",
    "            else:\n",
    "                encoder.train(False)\n",
    "                decoder.train(False)\n",
    "            for data in dataloader[phase]:\n",
    "                encoder_optimizer.zero_grad()\n",
    "                decoder_optimizer.zero_grad()\n",
    "\n",
    "                encoder_i = data[0].cuda()\n",
    "                decoder_i = data[1].cuda()\n",
    "                                \n",
    "                out, hidden = encode_decode(encoder,decoder,encoder_i,decoder_i)\n",
    "#                 print(out.size())\n",
    "#                 print(decoder_i.size())\n",
    "#                 _, top1_predicted = torch.max(out, dim=2)\n",
    "#                 print(out.float().view(-1,out.size(-1)).size(),decoder_i.long().view(-1).size())\n",
    "#                 print(top1_predicted.float().view(-1).size())\n",
    "                loss = loss_fun(out.float().view(-1,out.size(-1)), decoder_i.long().view(-1))\n",
    "                N = decoder_i.size(0)\n",
    "                running_loss += loss.item() * N\n",
    "                \n",
    "                total += N\n",
    "                \n",
    "#                 _, top1_predicted = torch.max(y_out, dim=1)\n",
    "#                 top1_correct += int((top1_predicted == y).sum())\n",
    "\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    encoder_optimizer.step()\n",
    "                    decoder_optimizer.step()\n",
    "\n",
    "#                 running_total += N\n",
    "            epoch_loss = running_loss / total\n",
    "#             epoch_acc = top1_correct / total\n",
    "            epoch_acc = 0\n",
    "            loss_hist[phase].append(epoch_loss)\n",
    "            acc_hist[phase].append(epoch_acc)\n",
    "            print(\"epoch {} {} loss = {}, accurancy = {} time = {}\".format(epoch, phase, epoch_loss, epoch_acc,\n",
    "                                                                           time.time() - start))\n",
    "        if phase == 'validate' and epoch_acc > best_score:\n",
    "            best_score = epoch_acc\n",
    "#             torch.save(model, save_dir+save_name+str(n_channel)+str(n_top)+str(vocab_size))\n",
    "    print(\"Training completed. Best accuracy is {}\".format(best_score))\n",
    "    return encoder,decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='gpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "encoder = EncoderRNN(en_lang.n_words,100).cuda()\n",
    "decoder = DecoderRNN(100,vi_lang.n_words).cuda()\n",
    "encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 train loss = 7.464464996602623, accurancy = 0 time = 533.3160026073456\n",
      "epoch 0 validate loss = 6.116853407895194, accurancy = 0 time = 2.265810489654541\n",
      "epoch 1 train loss = 5.675949598277939, accurancy = 0 time = 532.4015116691589\n",
      "epoch 1 validate loss = 5.9483616741872956, accurancy = 0 time = 2.2276804447174072\n",
      "epoch 2 train loss = 5.539535002398212, accurancy = 0 time = 532.3798794746399\n",
      "epoch 2 validate loss = 5.784652586088496, accurancy = 0 time = 2.242927074432373\n",
      "epoch 3 train loss = 5.3973482541677, accurancy = 0 time = 532.509863615036\n",
      "epoch 3 validate loss = 5.641707108994066, accurancy = 0 time = 2.228219747543335\n",
      "epoch 4 train loss = 5.250510054005705, accurancy = 0 time = 532.7513363361359\n",
      "epoch 4 validate loss = 5.49692843885814, accurancy = 0 time = 2.2058563232421875\n",
      "epoch 5 train loss = 5.130511691050047, accurancy = 0 time = 532.4258580207825\n",
      "epoch 5 validate loss = 5.405985553381795, accurancy = 0 time = 2.2491140365600586\n",
      "epoch 6 train loss = 5.032417398512437, accurancy = 0 time = 532.6690664291382\n",
      "epoch 6 validate loss = 5.33031641253258, accurancy = 0 time = 2.2617316246032715\n",
      "epoch 7 train loss = 4.957161149741041, accurancy = 0 time = 532.6797957420349\n",
      "epoch 7 validate loss = 5.2339864221953585, accurancy = 0 time = 2.236976385116577\n",
      "epoch 8 train loss = 4.883402105539187, accurancy = 0 time = 532.3393733501434\n",
      "epoch 8 validate loss = 5.168979610339808, accurancy = 0 time = 2.2486519813537598\n",
      "epoch 9 train loss = 4.8193493503468785, accurancy = 0 time = 532.6540114879608\n",
      "epoch 9 validate loss = 5.099697491350241, accurancy = 0 time = 2.2686662673950195\n",
      "epoch 10 train loss = 4.755318108129271, accurancy = 0 time = 532.9386587142944\n",
      "epoch 10 validate loss = 5.024743584237821, accurancy = 0 time = 2.2340402603149414\n",
      "epoch 11 train loss = 4.692021454041154, accurancy = 0 time = 532.6146318912506\n",
      "epoch 11 validate loss = 4.959401861475846, accurancy = 0 time = 2.2038464546203613\n",
      "epoch 12 train loss = 4.635894739926163, accurancy = 0 time = 532.8576714992523\n",
      "epoch 12 validate loss = 4.886612186221445, accurancy = 0 time = 2.1981217861175537\n",
      "epoch 13 train loss = 4.576902440453219, accurancy = 0 time = 532.8792941570282\n",
      "epoch 13 validate loss = 4.845143059909404, accurancy = 0 time = 2.2246999740600586\n",
      "epoch 14 train loss = 4.518888235519766, accurancy = 0 time = 532.917014837265\n",
      "epoch 14 validate loss = 4.799510310143382, accurancy = 0 time = 2.2663514614105225\n",
      "epoch 15 train loss = 4.467993791613983, accurancy = 0 time = 532.8270025253296\n",
      "epoch 15 validate loss = 4.744670770352439, accurancy = 0 time = 2.2460901737213135\n",
      "epoch 16 train loss = 4.42469931461471, accurancy = 0 time = 532.6754565238953\n",
      "epoch 16 validate loss = 4.684635393837151, accurancy = 0 time = 2.2436459064483643\n",
      "epoch 17 train loss = 4.380255103311775, accurancy = 0 time = 532.6214878559113\n",
      "epoch 17 validate loss = 4.635551002582791, accurancy = 0 time = 2.238999128341675\n",
      "epoch 19 train loss = 4.3024741643144955, accurancy = 0 time = 532.849503993988\n",
      "epoch 19 validate loss = 4.550584751002886, accurancy = 0 time = 2.2063686847686768\n",
      "epoch 20 train loss = 4.2664440331582805, accurancy = 0 time = 533.0216763019562\n",
      "epoch 20 validate loss = 4.556540874205717, accurancy = 0 time = 2.2283453941345215\n",
      "epoch 21 train loss = 4.238538064223847, accurancy = 0 time = 534.1068961620331\n",
      "epoch 21 validate loss = 4.506140455678329, accurancy = 0 time = 2.252248764038086\n",
      "epoch 22 train loss = 4.215810510928787, accurancy = 0 time = 534.8434314727783\n",
      "epoch 22 validate loss = 4.473034015510123, accurancy = 0 time = 2.2637619972229004\n",
      "epoch 23 train loss = 4.187636097343504, accurancy = 0 time = 534.9065191745758\n",
      "epoch 23 validate loss = 4.449921270072042, accurancy = 0 time = 2.204235553741455\n",
      "epoch 24 train loss = 4.161083298554744, accurancy = 0 time = 530.2960832118988\n",
      "epoch 24 validate loss = 4.432089774515349, accurancy = 0 time = 2.2057065963745117\n",
      "epoch 25 train loss = 4.135270953649212, accurancy = 0 time = 530.2839090824127\n",
      "epoch 25 validate loss = 4.3688740127662955, accurancy = 0 time = 2.1963980197906494\n",
      "epoch 26 train loss = 4.111186281141958, accurancy = 0 time = 530.6111891269684\n",
      "epoch 26 validate loss = 4.350733822542304, accurancy = 0 time = 2.201092481613159\n",
      "epoch 27 train loss = 4.0839329247022205, accurancy = 0 time = 530.7574224472046\n",
      "epoch 27 validate loss = 4.3467713660199045, accurancy = 0 time = 2.251652479171753\n",
      "epoch 28 train loss = 4.047669297893705, accurancy = 0 time = 530.3684966564178\n",
      "epoch 28 validate loss = 4.278443788453832, accurancy = 0 time = 2.1914799213409424\n",
      "epoch 29 train loss = 4.014860828748937, accurancy = 0 time = 530.5781216621399\n",
      "epoch 29 validate loss = 4.234426965923941, accurancy = 0 time = 2.193516492843628\n",
      "epoch 30 train loss = 3.9779305816546384, accurancy = 0 time = 534.2683010101318\n",
      "epoch 30 validate loss = 4.2144651391441625, accurancy = 0 time = 2.275080442428589\n",
      "epoch 31 train loss = 3.945695777598787, accurancy = 0 time = 535.067622423172\n",
      "epoch 31 validate loss = 4.175851884553043, accurancy = 0 time = 2.2741684913635254\n",
      "epoch 32 train loss = 3.9249222359773075, accurancy = 0 time = 534.9716641902924\n",
      "epoch 32 validate loss = 4.1733738967145575, accurancy = 0 time = 2.2814009189605713\n",
      "epoch 33 train loss = 3.91132004487159, accurancy = 0 time = 535.0168142318726\n",
      "epoch 33 validate loss = 4.154400576081653, accurancy = 0 time = 2.2307581901550293\n",
      "epoch 34 train loss = 3.8953655725841005, accurancy = 0 time = 534.6915619373322\n",
      "epoch 34 validate loss = 4.1366094012437395, accurancy = 0 time = 2.267559766769409\n",
      "epoch 35 train loss = 3.8809159297575597, accurancy = 0 time = 535.0625522136688\n",
      "epoch 35 validate loss = 4.115925169517188, accurancy = 0 time = 2.2599689960479736\n",
      "epoch 36 train loss = 3.8672952492036297, accurancy = 0 time = 535.4029965400696\n",
      "epoch 36 validate loss = 4.112650395157106, accurancy = 0 time = 2.2773327827453613\n",
      "epoch 37 train loss = 3.8549595172745263, accurancy = 0 time = 535.7525997161865\n",
      "epoch 37 validate loss = 4.088171533739555, accurancy = 0 time = 2.258622646331787\n"
     ]
    }
   ],
   "source": [
    "enc, dec = train_model(encoder_optimizer,decoder_optimizer, encoder, decoder, criterion, num_epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(dataloader['validate']))\n",
    "out,hid = encode_decode(enc,dec,data[0][1].view(1,-1).cuda(),data[1][1].view(1,-1).cuda())\n",
    "_, top1_predicted = torch.max(out, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "given=[]\n",
    "for i in data[1][1]:\n",
    "    given.append(vi_lang.index2word[i.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "for i in top1_predicted[0]:\n",
    "#     print(i.item())\n",
    "    pred.append(vi_lang.index2word[i.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['đây',\n",
       " 'nên',\n",
       " 'đáng',\n",
       " 'là',\n",
       " 'nguyên_tắc',\n",
       " 'đầu_tiên',\n",
       " 'về',\n",
       " 'viện_trợ',\n",
       " '.',\n",
       " 'EOS',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD']"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
