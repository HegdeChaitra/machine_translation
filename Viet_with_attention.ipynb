{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pdb\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IDX = 2\n",
    "PAD_IDX = 3\n",
    "SOS_token = 0\n",
    "EOS_token = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(file):\n",
    "    f = open(file)\n",
    "    list_l = []\n",
    "    for line in f:\n",
    "        list_l.append(line.strip())\n",
    "    df = pd.DataFrame()\n",
    "    df['data'] = list_l\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_train = read_dataset(\"/scratch/ark576/machine_translation_data/iwslt-vi-en/train.tok.en\")\n",
    "en_val = read_dataset(\"/scratch/ark576/machine_translation_data/iwslt-vi-en/dev.tok.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_train = read_dataset(\"/scratch/ark576/machine_translation_data/iwslt-vi-en/train.tok.vi\")\n",
    "vi_val = read_dataset(\"/scratch/ark576/machine_translation_data/iwslt-vi-en/dev.tok.vi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(133317, 1) (133317, 1)\n",
      "(1268, 1) (1268, 1)\n"
     ]
    }
   ],
   "source": [
    "print(en_train.shape,vi_train.shape)\n",
    "print(en_val.shape,vi_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame()\n",
    "train['en_data'] = en_train['data']\n",
    "train['vi_data'] = vi_train['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ark576/.conda/envs/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "en_data                  \n",
       "vi_data    \" tán_thưởng \"\n",
       "Name: 38600, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.ix[38600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = pd.DataFrame()\n",
    "val['en_data'] = en_val['data']\n",
    "val['vi_data'] = vi_val['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(133317, 2) (1268, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape,val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2:\"UKN\",3:\"PAD\"}\n",
    "        self.n_words = 4  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word.lower())\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['en_data'] = train['en_data'].apply(lambda x: x.lower())\n",
    "# train['vi_data'] = train['vi_data'].apply(lambda x:x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_lang = Lang(\"en\")\n",
    "for ex in train['en_data']:\n",
    "#     ex = unicodeToAscii(ex)\n",
    "#     ex = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", ex)\n",
    "    en_lang.addSentence(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_lang = Lang(\"vi\")\n",
    "for ex in train['vi_data']:\n",
    "#     ex = unicodeToAscii(ex)\n",
    "#     ex = re.sub(r\"[^a-zA-Z.!?_]+\", r\" \", ex)\n",
    "    vi_lang.addSentence(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vi_lang.word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(df):\n",
    "    df['en_tokenized'] = df[\"en_data\"].apply(lambda x:x.lower().split( ))\n",
    "    df['vi_tokenized'] = df['vi_data'].apply(lambda x:x.lower().split( ))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = split(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = split(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en_data</th>\n",
       "      <th>vi_data</th>\n",
       "      <th>en_tokenized</th>\n",
       "      <th>vi_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rachel Pike : The science behind a climate hea...</td>\n",
       "      <td>Khoa_học đằng_sau một tiêu_đề về khí_hậu</td>\n",
       "      <td>[rachel, pike, :, the, science, behind, a, cli...</td>\n",
       "      <td>[khoa_học, đằng_sau, một, tiêu_đề, về, khí_hậu]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In 4 minutes , atmospheric chemist Rachel Pike...</td>\n",
       "      <td>Trong 4 phút , chuyên_gia hoá_học khí_quyển Ra...</td>\n",
       "      <td>[in, 4, minutes, ,, atmospheric, chemist, rach...</td>\n",
       "      <td>[trong, 4, phút, ,, chuyên_gia, hoá_học, khí_q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I &amp;apos;d like to talk to you today about the ...</td>\n",
       "      <td>Tôi muốn cho các bạn biết về sự to_lớn của nhữ...</td>\n",
       "      <td>[i, &amp;apos;d, like, to, talk, to, you, today, a...</td>\n",
       "      <td>[tôi, muốn, cho, các, bạn, biết, về, sự, to_lớ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Headlines that look like this when they have t...</td>\n",
       "      <td>Có những dòng trông như thế_này khi bàn về biế...</td>\n",
       "      <td>[headlines, that, look, like, this, when, they...</td>\n",
       "      <td>[có, những, dòng, trông, như, thế_này, khi, bà...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>They are both two branches of the same field o...</td>\n",
       "      <td>Cả hai đều là một nhánh của cùng một lĩnh_vực ...</td>\n",
       "      <td>[they, are, both, two, branches, of, the, same...</td>\n",
       "      <td>[cả, hai, đều, là, một, nhánh, của, cùng, một,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             en_data  \\\n",
       "0  Rachel Pike : The science behind a climate hea...   \n",
       "1  In 4 minutes , atmospheric chemist Rachel Pike...   \n",
       "2  I &apos;d like to talk to you today about the ...   \n",
       "3  Headlines that look like this when they have t...   \n",
       "4  They are both two branches of the same field o...   \n",
       "\n",
       "                                             vi_data  \\\n",
       "0           Khoa_học đằng_sau một tiêu_đề về khí_hậu   \n",
       "1  Trong 4 phút , chuyên_gia hoá_học khí_quyển Ra...   \n",
       "2  Tôi muốn cho các bạn biết về sự to_lớn của nhữ...   \n",
       "3  Có những dòng trông như thế_này khi bàn về biế...   \n",
       "4  Cả hai đều là một nhánh của cùng một lĩnh_vực ...   \n",
       "\n",
       "                                        en_tokenized  \\\n",
       "0  [rachel, pike, :, the, science, behind, a, cli...   \n",
       "1  [in, 4, minutes, ,, atmospheric, chemist, rach...   \n",
       "2  [i, &apos;d, like, to, talk, to, you, today, a...   \n",
       "3  [headlines, that, look, like, this, when, they...   \n",
       "4  [they, are, both, two, branches, of, the, same...   \n",
       "\n",
       "                                        vi_tokenized  \n",
       "0    [khoa_học, đằng_sau, một, tiêu_đề, về, khí_hậu]  \n",
       "1  [trong, 4, phút, ,, chuyên_gia, hoá_học, khí_q...  \n",
       "2  [tôi, muốn, cho, các, bạn, biết, về, sự, to_lớ...  \n",
       "3  [có, những, dòng, trông, như, thế_này, khi, bà...  \n",
       "4  [cả, hai, đều, là, một, nhánh, của, cùng, một,...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train[train['vi_len']==713]['en_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vi_lang.word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token2index_dataset(df):\n",
    "    for lan in ['en','vi']:\n",
    "        indices_data = []\n",
    "        if lan=='en':\n",
    "            lang_obj = en_lang\n",
    "        else:\n",
    "            lang_obj = vi_lang\n",
    "        for tokens in df[lan+'_tokenized']:\n",
    "            index_list = [lang_obj.word2index[token] if token in lang_obj.word2index else UNK_IDX for token in tokens]\n",
    "            index_list.append(EOS_token)\n",
    "#             index_list.insert(0,SOS_token)\n",
    "            indices_data.append(index_list)\n",
    "        df[lan+'_idized'] = indices_data\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = token2index_dataset(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = token2index_dataset(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vietnamese(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        english = self.df.iloc[idx]['en_idized']\n",
    "        viet = self.df.iloc[idx]['vi_idized']\n",
    "        en_len = self.df.iloc[idx]['en_len']\n",
    "        vi_len = self.df.iloc[idx]['vi_len']\n",
    "        return [english,viet,en_len,vi_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en_data</th>\n",
       "      <th>vi_data</th>\n",
       "      <th>en_tokenized</th>\n",
       "      <th>vi_tokenized</th>\n",
       "      <th>en_idized</th>\n",
       "      <th>vi_idized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rachel Pike : The science behind a climate hea...</td>\n",
       "      <td>Khoa_học đằng_sau một tiêu_đề về khí_hậu</td>\n",
       "      <td>[rachel, pike, :, the, science, behind, a, cli...</td>\n",
       "      <td>[khoa_học, đằng_sau, một, tiêu_đề, về, khí_hậu]</td>\n",
       "      <td>[4, 5, 6, 7, 8, 9, 10, 11, 12, 1]</td>\n",
       "      <td>[4, 5, 6, 7, 8, 9, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In 4 minutes , atmospheric chemist Rachel Pike...</td>\n",
       "      <td>Trong 4 phút , chuyên_gia hoá_học khí_quyển Ra...</td>\n",
       "      <td>[in, 4, minutes, ,, atmospheric, chemist, rach...</td>\n",
       "      <td>[trong, 4, phút, ,, chuyên_gia, hoá_học, khí_q...</td>\n",
       "      <td>[13, 14, 15, 16, 17, 18, 4, 5, 19, 10, 20, 21,...</td>\n",
       "      <td>[10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I &amp;apos;d like to talk to you today about the ...</td>\n",
       "      <td>Tôi muốn cho các bạn biết về sự to_lớn của nhữ...</td>\n",
       "      <td>[i, &amp;apos;d, like, to, talk, to, you, today, a...</td>\n",
       "      <td>[tôi, muốn, cho, các, bạn, biết, về, sự, to_lớ...</td>\n",
       "      <td>[47, 48, 49, 50, 51, 50, 52, 53, 54, 7, 55, 21...</td>\n",
       "      <td>[52, 53, 38, 54, 55, 56, 8, 57, 58, 30, 21, 22...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Headlines that look like this when they have t...</td>\n",
       "      <td>Có những dòng trông như thế_này khi bàn về biế...</td>\n",
       "      <td>[headlines, that, look, like, this, when, they...</td>\n",
       "      <td>[có, những, dòng, trông, như, thế_này, khi, bà...</td>\n",
       "      <td>[26, 56, 62, 49, 63, 64, 65, 66, 50, 67, 29, 1...</td>\n",
       "      <td>[67, 21, 61, 68, 69, 70, 71, 72, 8, 25, 9, 13,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>They are both two branches of the same field o...</td>\n",
       "      <td>Cả hai đều là một nhánh của cùng một lĩnh_vực ...</td>\n",
       "      <td>[they, are, both, two, branches, of, the, same...</td>\n",
       "      <td>[cả, hai, đều, là, một, nhánh, của, cùng, một,...</td>\n",
       "      <td>[65, 73, 74, 75, 76, 21, 7, 77, 78, 21, 17, 8,...</td>\n",
       "      <td>[80, 81, 82, 83, 6, 84, 30, 26, 6, 85, 10, 86,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             en_data  \\\n",
       "0  Rachel Pike : The science behind a climate hea...   \n",
       "1  In 4 minutes , atmospheric chemist Rachel Pike...   \n",
       "2  I &apos;d like to talk to you today about the ...   \n",
       "3  Headlines that look like this when they have t...   \n",
       "4  They are both two branches of the same field o...   \n",
       "\n",
       "                                             vi_data  \\\n",
       "0           Khoa_học đằng_sau một tiêu_đề về khí_hậu   \n",
       "1  Trong 4 phút , chuyên_gia hoá_học khí_quyển Ra...   \n",
       "2  Tôi muốn cho các bạn biết về sự to_lớn của nhữ...   \n",
       "3  Có những dòng trông như thế_này khi bàn về biế...   \n",
       "4  Cả hai đều là một nhánh của cùng một lĩnh_vực ...   \n",
       "\n",
       "                                        en_tokenized  \\\n",
       "0  [rachel, pike, :, the, science, behind, a, cli...   \n",
       "1  [in, 4, minutes, ,, atmospheric, chemist, rach...   \n",
       "2  [i, &apos;d, like, to, talk, to, you, today, a...   \n",
       "3  [headlines, that, look, like, this, when, they...   \n",
       "4  [they, are, both, two, branches, of, the, same...   \n",
       "\n",
       "                                        vi_tokenized  \\\n",
       "0    [khoa_học, đằng_sau, một, tiêu_đề, về, khí_hậu]   \n",
       "1  [trong, 4, phút, ,, chuyên_gia, hoá_học, khí_q...   \n",
       "2  [tôi, muốn, cho, các, bạn, biết, về, sự, to_lớ...   \n",
       "3  [có, những, dòng, trông, như, thế_này, khi, bà...   \n",
       "4  [cả, hai, đều, là, một, nhánh, của, cùng, một,...   \n",
       "\n",
       "                                           en_idized  \\\n",
       "0                  [4, 5, 6, 7, 8, 9, 10, 11, 12, 1]   \n",
       "1  [13, 14, 15, 16, 17, 18, 4, 5, 19, 10, 20, 21,...   \n",
       "2  [47, 48, 49, 50, 51, 50, 52, 53, 54, 7, 55, 21...   \n",
       "3  [26, 56, 62, 49, 63, 64, 65, 66, 50, 67, 29, 1...   \n",
       "4  [65, 73, 74, 75, 76, 21, 7, 77, 78, 21, 17, 8,...   \n",
       "\n",
       "                                           vi_idized  \n",
       "0                              [4, 5, 6, 7, 8, 9, 1]  \n",
       "1  [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 8...  \n",
       "2  [52, 53, 38, 54, 55, 56, 8, 57, 58, 30, 21, 22...  \n",
       "3  [67, 21, 61, 68, 69, 70, 71, 72, 8, 25, 9, 13,...  \n",
       "4  [80, 81, 82, 83, 6, 84, 30, 26, 6, 85, 10, 86,...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['en_len'] = train['en_idized'].apply(lambda x: len(x))\n",
    "train['vi_len'] = train['vi_idized'].apply(lambda x:len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "val['en_len'] = val['en_idized'].apply(lambda x: len(x))\n",
    "val['vi_len'] = val['vi_idized'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    133317.000000\n",
       "mean         21.299399\n",
       "std          15.035268\n",
       "min           1.000000\n",
       "25%          11.000000\n",
       "50%          17.000000\n",
       "75%          27.000000\n",
       "max         629.000000\n",
       "Name: en_len, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['en_len'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    133317.000000\n",
       "mean         21.120157\n",
       "std          15.115259\n",
       "min           1.000000\n",
       "25%          11.000000\n",
       "50%          17.000000\n",
       "75%          27.000000\n",
       "max         712.000000\n",
       "Name: vi_len, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['vi_len'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[np.logical_and(train['en_len']>=2,train['vi_len']>=2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train['vi_len']<=30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = val[np.logical_and(val['en_len']>=2,val['vi_len']>=2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = val[val['vi_len']<=30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 30\n",
    "def vocab_collate_func(batch):\n",
    "    en_data = []\n",
    "    vi_data = []\n",
    "    en_len = []\n",
    "    vi_len = []\n",
    "\n",
    "    for datum in batch:\n",
    "        en_len.append(datum[2])\n",
    "        vi_len.append(datum[3])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        if datum[2]>MAX_LEN:\n",
    "            padded_vec_s1 = np.array(datum[0])[:MAX_LEN]\n",
    "        else:\n",
    "            padded_vec_s1 = np.pad(np.array(datum[0]),\n",
    "                                pad_width=((0,MAX_LEN - datum[2])),\n",
    "                                mode=\"constant\", constant_values=PAD_IDX)\n",
    "        if datum[3]>MAX_LEN:\n",
    "            padded_vec_s2 = np.array(datum[1])[:MAX_LEN]\n",
    "        else:\n",
    "            padded_vec_s2 = np.pad(np.array(datum[1]),\n",
    "                                pad_width=((0,MAX_LEN - datum[3])),\n",
    "                                mode=\"constant\", constant_values=PAD_IDX)\n",
    "        en_data.append(padded_vec_s1)\n",
    "        vi_data.append(padded_vec_s2)\n",
    "        \n",
    "    return [torch.from_numpy(np.array(vi_data)), torch.from_numpy(np.array(en_data)),\n",
    "            torch.from_numpy(np.array(vi_len)), torch.from_numpy(np.array(en_len))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 10\n",
    "transformed_dataset = {'train': Vietnamese(train[:20]),\n",
    "                       'validate': Vietnamese(val[:3])\n",
    "                                               }\n",
    "\n",
    "dataloader = {x: DataLoader(transformed_dataset[x], batch_size=bs, collate_fn=vocab_collate_func,\n",
    "                        shuffle=True, num_workers=0) for x in ['train', 'validate']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(dataloader['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([79,  7, 26, 80, 49, 63, 64,  7, 81, 82, 27, 11, 28, 16, 71, 83, 16, 84,\n",
      "        85, 86, 87, 27,  7, 88, 21, 89, 21,  7, 17, 90])\n",
      "tensor([47, 48, 49, 50, 51, 50, 52, 53, 54,  7, 55, 21,  7, 23, 24, 56, 57, 58,\n",
      "        59,  7, 26, 52, 60, 13,  7, 61, 46,  1,  3,  3])\n",
      "tensor([ 68, 104,  21, 105, 102, 106, 107,  93, 108, 109,  95,  68, 110,  16,\n",
      "         96, 111,  98,  46,   1,   3,   3,   3,   3,   3,   3,   3,   3,   3,\n",
      "          3,   3])\n",
      "tensor([ 68, 129,  33,  21, 105,  95, 121,  13,  10, 132, 133,  16,  68, 129,\n",
      "        132, 133, 134,  10, 135, 136,  21, 137,  46,   1,   3,   3,   3,   3,\n",
      "          3,   3])\n",
      "tensor([112, 113,  10, 114, 115,  46, 112, 113, 116,  10, 114, 115,  16,  13,\n",
      "        117,  16,  56, 118, 119, 120, 121,   7, 122,  23, 123,  13,   7, 124,\n",
      "         46,   1])\n",
      "tensor([ 17, 217,  16,  71, 218, 219,  16, 220,  54, 125, 221, 222, 223, 155,\n",
      "        224,  13, 225, 226,  46,   1,   3,   3,   3,   3,   3,   3,   3,   3,\n",
      "          3,   3])\n",
      "tensor([65, 73, 74, 75, 76, 21,  7, 77, 78, 21, 17,  8, 46,  1,  3,  3,  3,  3,\n",
      "         3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3])\n",
      "tensor([112, 113,  10, 200, 201,  21, 202,  46, 112, 113, 186,  50,   7, 183,\n",
      "         21, 203,  46,   1,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,\n",
      "          3,   3])\n",
      "tensor([ 4,  5,  6,  7,  8,  9, 10, 11, 12,  1,  3,  3,  3,  3,  3,  3,  3,  3,\n",
      "         3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3])\n",
      "tensor([ 65,  99, 100,  10, 101, 102,  27,   7, 103,  46,   1,   3,   3,   3,\n",
      "          3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,\n",
      "          3,   3])\n"
     ]
    }
   ],
   "source": [
    "for d in data[1]:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([54,  7, 87, 68, 69, 70, 71, 88, 89, 25,  9, 90, 13, 91, 92, 83, 93, 94,\n",
       "        95, 29, 30, 96,  8, 97, 16, 51,  1,  3,  3,  3])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_idx_2_sent(tensor, lang_obj):\n",
    "    word_list = []\n",
    "    for i in tensor:\n",
    "        if i.item() not in set([PAD_IDX,EOS_token,SOS_token]):\n",
    "            word_list.append(lang_obj.index2word[i.item()])\n",
    "    return (' ').join(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bleu_score import BLEU_SCORE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl = BLEU_SCORE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(encoder, decoder, dataloader, loss_fun, lang_en):\n",
    "    encoder.train(False)\n",
    "    decoder.train(False)\n",
    "    pred_corpus = []\n",
    "    true_corpus = []\n",
    "    running_loss = 0\n",
    "    running_total = 0\n",
    "    bl = BLEU_SCORE()\n",
    "    for data in dataloader:\n",
    "        encoder_i = data[0].cuda()\n",
    "        decoder_i = data[1].cuda()\n",
    "        bs,sl = encoder_i.size()[:2]\n",
    "        out, hidden = encode_decode(encoder,decoder,encoder_i,decoder_i)\n",
    "        loss = loss_fun(out.float(), decoder_i.long())\n",
    "        running_loss += loss.item()*bs\n",
    "        running_total += bs\n",
    "        pred = torch.max(out,dim = 1)[1]\n",
    "        for t,p in zip(data[1],pred):\n",
    "            t,p = convert_idx_2_sent(t,lang_en), convert_idx_2_sent(p,lang_en)\n",
    "            true_corpus.append(t)\n",
    "            pred_corpus.append(p)\n",
    "    score = bl.corpus_bleu(pred_corpus,[true_corpus],lowercase=True)[0]\n",
    "    return running_loss/running_total, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 30])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'and every one of those scientists is in a research group , and every research group studies a wide variety of topics .'"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_idx_2_sent(data[1][3],en_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 30])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size,bi):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.bi=bi\n",
    "        if self.bi:\n",
    "            self.mul=2\n",
    "        else:\n",
    "            self.mul=1\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size,batch_first=True,bidirectional=self.bi)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)\n",
    "#         bss = input.size(0)\n",
    "        output = embedded\n",
    "#         print(\"emb size\",output.size(),hidden.size())\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "#         print(\"rnn out\",output.size())\n",
    "#         print(\"rnn hid\",hidden.size())\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self,bs):\n",
    "        return torch.zeros(self.mul, bs, self.hidden_size).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size,bi):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.bi = bi\n",
    "        if self.bi:\n",
    "            self.mul=2\n",
    "        else:\n",
    "            self.mul=1\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size,batch_first=True,bidirectional=self.bi)\n",
    "        \n",
    "        self.attn = nn.Linear(self.hidden_size * self.mul, MAX_LEN)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * self.mul+self.hidden_size, self.hidden_size)\n",
    "        \n",
    "        self.out = nn.Linear(self.mul*hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden,encoder_outputs):\n",
    "        bss = input.size(0)\n",
    "#         print(input.size(),bss)\n",
    "#         print(bss)\n",
    "#         print(\"in\",input.size())\n",
    "#         print(\"hid\",hidden.size())\n",
    "        \n",
    "        output = self.embedding(input)\n",
    "        output = self.dropout(output)\n",
    "#         print(output.size(),hidden.size())\n",
    "#         print(output.size(),hidden[0].unsqueeze(1).size())\n",
    "#         print(torch.cat((output, hidden[0].unsqueeze(1)), 2).size())\n",
    "        cat = torch.cat((output, hidden[0].unsqueeze(1)), 2)\n",
    "#         print(cat.size())\n",
    "        att_out = F.softmax(self.attn(cat),dim=1)\n",
    "#         print(\"attn out\",att_out.size(),\"en_out\",encoder_outputs.size())\n",
    "        attn_applied = torch.bmm(att_out,encoder_outputs)\n",
    "#         print(\"attn applied\",attn_applied.size(),\"enb out\",output.size())\n",
    "        attn_cat = torch.cat((output, attn_applied), 2)\n",
    "#         print(\"attn cat\",attn_cat.size())\n",
    "        attn_comb = self.attn_combine(attn_cat)\n",
    "#         print(attn_comb.size())\n",
    "        \n",
    "        \n",
    "#         .view(bss,-1,self.hidden_size)\n",
    "#         print(\"ou\",output.size())\n",
    "        output = F.relu(attn_comb)\n",
    "#         print(\"ou rel\",output.size())\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "#         print(\"out\",output.size(),\"hid\",hidden.size())\n",
    "        output = self.out(output.squeeze(dim=1))\n",
    "#         print(output.size())\n",
    "        output = self.softmax(output)\n",
    "#         print(\"sm\",output.size())\n",
    "#         print(output.size(),hidden.size())\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.mul, bs, self.hidden_size).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def encode_decode(encoder,decoder,data_en,data_de):\n",
    "    use_teacher_forcing = True if random.random() < 0.5 else False\n",
    "#     use_teacher_forcing = True\n",
    "    bss = data_en.size(0)\n",
    "#     print(\"data de\",data_de.size())\n",
    "    en_h = encoder.initHidden(bss)\n",
    "    en_out,en_hid = encoder(data_en,en_h)\n",
    "    \n",
    "    decoder_hidden = en_hid\n",
    "    decoder_input = torch.tensor([[SOS_token]]*bss).cuda()\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        d_out = []\n",
    "        for i in range(MAX_LEN):\n",
    "            decoder_output,decoder_hidden = decoder(decoder_input,decoder_hidden,en_out)\n",
    "            d_out.append(decoder_output.unsqueeze(-1))\n",
    "            decoder_input = data_de[:,i].view(-1,1)\n",
    "#             print(decoder_input.size())\n",
    "        d_hid = decoder_hidden\n",
    "        d_out = torch.cat(d_out,dim=-1)\n",
    "#         d_out,d_hid = decoder(decoder_input,decoder_hidden)  \n",
    "#         d_out,d_hid = decoder(data_de,d_hid)\n",
    "    else:\n",
    "        d_out = []\n",
    "        for i in range(MAX_LEN):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input,decoder_hidden,en_out)\n",
    "            d_out.append(decoder_output.unsqueeze(-1))\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach().view(-1,1)\n",
    "        d_hid = decoder_hidden\n",
    "        d_out = torch.cat(d_out,dim=-1)\n",
    "#         print(len(d_out),d_out[0].size(),torch.cat(d_out,dim=1).size())\n",
    "    return d_out, d_hid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(encoder_optimizer,decoder_optimizer, encoder, decoder, loss_fun, num_epochs=60):\n",
    "    best_score = 0\n",
    "    best_au = 0\n",
    "    loss_hist = {'train': [], 'validate': []}\n",
    "    acc_hist = {'train': [], 'validate': []}\n",
    "    for epoch in range(num_epochs):\n",
    "        for ex, phase in enumerate(['train']):\n",
    "            start = time.time()\n",
    "            total = 0\n",
    "            top1_correct = 0\n",
    "            running_loss = 0\n",
    "            running_total = 0\n",
    "            if phase == 'train':\n",
    "                encoder.train(True)\n",
    "                decoder.train(True)\n",
    "            else:\n",
    "                encoder.train(False)\n",
    "                decoder.train(False)\n",
    "            for data in dataloader[phase]:\n",
    "                encoder_optimizer.zero_grad()\n",
    "                decoder_optimizer.zero_grad()\n",
    "\n",
    "                encoder_i = data[0].cuda()\n",
    "                decoder_i = data[1].cuda()\n",
    "                                \n",
    "                out, hidden = encode_decode(encoder,decoder,encoder_i,decoder_i)\n",
    "#                 print(out.size(),hidden.size())\n",
    "#                 print(out.float().size(),decoder_i.long().view(-1).size())\n",
    "#                 print(out.size())\n",
    "#                 print(decoder_i.size())\n",
    "#                 _, top1_predicted = torch.max(out, dim=2)\n",
    "#                 print(out.float().view(-1,out.size(-1)).size(),decoder_i.long().view(-1).size())\n",
    "#                 print(top1_predicted.float().view(-1).size())\n",
    "#                 loss = loss_fun(out.float().view(-1,out.size(-1)), decoder_i.long().view(-1))\n",
    "                loss = loss_fun(out.float(), decoder_i.long())\n",
    "                N = decoder_i.size(0)\n",
    "#                 print(N)\n",
    "                running_loss += loss.item() * N\n",
    "                \n",
    "                total += N\n",
    "                \n",
    "#                 _, top1_predicted = torch.max(y_out, dim=1)\n",
    "#                 top1_correct += int((top1_predicted == y).sum())\n",
    "\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    encoder_optimizer.step()\n",
    "                    decoder_optimizer.step()\n",
    "                    \n",
    "            loss, score = validation(encoder,decoder, dataloader['validate'],loss_fun, en_lang)\n",
    "            print('Validation Loss = ', loss)\n",
    "            print('Validation BLEU = ', score)\n",
    "            loss, score = validation(encoder,decoder, dataloader['train'],loss_fun, en_lang)\n",
    "            print('Training Loss = ', loss)\n",
    "            print('Traning BLEU = ', score)\n",
    "#                 running_total += N\n",
    "            epoch_loss = running_loss / total\n",
    "#             epoch_acc = top1_correct / total\n",
    "            epoch_acc = 0\n",
    "            loss_hist[phase].append(epoch_loss)\n",
    "            acc_hist[phase].append(epoch_acc)\n",
    "            print(\"epoch {} {} loss = {}, accurancy = {} time = {}\".format(epoch, phase, epoch_loss, epoch_acc,\n",
    "                                                                           time.time() - start))\n",
    "        if phase == 'validate' and epoch_acc > best_score:\n",
    "            best_score = epoch_acc\n",
    "#             torch.save(model, save_dir+save_name+str(n_channel)+str(n_top)+str(vocab_size))\n",
    "    print(\"Training completed. Best accuracy is {}\".format(best_score))\n",
    "    return encoder,decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='gpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47862"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_lang.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "bi=True\n",
    "# bi=False\n",
    "encoder = EncoderRNN(vi_lang.n_words,300,bi).cuda()\n",
    "decoder = DecoderRNN(300,en_lang.n_words,bi).cuda()\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss =  5.68177604675293\n",
      "Validation BLEU =  1.024008106506266\n",
      "Training Loss =  0.8640546202659607\n",
      "Traning BLEU =  78.67978620725064\n",
      "epoch 0 train loss = 1.0507247745990753, accurancy = 0 time = 0.6301066875457764\n",
      "Validation Loss =  6.092445373535156\n",
      "Validation BLEU =  1.1087842929081968\n",
      "Training Loss =  0.7990827560424805\n",
      "Traning BLEU =  78.24893707684978\n",
      "epoch 1 train loss = 0.7349399924278259, accurancy = 0 time = 0.572711706161499\n",
      "Validation Loss =  6.070290565490723\n",
      "Validation BLEU =  1.1007005774095222\n",
      "Training Loss =  0.71932452917099\n",
      "Traning BLEU =  80.21174070133692\n",
      "epoch 2 train loss = 0.9777535796165466, accurancy = 0 time = 0.5797584056854248\n",
      "Validation Loss =  5.67225456237793\n",
      "Validation BLEU =  1.0239830116798412\n",
      "Training Loss =  0.848278820514679\n",
      "Traning BLEU =  77.3752002466833\n",
      "epoch 3 train loss = 1.0493963956832886, accurancy = 0 time = 0.5810813903808594\n",
      "Validation Loss =  5.695442199707031\n",
      "Validation BLEU =  1.0239830116798412\n",
      "Training Loss =  0.967315286397934\n",
      "Traning BLEU =  78.13886055039846\n",
      "epoch 4 train loss = 0.8437196910381317, accurancy = 0 time = 0.5798523426055908\n",
      "Validation Loss =  6.084453105926514\n",
      "Validation BLEU =  1.1155017400898686\n",
      "Training Loss =  0.8288826942443848\n",
      "Traning BLEU =  79.79520357338819\n",
      "epoch 5 train loss = 0.8996195793151855, accurancy = 0 time = 0.5849013328552246\n",
      "Validation Loss =  6.1077799797058105\n",
      "Validation BLEU =  1.2598221868372657\n",
      "Training Loss =  0.9251346290111542\n",
      "Traning BLEU =  79.67854435971967\n",
      "epoch 6 train loss = 0.7143734991550446, accurancy = 0 time = 0.5789899826049805\n",
      "Validation Loss =  6.156959533691406\n",
      "Validation BLEU =  1.250637312734658\n",
      "Training Loss =  0.937054842710495\n",
      "Traning BLEU =  77.24522312383444\n",
      "epoch 7 train loss = 0.8246752321720123, accurancy = 0 time = 0.584587574005127\n",
      "Validation Loss =  5.702129364013672\n",
      "Validation BLEU =  1.0239830116798412\n",
      "Training Loss =  0.7876676023006439\n",
      "Traning BLEU =  82.08557192827874\n",
      "epoch 8 train loss = 0.811004638671875, accurancy = 0 time = 0.5739126205444336\n",
      "Validation Loss =  6.115053176879883\n",
      "Validation BLEU =  1.1087842929081968\n",
      "Training Loss =  0.8866149485111237\n",
      "Traning BLEU =  80.1505458210841\n",
      "epoch 9 train loss = 0.9514788687229156, accurancy = 0 time = 0.5856738090515137\n",
      "Validation Loss =  6.103518486022949\n",
      "Validation BLEU =  1.1087842929081968\n",
      "Training Loss =  0.9066832363605499\n",
      "Traning BLEU =  82.31640006179859\n",
      "epoch 10 train loss = 0.9535482227802277, accurancy = 0 time = 0.5909106731414795\n",
      "Validation Loss =  5.714533805847168\n",
      "Validation BLEU =  1.0239830116798412\n",
      "Training Loss =  0.6769337356090546\n",
      "Traning BLEU =  82.75001067822552\n",
      "epoch 11 train loss = 0.7595413625240326, accurancy = 0 time = 0.5681211948394775\n",
      "Validation Loss =  5.723662853240967\n",
      "Validation BLEU =  1.0239830116798412\n",
      "Training Loss =  0.6726250201463699\n",
      "Traning BLEU =  84.184319649536\n",
      "epoch 12 train loss = 0.9096527993679047, accurancy = 0 time = 0.5748815536499023\n",
      "Validation Loss =  6.126044273376465\n",
      "Validation BLEU =  1.2598221868372657\n",
      "Training Loss =  0.7695966064929962\n",
      "Traning BLEU =  82.08944717980435\n",
      "epoch 13 train loss = 0.6800960302352905, accurancy = 0 time = 0.5732600688934326\n",
      "Validation Loss =  6.169698715209961\n",
      "Validation BLEU =  1.250637312734658\n",
      "Training Loss =  0.6798155903816223\n",
      "Traning BLEU =  84.97434894907923\n",
      "epoch 14 train loss = 0.8633255064487457, accurancy = 0 time = 0.5847809314727783\n",
      "Validation Loss =  5.709371566772461\n",
      "Validation BLEU =  1.0239830116798412\n",
      "Training Loss =  0.7901199162006378\n",
      "Traning BLEU =  81.72414799880343\n",
      "epoch 15 train loss = 0.6694919466972351, accurancy = 0 time = 0.568798303604126\n",
      "Validation Loss =  6.03251838684082\n",
      "Validation BLEU =  0.9102267982746022\n",
      "Training Loss =  0.8260704874992371\n",
      "Traning BLEU =  80.15892627881317\n",
      "epoch 16 train loss = 0.8797446489334106, accurancy = 0 time = 0.5901315212249756\n",
      "Validation Loss =  5.696450233459473\n",
      "Validation BLEU =  1.0239830116798412\n",
      "Training Loss =  0.6455813348293304\n",
      "Traning BLEU =  87.07850360880349\n",
      "epoch 17 train loss = 0.845188319683075, accurancy = 0 time = 0.5738744735717773\n",
      "Validation Loss =  5.704976558685303\n",
      "Validation BLEU =  1.0239830116798412\n",
      "Training Loss =  0.7676390111446381\n",
      "Traning BLEU =  83.98313580616761\n",
      "epoch 18 train loss = 0.75210240483284, accurancy = 0 time = 0.5740294456481934\n",
      "Validation Loss =  5.713956832885742\n",
      "Validation BLEU =  1.0239830116798412\n",
      "Training Loss =  0.6373477578163147\n",
      "Traning BLEU =  86.79328140579062\n",
      "epoch 19 train loss = 0.8387214839458466, accurancy = 0 time = 0.5695865154266357\n",
      "Validation Loss =  6.093371391296387\n",
      "Validation BLEU =  0.99538064335188\n",
      "Training Loss =  0.6301676630973816\n",
      "Traning BLEU =  87.54474010508275\n",
      "epoch 20 train loss = 0.6754291653633118, accurancy = 0 time = 0.570749044418335\n",
      "Validation Loss =  5.721146106719971\n",
      "Validation BLEU =  1.0239830116798412\n",
      "Training Loss =  0.700316995382309\n",
      "Traning BLEU =  86.69723111352927\n",
      "epoch 21 train loss = 0.6335040032863617, accurancy = 0 time = 0.5704555511474609\n",
      "Validation Loss =  6.094222068786621\n",
      "Validation BLEU =  0.99538064335188\n",
      "Training Loss =  0.6163754761219025\n",
      "Traning BLEU =  88.75061693617317\n",
      "epoch 22 train loss = 0.759683221578598, accurancy = 0 time = 0.5721607208251953\n",
      "Validation Loss =  6.091517448425293\n",
      "Validation BLEU =  0.99538064335188\n",
      "Training Loss =  0.6116353273391724\n",
      "Traning BLEU =  88.24083862058099\n",
      "epoch 23 train loss = 0.7801775336265564, accurancy = 0 time = 0.5775926113128662\n",
      "Validation Loss =  5.704530239105225\n",
      "Validation BLEU =  1.0239830116798412\n",
      "Training Loss =  0.7697536647319794\n",
      "Traning BLEU =  82.76403314845692\n",
      "epoch 24 train loss = 0.7256855964660645, accurancy = 0 time = 0.5788781642913818\n",
      "Validation Loss =  5.701312065124512\n",
      "Validation BLEU =  1.0239830116798412\n",
      "Training Loss =  0.6046393811702728\n",
      "Traning BLEU =  91.78551511108145\n",
      "epoch 25 train loss = 0.7280429899692535, accurancy = 0 time = 0.5686712265014648\n",
      "Validation Loss =  5.700418472290039\n",
      "Validation BLEU =  1.0239830116798412\n",
      "Training Loss =  0.8063792884349823\n",
      "Traning BLEU =  85.35101227958121\n",
      "epoch 26 train loss = 0.7649208903312683, accurancy = 0 time = 0.5845601558685303\n",
      "Validation Loss =  5.7054572105407715\n",
      "Validation BLEU =  1.0239830116798412\n",
      "Training Loss =  0.765256255865097\n",
      "Traning BLEU =  84.34015927367392\n",
      "epoch 27 train loss = 0.6067363023757935, accurancy = 0 time = 0.5683295726776123\n",
      "Validation Loss =  6.111793518066406\n",
      "Validation BLEU =  1.0342173160773085\n",
      "Training Loss =  0.7269018590450287\n",
      "Traning BLEU =  86.7816127020206\n",
      "epoch 28 train loss = 0.8320780694484711, accurancy = 0 time = 0.5885546207427979\n",
      "Validation Loss =  6.11321496963501\n",
      "Validation BLEU =  1.0342173160773085\n",
      "Training Loss =  0.6558685600757599\n",
      "Traning BLEU =  87.08024628827323\n",
      "epoch 29 train loss = 0.6553978323936462, accurancy = 0 time = 0.5769941806793213\n",
      "Validation Loss =  5.710544109344482\n",
      "Validation BLEU =  1.0239830116798412\n",
      "Training Loss =  0.5831932127475739\n",
      "Traning BLEU =  90.5322815780048\n",
      "epoch 30 train loss = 0.6364312469959259, accurancy = 0 time = 0.5681967735290527\n",
      "Validation Loss =  6.10376501083374\n",
      "Validation BLEU =  0.99538064335188\n",
      "Training Loss =  0.6877910792827606\n",
      "Traning BLEU =  89.82802815407626\n",
      "epoch 31 train loss = 0.5892358422279358, accurancy = 0 time = 0.5717110633850098\n",
      "Validation Loss =  6.111862659454346\n",
      "Validation BLEU =  0.99538064335188\n",
      "Training Loss =  0.6508575975894928\n",
      "Traning BLEU =  88.61045839815338\n",
      "epoch 32 train loss = 0.7118270397186279, accurancy = 0 time = 0.5822598934173584\n",
      "Validation Loss =  5.721348762512207\n",
      "Validation BLEU =  1.0233204147948942\n",
      "Training Loss =  0.6783157736063004\n",
      "Traning BLEU =  89.46227501220731\n",
      "epoch 33 train loss = 0.6775154173374176, accurancy = 0 time = 0.5734803676605225\n",
      "Validation Loss =  6.052371025085449\n",
      "Validation BLEU =  0.9102267982746022\n",
      "Training Loss =  0.7080252468585968\n",
      "Traning BLEU =  88.98304384889113\n",
      "epoch 34 train loss = 0.7428307831287384, accurancy = 0 time = 0.5878980159759521\n",
      "Validation Loss =  5.7307515144348145\n",
      "Validation BLEU =  1.0233204147948942\n",
      "Training Loss =  0.5591281354427338\n",
      "Traning BLEU =  91.41943111802574\n",
      "epoch 35 train loss = 0.6922643482685089, accurancy = 0 time = 0.5672838687896729\n",
      "Validation Loss =  6.14777946472168\n",
      "Validation BLEU =  0.99538064335188\n",
      "Training Loss =  0.6063341498374939\n",
      "Traning BLEU =  88.96676635178402\n",
      "epoch 36 train loss = 0.6797850430011749, accurancy = 0 time = 0.5779967308044434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss =  5.720101356506348\n",
      "Validation BLEU =  1.0233204147948942\n",
      "Training Loss =  0.7013128101825714\n",
      "Traning BLEU =  88.95576457742821\n",
      "epoch 37 train loss = 0.5626287460327148, accurancy = 0 time = 0.5676476955413818\n",
      "Validation Loss =  6.14041805267334\n",
      "Validation BLEU =  1.0342173160773085\n",
      "Training Loss =  0.695174902677536\n",
      "Traning BLEU =  91.66013984763782\n",
      "epoch 38 train loss = 0.5985316634178162, accurancy = 0 time = 0.5820577144622803\n",
      "Validation Loss =  6.062329292297363\n",
      "Validation BLEU =  1.0112110931564486\n",
      "Training Loss =  0.5383468866348267\n",
      "Traning BLEU =  93.5549214144863\n",
      "epoch 39 train loss = 0.6219794750213623, accurancy = 0 time = 0.571796178817749\n",
      "Validation Loss =  5.697659015655518\n",
      "Validation BLEU =  1.0233204147948942\n",
      "Training Loss =  0.5372811406850815\n",
      "Traning BLEU =  94.31744170203211\n",
      "epoch 40 train loss = 0.6371090710163116, accurancy = 0 time = 0.5674548149108887\n",
      "Validation Loss =  6.114063739776611\n",
      "Validation BLEU =  1.0342173160773085\n",
      "Training Loss =  0.6170539855957031\n",
      "Traning BLEU =  92.5611268031871\n",
      "epoch 41 train loss = 0.6348854303359985, accurancy = 0 time = 0.5827105045318604\n",
      "Validation Loss =  5.713873863220215\n",
      "Validation BLEU =  1.0233204147948942\n",
      "Training Loss =  0.5289008915424347\n",
      "Traning BLEU =  93.85041773932441\n",
      "epoch 42 train loss = 0.5550681203603745, accurancy = 0 time = 0.5685067176818848\n",
      "Validation Loss =  5.728332042694092\n",
      "Validation BLEU =  1.0233204147948942\n",
      "Training Loss =  0.5198671072721481\n",
      "Traning BLEU =  95.09521222985529\n",
      "epoch 43 train loss = 0.5355041921138763, accurancy = 0 time = 0.5620617866516113\n",
      "Validation Loss =  6.082438945770264\n",
      "Validation BLEU =  1.040921919976357\n",
      "Training Loss =  0.6139846593141556\n",
      "Traning BLEU =  91.67194870848891\n",
      "epoch 44 train loss = 0.5773452818393707, accurancy = 0 time = 0.5829482078552246\n",
      "Validation Loss =  6.135870933532715\n",
      "Validation BLEU =  1.069225551175948\n",
      "Training Loss =  0.5886013954877853\n",
      "Traning BLEU =  92.27049662690878\n",
      "epoch 45 train loss = 0.5572373569011688, accurancy = 0 time = 0.5775206089019775\n",
      "Validation Loss =  6.127722263336182\n",
      "Validation BLEU =  0.99538064335188\n",
      "Training Loss =  0.5017047822475433\n",
      "Traning BLEU =  94.5898752523\n",
      "epoch 46 train loss = 0.5136369317770004, accurancy = 0 time = 0.5658912658691406\n",
      "Validation Loss =  6.130949020385742\n",
      "Validation BLEU =  0.99538064335188\n",
      "Training Loss =  0.6191646754741669\n",
      "Traning BLEU =  91.94434817686407\n",
      "epoch 47 train loss = 0.5649784207344055, accurancy = 0 time = 0.581714391708374\n",
      "Validation Loss =  5.710463523864746\n",
      "Validation BLEU =  1.0233204147948942\n",
      "Training Loss =  0.49170802533626556\n",
      "Traning BLEU =  95.09521222985529\n",
      "epoch 48 train loss = 0.6207540035247803, accurancy = 0 time = 0.5733561515808105\n",
      "Validation Loss =  6.090756416320801\n",
      "Validation BLEU =  1.0112110931564486\n",
      "Training Loss =  0.594021737575531\n",
      "Traning BLEU =  90.91248607559994\n",
      "epoch 49 train loss = 0.5247418135404587, accurancy = 0 time = 0.5815467834472656\n",
      "Validation Loss =  5.707911491394043\n",
      "Validation BLEU =  1.0233204147948942\n",
      "Training Loss =  0.48135654628276825\n",
      "Traning BLEU =  95.32778177911217\n",
      "epoch 50 train loss = 0.5847732722759247, accurancy = 0 time = 0.5719656944274902\n",
      "Validation Loss =  5.697192192077637\n",
      "Validation BLEU =  1.0233204147948942\n",
      "Training Loss =  0.5802667140960693\n",
      "Traning BLEU =  91.8928958725527\n",
      "epoch 51 train loss = 0.5224614441394806, accurancy = 0 time = 0.5776782035827637\n",
      "Validation Loss =  6.149547576904297\n",
      "Validation BLEU =  1.0342173160773085\n",
      "Training Loss =  0.6009396463632584\n",
      "Traning BLEU =  91.91345798248922\n",
      "epoch 52 train loss = 0.4966662973165512, accurancy = 0 time = 0.5816209316253662\n",
      "Validation Loss =  6.090121746063232\n",
      "Validation BLEU =  1.0112110931564486\n",
      "Training Loss =  0.4671773314476013\n",
      "Traning BLEU =  95.83260078073766\n",
      "epoch 53 train loss = 0.5690332800149918, accurancy = 0 time = 0.5721414089202881\n",
      "Validation Loss =  6.083540916442871\n",
      "Validation BLEU =  1.1119587265542117\n",
      "Training Loss =  0.488629549741745\n",
      "Traning BLEU =  93.7242177394106\n",
      "epoch 54 train loss = 0.5538072884082794, accurancy = 0 time = 0.577019214630127\n",
      "Validation Loss =  5.699404716491699\n",
      "Validation BLEU =  1.0233204147948942\n",
      "Training Loss =  0.5638659000396729\n",
      "Traning BLEU =  91.67194870848891\n",
      "epoch 55 train loss = 0.47009579837322235, accurancy = 0 time = 0.5734248161315918\n",
      "Validation Loss =  6.143941879272461\n",
      "Validation BLEU =  1.1536652305263282\n",
      "Training Loss =  0.4798572063446045\n",
      "Traning BLEU =  93.7242177394106\n",
      "epoch 56 train loss = 0.5215170830488205, accurancy = 0 time = 0.5771710872650146\n",
      "Validation Loss =  6.096822261810303\n",
      "Validation BLEU =  1.1119587265542117\n",
      "Training Loss =  0.4496217221021652\n",
      "Traning BLEU =  95.32778177911217\n",
      "epoch 57 train loss = 0.5118466019630432, accurancy = 0 time = 0.5722799301147461\n",
      "Validation Loss =  5.712480545043945\n",
      "Validation BLEU =  0.8605064684569708\n",
      "Training Loss =  0.445914089679718\n",
      "Traning BLEU =  96.06417501738943\n",
      "epoch 58 train loss = 0.5543957054615021, accurancy = 0 time = 0.572023868560791\n",
      "Validation Loss =  5.705680847167969\n",
      "Validation BLEU =  1.0233204147948942\n",
      "Training Loss =  0.46308328211307526\n",
      "Traning BLEU =  96.45129821571211\n",
      "epoch 59 train loss = 0.4530963897705078, accurancy = 0 time = 0.5670123100280762\n",
      "Validation Loss =  5.7031989097595215\n",
      "Validation BLEU =  1.0233204147948942\n",
      "Training Loss =  0.43648016452789307\n",
      "Traning BLEU =  96.79909129423824\n",
      "epoch 60 train loss = 0.5072066932916641, accurancy = 0 time = 0.5730233192443848\n",
      "Validation Loss =  5.9819560050964355\n",
      "Validation BLEU =  1.1914662620373753\n",
      "Training Loss =  0.47686126828193665\n",
      "Traning BLEU =  95.63166492237713\n",
      "epoch 61 train loss = 0.5118584185838699, accurancy = 0 time = 0.5881927013397217\n",
      "Validation Loss =  5.98326301574707\n",
      "Validation BLEU =  1.1914662620373753\n",
      "Training Loss =  0.47190676629543304\n",
      "Traning BLEU =  95.63166492237713\n",
      "epoch 62 train loss = 0.4912416636943817, accurancy = 0 time = 0.5816082954406738\n",
      "Validation Loss =  6.095855712890625\n",
      "Validation BLEU =  1.1119587265542117\n",
      "Training Loss =  0.4248359054327011\n",
      "Traning BLEU =  96.56849084663749\n",
      "epoch 63 train loss = 0.44711561501026154, accurancy = 0 time = 0.5713675022125244\n",
      "Validation Loss =  5.696512699127197\n",
      "Validation BLEU =  1.0233204147948942\n",
      "Training Loss =  0.5025776773691177\n",
      "Traning BLEU =  93.84178063299605\n",
      "epoch 64 train loss = 0.523667722940445, accurancy = 0 time = 0.5833272933959961\n",
      "Validation Loss =  5.699662208557129\n",
      "Validation BLEU =  0.8605064684569708\n",
      "Training Loss =  0.4833116680383682\n",
      "Traning BLEU =  95.28376369438097\n",
      "epoch 65 train loss = 0.5069347620010376, accurancy = 0 time = 0.582371711730957\n",
      "Validation Loss =  5.707961559295654\n",
      "Validation BLEU =  0.8605064684569708\n",
      "Training Loss =  0.45345543324947357\n",
      "Traning BLEU =  95.63166492237713\n",
      "epoch 66 train loss = 0.4857948571443558, accurancy = 0 time = 0.5832042694091797\n",
      "Validation Loss =  5.711403846740723\n",
      "Validation BLEU =  1.0233204147948942\n",
      "Training Loss =  0.43966948986053467\n",
      "Traning BLEU =  95.63166492237713\n",
      "epoch 67 train loss = 0.428657665848732, accurancy = 0 time = 0.5723469257354736\n",
      "Validation Loss =  5.998764991760254\n",
      "Validation BLEU =  1.1604258422741849\n",
      "Training Loss =  0.42442528903484344\n",
      "Traning BLEU =  97.04129713549354\n",
      "epoch 68 train loss = 0.49625200033187866, accurancy = 0 time = 0.5823614597320557\n",
      "Validation Loss =  5.705739498138428\n",
      "Validation BLEU =  0.8605064684569708\n",
      "Training Loss =  0.403038814663887\n",
      "Traning BLEU =  97.30291818816269\n",
      "epoch 69 train loss = 0.48727867007255554, accurancy = 0 time = 0.5724349021911621\n",
      "Validation Loss =  6.146906852722168\n",
      "Validation BLEU =  1.1248984529973598\n",
      "Training Loss =  0.43017837405204773\n",
      "Traning BLEU =  97.52268548242196\n",
      "epoch 70 train loss = 0.4091748297214508, accurancy = 0 time = 0.5736997127532959\n",
      "Validation Loss =  6.138978481292725\n",
      "Validation BLEU =  1.1275273980624485\n",
      "Training Loss =  0.4446725994348526\n",
      "Traning BLEU =  96.1134461581606\n",
      "epoch 71 train loss = 0.45492514967918396, accurancy = 0 time = 0.5843079090118408\n",
      "Validation Loss =  6.144855976104736\n",
      "Validation BLEU =  1.1248984529973598\n",
      "Training Loss =  0.439443439245224\n",
      "Traning BLEU =  96.1134461581606\n",
      "epoch 72 train loss = 0.4498436152935028, accurancy = 0 time = 0.5880589485168457\n",
      "Validation Loss =  6.1473307609558105\n",
      "Validation BLEU =  1.1248984529973598\n",
      "Training Loss =  0.3883621394634247\n",
      "Traning BLEU =  97.80652649882437\n",
      "epoch 73 train loss = 0.445099338889122, accurancy = 0 time = 0.5776464939117432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss =  6.1533732414245605\n",
      "Validation BLEU =  1.1248984529973598\n",
      "Training Loss =  0.42963166534900665\n",
      "Traning BLEU =  96.04930635190142\n",
      "epoch 74 train loss = 0.39423687756061554, accurancy = 0 time = 0.5832028388977051\n",
      "Validation Loss =  5.712463855743408\n",
      "Validation BLEU =  1.0233204147948942\n",
      "Training Loss =  0.4267467111349106\n",
      "Traning BLEU =  96.1134461581606\n",
      "epoch 75 train loss = 0.38978005945682526, accurancy = 0 time = 0.5730526447296143\n",
      "Validation Loss =  5.955895900726318\n",
      "Validation BLEU =  1.2045646412213915\n",
      "Training Loss =  0.3948701024055481\n",
      "Traning BLEU =  98.25529287884774\n",
      "epoch 76 train loss = 0.4320783466100693, accurancy = 0 time = 0.5767767429351807\n",
      "Validation Loss =  5.704555988311768\n",
      "Validation BLEU =  0.8605064684569708\n",
      "Training Loss =  0.3715997636318207\n",
      "Traning BLEU =  98.53905577516564\n",
      "epoch 77 train loss = 0.42521414160728455, accurancy = 0 time = 0.5700106620788574\n",
      "Validation Loss =  5.7096123695373535\n",
      "Validation BLEU =  0.8605064684569708\n",
      "Training Loss =  0.38709789514541626\n",
      "Traning BLEU =  98.25529287884774\n",
      "epoch 78 train loss = 0.4203742444515228, accurancy = 0 time = 0.5774586200714111\n",
      "Validation Loss =  5.713378429412842\n",
      "Validation BLEU =  1.0233204147948942\n",
      "Training Loss =  0.39231596887111664\n",
      "Traning BLEU =  97.03333706229978\n",
      "epoch 79 train loss = 0.37356971204280853, accurancy = 0 time = 0.5677394866943359\n",
      "Validation Loss =  5.7041707038879395\n",
      "Validation BLEU =  1.0233204147948942\n",
      "Training Loss =  0.40548036992549896\n",
      "Traning BLEU =  95.62398630079502\n",
      "epoch 80 train loss = 0.41073183715343475, accurancy = 0 time = 0.5827927589416504\n",
      "Validation Loss =  6.030794143676758\n",
      "Validation BLEU =  1.1303478539187033\n",
      "Training Loss =  0.3562258630990982\n",
      "Traning BLEU =  97.80652649882437\n",
      "epoch 81 train loss = 0.3675714582204819, accurancy = 0 time = 0.5707480907440186\n",
      "Validation Loss =  6.1068339347839355\n",
      "Validation BLEU =  1.0910692113689895\n",
      "Training Loss =  0.35234856605529785\n",
      "Traning BLEU =  97.80652649882437\n",
      "epoch 82 train loss = 0.3625600188970566, accurancy = 0 time = 0.5721209049224854\n",
      "Validation Loss =  6.104348182678223\n",
      "Validation BLEU =  1.0674648204759571\n",
      "Training Loss =  0.3583337068557739\n",
      "Traning BLEU =  97.80652649882437\n",
      "epoch 83 train loss = 0.3581116795539856, accurancy = 0 time = 0.5710887908935547\n",
      "Validation Loss =  5.700199604034424\n",
      "Validation BLEU =  0.8605064684569708\n",
      "Training Loss =  0.3745180517435074\n",
      "Traning BLEU =  96.397279160888\n",
      "epoch 84 train loss = 0.39167843759059906, accurancy = 0 time = 0.573366641998291\n",
      "Validation Loss =  5.701757907867432\n",
      "Validation BLEU =  1.0233204147948942\n",
      "Training Loss =  0.3703954666852951\n",
      "Traning BLEU =  97.52268548242196\n",
      "epoch 85 train loss = 0.3508334159851074, accurancy = 0 time = 0.567291259765625\n",
      "Validation Loss =  6.105430603027344\n",
      "Validation BLEU =  1.0910692113689895\n",
      "Training Loss =  0.3478521406650543\n",
      "Traning BLEU =  98.53905577516564\n",
      "epoch 86 train loss = 0.37895989418029785, accurancy = 0 time = 0.5773458480834961\n",
      "Validation Loss =  6.104340076446533\n",
      "Validation BLEU =  1.0910692113689895\n",
      "Training Loss =  0.3335952162742615\n",
      "Traning BLEU =  98.53905577516564\n",
      "epoch 87 train loss = 0.36170732975006104, accurancy = 0 time = 0.5779280662536621\n",
      "Validation Loss =  5.695770740509033\n",
      "Validation BLEU =  0.8605064684569708\n",
      "Training Loss =  0.33109329640865326\n",
      "Traning BLEU =  97.80652649882437\n",
      "epoch 88 train loss = 0.37852413952350616, accurancy = 0 time = 0.5715785026550293\n",
      "Validation Loss =  6.1020636558532715\n",
      "Validation BLEU =  1.0674648204759571\n",
      "Training Loss =  0.3751522898674011\n",
      "Traning BLEU =  96.1134461581606\n",
      "epoch 89 train loss = 0.3529615253210068, accurancy = 0 time = 0.5820236206054688\n",
      "Validation Loss =  5.699941635131836\n",
      "Validation BLEU =  0.8605064684569708\n",
      "Training Loss =  0.3407886475324631\n",
      "Traning BLEU =  97.12965468590573\n",
      "epoch 90 train loss = 0.3806682825088501, accurancy = 0 time = 0.5751447677612305\n",
      "Validation Loss =  6.104760646820068\n",
      "Validation BLEU =  1.0674648204759571\n",
      "Training Loss =  0.3235657811164856\n",
      "Traning BLEU =  99.27020306293286\n",
      "epoch 91 train loss = 0.34658314287662506, accurancy = 0 time = 0.5739734172821045\n",
      "Validation Loss =  5.699982166290283\n",
      "Validation BLEU =  1.0233204147948942\n",
      "Training Loss =  0.33397457003593445\n",
      "Traning BLEU =  97.86063382023244\n",
      "epoch 92 train loss = 0.32977744936943054, accurancy = 0 time = 0.5703294277191162\n",
      "Validation Loss =  5.69875955581665\n",
      "Validation BLEU =  1.0233204147948942\n",
      "Training Loss =  0.33078527450561523\n",
      "Traning BLEU =  97.86063382023244\n",
      "epoch 93 train loss = 0.3393126130104065, accurancy = 0 time = 0.5768985748291016\n",
      "Validation Loss =  6.131392478942871\n",
      "Validation BLEU =  1.0112110931564486\n",
      "Training Loss =  0.3115106076002121\n",
      "Traning BLEU =  99.27020306293286\n",
      "epoch 94 train loss = 0.32078778743743896, accurancy = 0 time = 0.5709426403045654\n",
      "Validation Loss =  6.125330448150635\n",
      "Validation BLEU =  0.9781023485658998\n",
      "Training Loss =  0.3078816533088684\n",
      "Traning BLEU =  99.27020306293286\n",
      "epoch 95 train loss = 0.3167108744382858, accurancy = 0 time = 0.5762407779693604\n",
      "Validation Loss =  6.131190776824951\n",
      "Validation BLEU =  0.9781023485658998\n",
      "Training Loss =  0.3219428062438965\n",
      "Traning BLEU =  98.14431136053088\n",
      "epoch 96 train loss = 0.34507785737514496, accurancy = 0 time = 0.5797762870788574\n",
      "Validation Loss =  5.700315952301025\n",
      "Validation BLEU =  0.8605064684569708\n",
      "Training Loss =  0.3184834420681\n",
      "Traning BLEU =  98.14431136053088\n",
      "epoch 97 train loss = 0.31037114560604095, accurancy = 0 time = 0.5671885013580322\n",
      "Validation Loss =  5.692702293395996\n",
      "Validation BLEU =  0.8605064684569708\n",
      "Training Loss =  0.29873980581760406\n",
      "Traning BLEU =  99.27020306293286\n",
      "epoch 98 train loss = 0.32626792788505554, accurancy = 0 time = 0.572617769241333\n",
      "Validation Loss =  5.694674968719482\n",
      "Validation BLEU =  0.8605064684569708\n",
      "Training Loss =  0.29514554142951965\n",
      "Traning BLEU =  99.27020306293286\n",
      "epoch 99 train loss = 0.30486729741096497, accurancy = 0 time = 0.5629980564117432\n",
      "Training completed. Best accuracy is 0\n"
     ]
    }
   ],
   "source": [
    "enc, dec = train_model(encoder_optimizer,decoder_optimizer, encoder, decoder, criterion, num_epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 47862])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = next(iter(dataloader['train']))\n",
    "out,hid = encode_decode(enc,dec,data[0][1].view(1,-1).cuda(),data[1][1].view(1,-1).cuda())\n",
    "_, top1_predicted = torch.max(out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 68, 104,  21, 105, 102, 106, 107,  93, 108, 109,  95,  68, 110,  16,\n",
       "          96, 111,  98,  46,   1,   3,   3,   3,   3,   3,   3,   3,   3,   3,\n",
       "           3,   3]], device='cuda:0')"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top1_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "given=[]\n",
    "for i in data[1][1]:\n",
    "    given.append(vi_lang.index2word[i.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "for i in top1_predicted[0]:\n",
    "#     print(i.item())\n",
    "    pred.append(en_lang.index2word[i.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trông',\n",
       " '40',\n",
       " 'những',\n",
       " 'quốc_gia',\n",
       " 'nhà',\n",
       " 'khác',\n",
       " 'nhau',\n",
       " 'ipcc',\n",
       " 'gần',\n",
       " '1000',\n",
       " 'ra_bài',\n",
       " 'trông',\n",
       " 'trang',\n",
       " 'khí_quyển',\n",
       " 'họ',\n",
       " 'chủ_đề',\n",
       " 'được',\n",
       " 'để',\n",
       " 'EOS',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD']"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and',\n",
       " 'all',\n",
       " 'of',\n",
       " 'those',\n",
       " 'pages',\n",
       " 'were',\n",
       " 'reviewed',\n",
       " 'by',\n",
       " 'another',\n",
       " '400-plus',\n",
       " 'scientists',\n",
       " 'and',\n",
       " 'reviewers',\n",
       " ',',\n",
       " 'from',\n",
       " '113',\n",
       " 'countries',\n",
       " '.',\n",
       " 'EOS',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD']"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = []\n",
    "for i in data[1][1]:\n",
    "#     print(i.item())\n",
    "    true.append(en_lang.index2word[i.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and',\n",
       " 'all',\n",
       " 'of',\n",
       " 'those',\n",
       " 'pages',\n",
       " 'were',\n",
       " 'reviewed',\n",
       " 'by',\n",
       " 'another',\n",
       " '400-plus',\n",
       " 'scientists',\n",
       " 'and',\n",
       " 'reviewers',\n",
       " ',',\n",
       " 'from',\n",
       " '113',\n",
       " 'countries',\n",
       " '.',\n",
       " 'EOS',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD']"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
