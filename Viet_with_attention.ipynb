{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pdb\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IDX = 2\n",
    "PAD_IDX = 3\n",
    "SOS_token = 0\n",
    "EOS_token = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(file):\n",
    "    f = open(file)\n",
    "    list_l = []\n",
    "    for line in f:\n",
    "        list_l.append(line.strip())\n",
    "    df = pd.DataFrame()\n",
    "    df['data'] = list_l\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_train = read_dataset(\"/scratch/ark576/machine_translation_data/iwslt-vi-en/train.tok.en\")\n",
    "en_val = read_dataset(\"/scratch/ark576/machine_translation_data/iwslt-vi-en/dev.tok.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_train = read_dataset(\"/scratch/ark576/machine_translation_data/iwslt-vi-en/train.tok.vi\")\n",
    "vi_val = read_dataset(\"/scratch/ark576/machine_translation_data/iwslt-vi-en/dev.tok.vi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(133317, 1) (133317, 1)\n",
      "(1268, 1) (1268, 1)\n"
     ]
    }
   ],
   "source": [
    "print(en_train.shape,vi_train.shape)\n",
    "print(en_val.shape,vi_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame()\n",
    "train['en_data'] = en_train['data']\n",
    "train['vi_data'] = vi_train['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ark576/.conda/envs/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "en_data                  \n",
       "vi_data    \" tán_thưởng \"\n",
       "Name: 38600, dtype: object"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.ix[38600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = pd.DataFrame()\n",
    "val['en_data'] = en_val['data']\n",
    "val['vi_data'] = vi_val['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(133317, 2) (1268, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape,val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2:\"UKN\",3:\"PAD\"}\n",
    "        self.n_words = 4  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word.lower())\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['en_data'] = train['en_data'].apply(lambda x: x.lower())\n",
    "# train['vi_data'] = train['vi_data'].apply(lambda x:x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_lang = Lang(\"en\")\n",
    "for ex in train['en_data']:\n",
    "#     ex = unicodeToAscii(ex)\n",
    "#     ex = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", ex)\n",
    "    en_lang.addSentence(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_lang = Lang(\"vi\")\n",
    "for ex in train['vi_data']:\n",
    "#     ex = unicodeToAscii(ex)\n",
    "#     ex = re.sub(r\"[^a-zA-Z.!?_]+\", r\" \", ex)\n",
    "    vi_lang.addSentence(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vi_lang.word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(df):\n",
    "    df['en_tokenized'] = df[\"en_data\"].apply(lambda x:x.lower().split( ))\n",
    "    df['vi_tokenized'] = df['vi_data'].apply(lambda x:x.lower().split( ))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = split(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = split(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en_data</th>\n",
       "      <th>vi_data</th>\n",
       "      <th>en_tokenized</th>\n",
       "      <th>vi_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rachel Pike : The science behind a climate hea...</td>\n",
       "      <td>Khoa_học đằng_sau một tiêu_đề về khí_hậu</td>\n",
       "      <td>[rachel, pike, :, the, science, behind, a, cli...</td>\n",
       "      <td>[khoa_học, đằng_sau, một, tiêu_đề, về, khí_hậu]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In 4 minutes , atmospheric chemist Rachel Pike...</td>\n",
       "      <td>Trong 4 phút , chuyên_gia hoá_học khí_quyển Ra...</td>\n",
       "      <td>[in, 4, minutes, ,, atmospheric, chemist, rach...</td>\n",
       "      <td>[trong, 4, phút, ,, chuyên_gia, hoá_học, khí_q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I &amp;apos;d like to talk to you today about the ...</td>\n",
       "      <td>Tôi muốn cho các bạn biết về sự to_lớn của nhữ...</td>\n",
       "      <td>[i, &amp;apos;d, like, to, talk, to, you, today, a...</td>\n",
       "      <td>[tôi, muốn, cho, các, bạn, biết, về, sự, to_lớ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Headlines that look like this when they have t...</td>\n",
       "      <td>Có những dòng trông như thế_này khi bàn về biế...</td>\n",
       "      <td>[headlines, that, look, like, this, when, they...</td>\n",
       "      <td>[có, những, dòng, trông, như, thế_này, khi, bà...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>They are both two branches of the same field o...</td>\n",
       "      <td>Cả hai đều là một nhánh của cùng một lĩnh_vực ...</td>\n",
       "      <td>[they, are, both, two, branches, of, the, same...</td>\n",
       "      <td>[cả, hai, đều, là, một, nhánh, của, cùng, một,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             en_data  \\\n",
       "0  Rachel Pike : The science behind a climate hea...   \n",
       "1  In 4 minutes , atmospheric chemist Rachel Pike...   \n",
       "2  I &apos;d like to talk to you today about the ...   \n",
       "3  Headlines that look like this when they have t...   \n",
       "4  They are both two branches of the same field o...   \n",
       "\n",
       "                                             vi_data  \\\n",
       "0           Khoa_học đằng_sau một tiêu_đề về khí_hậu   \n",
       "1  Trong 4 phút , chuyên_gia hoá_học khí_quyển Ra...   \n",
       "2  Tôi muốn cho các bạn biết về sự to_lớn của nhữ...   \n",
       "3  Có những dòng trông như thế_này khi bàn về biế...   \n",
       "4  Cả hai đều là một nhánh của cùng một lĩnh_vực ...   \n",
       "\n",
       "                                        en_tokenized  \\\n",
       "0  [rachel, pike, :, the, science, behind, a, cli...   \n",
       "1  [in, 4, minutes, ,, atmospheric, chemist, rach...   \n",
       "2  [i, &apos;d, like, to, talk, to, you, today, a...   \n",
       "3  [headlines, that, look, like, this, when, they...   \n",
       "4  [they, are, both, two, branches, of, the, same...   \n",
       "\n",
       "                                        vi_tokenized  \n",
       "0    [khoa_học, đằng_sau, một, tiêu_đề, về, khí_hậu]  \n",
       "1  [trong, 4, phút, ,, chuyên_gia, hoá_học, khí_q...  \n",
       "2  [tôi, muốn, cho, các, bạn, biết, về, sự, to_lớ...  \n",
       "3  [có, những, dòng, trông, như, thế_này, khi, bà...  \n",
       "4  [cả, hai, đều, là, một, nhánh, của, cùng, một,...  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train[train['vi_len']==713]['en_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vi_lang.word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token2index_dataset(df):\n",
    "    for lan in ['en','vi']:\n",
    "        indices_data = []\n",
    "        if lan=='en':\n",
    "            lang_obj = en_lang\n",
    "        else:\n",
    "            lang_obj = vi_lang\n",
    "        for tokens in df[lan+'_tokenized']:\n",
    "            index_list = [lang_obj.word2index[token] if token in lang_obj.word2index else UNK_IDX for token in tokens]\n",
    "            index_list.append(EOS_token)\n",
    "#             index_list.insert(0,SOS_token)\n",
    "            indices_data.append(index_list)\n",
    "        df[lan+'_idized'] = indices_data\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = token2index_dataset(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = token2index_dataset(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vietnamese(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        english = self.df.iloc[idx]['en_idized']\n",
    "        viet = self.df.iloc[idx]['vi_idized']\n",
    "        en_len = self.df.iloc[idx]['en_len']\n",
    "        vi_len = self.df.iloc[idx]['vi_len']\n",
    "        return [english,viet,en_len,vi_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en_data</th>\n",
       "      <th>vi_data</th>\n",
       "      <th>en_tokenized</th>\n",
       "      <th>vi_tokenized</th>\n",
       "      <th>en_idized</th>\n",
       "      <th>vi_idized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rachel Pike : The science behind a climate hea...</td>\n",
       "      <td>Khoa_học đằng_sau một tiêu_đề về khí_hậu</td>\n",
       "      <td>[rachel, pike, :, the, science, behind, a, cli...</td>\n",
       "      <td>[khoa_học, đằng_sau, một, tiêu_đề, về, khí_hậu]</td>\n",
       "      <td>[4, 5, 6, 7, 8, 9, 10, 11, 12, 1]</td>\n",
       "      <td>[4, 5, 6, 7, 8, 9, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In 4 minutes , atmospheric chemist Rachel Pike...</td>\n",
       "      <td>Trong 4 phút , chuyên_gia hoá_học khí_quyển Ra...</td>\n",
       "      <td>[in, 4, minutes, ,, atmospheric, chemist, rach...</td>\n",
       "      <td>[trong, 4, phút, ,, chuyên_gia, hoá_học, khí_q...</td>\n",
       "      <td>[13, 14, 15, 16, 17, 18, 4, 5, 19, 10, 20, 21,...</td>\n",
       "      <td>[10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I &amp;apos;d like to talk to you today about the ...</td>\n",
       "      <td>Tôi muốn cho các bạn biết về sự to_lớn của nhữ...</td>\n",
       "      <td>[i, &amp;apos;d, like, to, talk, to, you, today, a...</td>\n",
       "      <td>[tôi, muốn, cho, các, bạn, biết, về, sự, to_lớ...</td>\n",
       "      <td>[47, 48, 49, 50, 51, 50, 52, 53, 54, 7, 55, 21...</td>\n",
       "      <td>[52, 53, 38, 54, 55, 56, 8, 57, 58, 30, 21, 22...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Headlines that look like this when they have t...</td>\n",
       "      <td>Có những dòng trông như thế_này khi bàn về biế...</td>\n",
       "      <td>[headlines, that, look, like, this, when, they...</td>\n",
       "      <td>[có, những, dòng, trông, như, thế_này, khi, bà...</td>\n",
       "      <td>[26, 56, 62, 49, 63, 64, 65, 66, 50, 67, 29, 1...</td>\n",
       "      <td>[67, 21, 61, 68, 69, 70, 71, 72, 8, 25, 9, 13,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>They are both two branches of the same field o...</td>\n",
       "      <td>Cả hai đều là một nhánh của cùng một lĩnh_vực ...</td>\n",
       "      <td>[they, are, both, two, branches, of, the, same...</td>\n",
       "      <td>[cả, hai, đều, là, một, nhánh, của, cùng, một,...</td>\n",
       "      <td>[65, 73, 74, 75, 76, 21, 7, 77, 78, 21, 17, 8,...</td>\n",
       "      <td>[80, 81, 82, 83, 6, 84, 30, 26, 6, 85, 10, 86,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             en_data  \\\n",
       "0  Rachel Pike : The science behind a climate hea...   \n",
       "1  In 4 minutes , atmospheric chemist Rachel Pike...   \n",
       "2  I &apos;d like to talk to you today about the ...   \n",
       "3  Headlines that look like this when they have t...   \n",
       "4  They are both two branches of the same field o...   \n",
       "\n",
       "                                             vi_data  \\\n",
       "0           Khoa_học đằng_sau một tiêu_đề về khí_hậu   \n",
       "1  Trong 4 phút , chuyên_gia hoá_học khí_quyển Ra...   \n",
       "2  Tôi muốn cho các bạn biết về sự to_lớn của nhữ...   \n",
       "3  Có những dòng trông như thế_này khi bàn về biế...   \n",
       "4  Cả hai đều là một nhánh của cùng một lĩnh_vực ...   \n",
       "\n",
       "                                        en_tokenized  \\\n",
       "0  [rachel, pike, :, the, science, behind, a, cli...   \n",
       "1  [in, 4, minutes, ,, atmospheric, chemist, rach...   \n",
       "2  [i, &apos;d, like, to, talk, to, you, today, a...   \n",
       "3  [headlines, that, look, like, this, when, they...   \n",
       "4  [they, are, both, two, branches, of, the, same...   \n",
       "\n",
       "                                        vi_tokenized  \\\n",
       "0    [khoa_học, đằng_sau, một, tiêu_đề, về, khí_hậu]   \n",
       "1  [trong, 4, phút, ,, chuyên_gia, hoá_học, khí_q...   \n",
       "2  [tôi, muốn, cho, các, bạn, biết, về, sự, to_lớ...   \n",
       "3  [có, những, dòng, trông, như, thế_này, khi, bà...   \n",
       "4  [cả, hai, đều, là, một, nhánh, của, cùng, một,...   \n",
       "\n",
       "                                           en_idized  \\\n",
       "0                  [4, 5, 6, 7, 8, 9, 10, 11, 12, 1]   \n",
       "1  [13, 14, 15, 16, 17, 18, 4, 5, 19, 10, 20, 21,...   \n",
       "2  [47, 48, 49, 50, 51, 50, 52, 53, 54, 7, 55, 21...   \n",
       "3  [26, 56, 62, 49, 63, 64, 65, 66, 50, 67, 29, 1...   \n",
       "4  [65, 73, 74, 75, 76, 21, 7, 77, 78, 21, 17, 8,...   \n",
       "\n",
       "                                           vi_idized  \n",
       "0                              [4, 5, 6, 7, 8, 9, 1]  \n",
       "1  [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 8...  \n",
       "2  [52, 53, 38, 54, 55, 56, 8, 57, 58, 30, 21, 22...  \n",
       "3  [67, 21, 61, 68, 69, 70, 71, 72, 8, 25, 9, 13,...  \n",
       "4  [80, 81, 82, 83, 6, 84, 30, 26, 6, 85, 10, 86,...  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['en_len'] = train['en_idized'].apply(lambda x: len(x))\n",
    "train['vi_len'] = train['vi_idized'].apply(lambda x:len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "val['en_len'] = val['en_idized'].apply(lambda x: len(x))\n",
    "val['vi_len'] = val['vi_idized'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    133317.000000\n",
       "mean         21.299399\n",
       "std          15.035268\n",
       "min           1.000000\n",
       "25%          11.000000\n",
       "50%          17.000000\n",
       "75%          27.000000\n",
       "max         629.000000\n",
       "Name: en_len, dtype: float64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['en_len'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    133317.000000\n",
       "mean         21.120157\n",
       "std          15.115259\n",
       "min           1.000000\n",
       "25%          11.000000\n",
       "50%          17.000000\n",
       "75%          27.000000\n",
       "max         712.000000\n",
       "Name: vi_len, dtype: float64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['vi_len'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[np.logical_and(train['en_len']>=2,train['vi_len']>=2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train['vi_len']<=30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = val[np.logical_and(val['en_len']>=2,val['vi_len']>=2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = val[val['vi_len']<=30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 48\n",
    "def vocab_collate_func(batch):\n",
    "    en_data = []\n",
    "    vi_data = []\n",
    "    en_len = []\n",
    "    vi_len = []\n",
    "\n",
    "    for datum in batch:\n",
    "        en_len.append(datum[2])\n",
    "        vi_len.append(datum[3])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        if datum[2]>MAX_LEN:\n",
    "            padded_vec_s1 = np.array(datum[0])[:MAX_LEN]\n",
    "        else:\n",
    "            padded_vec_s1 = np.pad(np.array(datum[0]),\n",
    "                                pad_width=((0,MAX_LEN - datum[2])),\n",
    "                                mode=\"constant\", constant_values=PAD_IDX)\n",
    "        if datum[3]>MAX_LEN:\n",
    "            padded_vec_s2 = np.array(datum[1])[:MAX_LEN]\n",
    "        else:\n",
    "            padded_vec_s2 = np.pad(np.array(datum[1]),\n",
    "                                pad_width=((0,MAX_LEN - datum[3])),\n",
    "                                mode=\"constant\", constant_values=PAD_IDX)\n",
    "        en_data.append(padded_vec_s1)\n",
    "        vi_data.append(padded_vec_s2)\n",
    "        \n",
    "    return [torch.from_numpy(np.array(vi_data)), torch.from_numpy(np.array(en_data)),\n",
    "            torch.from_numpy(np.array(vi_len)), torch.from_numpy(np.array(en_len))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 10\n",
    "transformed_dataset = {'train': Vietnamese(train),\n",
    "                       'validate': Vietnamese(val)\n",
    "                                               }\n",
    "\n",
    "dataloader = {x: DataLoader(transformed_dataset[x], batch_size=bs, collate_fn=vocab_collate_func,\n",
    "                        shuffle=True, num_workers=0) for x in ['train', 'validate']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(dataloader['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1388,    35,   416,  1758,   131,     7, 26133,    46,     1,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3])\n",
      "tensor([ 35, 113,   7, 602, 547,   1,   3,   3,   3,   3,   3,   3,   3,   3,\n",
      "          3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,\n",
      "          3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,\n",
      "          3,   3,   3,   3,   3,   3])\n",
      "tensor([   63,   121,    10,    12,    96,     7,  1230, 14135,    13,     7,\n",
      "          717,     1,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3])\n",
      "tensor([361,  52,  46,   1,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,\n",
      "          3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,\n",
      "          3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,\n",
      "          3,   3,   3,   3,   3,   3])\n",
      "tensor([  56,  508,  693,  166,  262,  313,   50, 1051,   16, 1097,   16,  159,\n",
      "          75,   73, 2210,   46,    1,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3])\n",
      "tensor([  191,    63,    31,    21, 47449,    16,  6414, 19184,    68,  4845,\n",
      "         6409,  5023,    67,  1086,    27,    75,   170,    46,     1,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3])\n",
      "tensor([1097,   16,   47,  384,  385,   50,  374,   52, 1029, 1341,   54,  118,\n",
      "        8308,   46,    1,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3])\n",
      "tensor([   27,     7,   265,   357,    16,   429,  5807,    21,  4653,   287,\n",
      "         1564,    10,   200, 18846,    21,   193, 21875,    46,     1,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3])\n",
      "tensor([468, 779,  16, 542, 191,  47, 245,  56,  46, 542, 468, 779,  16, 542,\n",
      "         47, 245,  56,  46, 542,   1,   3,   3,   3,   3,   3,   3,   3,   3,\n",
      "          3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,\n",
      "          3,   3,   3,   3,   3,   3])\n",
      "tensor([361,  52, 194, 206,  46,   1,   3,   3,   3,   3,   3,   3,   3,   3,\n",
      "          3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,\n",
      "          3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,\n",
      "          3,   3,   3,   3,   3,   3])\n"
     ]
    }
   ],
   "source": [
    "for d in data[1]:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  6,  10, 179,  21,  49,  52,  29, 180,  83, 181,  51, 182,  51, 183,\n",
       "         83,   6,  49, 184, 169,  51, 185,  54,  55, 186, 187, 188, 189, 180,\n",
       "         51,   1,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,\n",
       "          3,   3,   3,   3,   3,   3])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_idx_2_sent(tensor, lang_obj):\n",
    "    word_list = []\n",
    "    for i in tensor:\n",
    "        if i.item() not in set([PAD_IDX,EOS_token,SOS_token]):\n",
    "            word_list.append(lang_obj.index2word[i.item()])\n",
    "    return (' ').join(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bleu_score import BLEU_SCORE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl = BLEU_SCORE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(encoder, decoder, dataloader, loss_fun, lang_en):\n",
    "    encoder.train(False)\n",
    "    decoder.train(False)\n",
    "    pred_corpus = []\n",
    "    true_corpus = []\n",
    "    running_loss = 0\n",
    "    running_total = 0\n",
    "    bl = BLEU_SCORE()\n",
    "    for data in dataloader:\n",
    "        encoder_i = data[0].cuda()\n",
    "        decoder_i = data[1].cuda()\n",
    "        bs,sl = encoder_i.size()[:2]\n",
    "        out, hidden = encode_decode(encoder,decoder,encoder_i,decoder_i)\n",
    "        loss = loss_fun(out.float(), decoder_i.long())\n",
    "        running_loss += loss.item()*bs\n",
    "        running_total += bs\n",
    "        pred = torch.max(out,dim = 1)[1]\n",
    "        for t,p in zip(data[1],pred):\n",
    "            t,p = convert_idx_2_sent(t,lang_en), convert_idx_2_sent(p,lang_en)\n",
    "            true_corpus.append(t)\n",
    "            pred_corpus.append(p)\n",
    "    score = bl.corpus_bleu(pred_corpus,[true_corpus],lowercase=True)[0]\n",
    "    return running_loss/running_total, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 48])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i &apos;d like to talk to you today about the scale of the scientific effort that goes into making the headlines you see in the paper .'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_idx_2_sent(data[1][3],en_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 48])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size,bi):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.bi=bi\n",
    "        if self.bi:\n",
    "            self.mul=2\n",
    "        else:\n",
    "            self.mul=1\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size,batch_first=True,bidirectional=self.bi)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)\n",
    "#         bss = input.size(0)\n",
    "        output = embedded\n",
    "#         print(\"emb size\",output.size(),hidden.size())\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "#         print(\"rnn out\",output.size())\n",
    "#         print(\"rnn hid\",hidden.size())\n",
    "        \n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self,bs):\n",
    "        return torch.zeros(self.mul, bs, self.hidden_size).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size,bi,attention_type_2 = None):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size,batch_first=True)\n",
    "        \n",
    "        self.attention_type_2 = attention_type_2\n",
    "        if self.attention_type_2 is not None:\n",
    "            self.attn = nn.Linear(self.hidden_size*2, self.hidden_size*2)\n",
    "            self.attn_drop = nn.Dropout(p = 0.5)\n",
    "            \n",
    "        else:\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, MAX_LEN)\n",
    "        \n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2+self.hidden_size, self.hidden_size)\n",
    "\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden,encoder_outputs):\n",
    "        bss = input.size(0)\n",
    "#         print(input.size(),bss)\n",
    "#         print(bss)\n",
    "#         print(\"in\",input.size())\n",
    "#         print(\"hid\",hidden.size())\n",
    "        \n",
    "        output = self.embedding(input)\n",
    "        output = self.dropout(output)\n",
    "#         print(output.size(),hidden.size())\n",
    "#         print(output.size(),hidden[0].unsqueeze(1).size())\n",
    "#         print(torch.cat((output, hidden[0].unsqueeze(1)), 2).size())\n",
    "        cat = torch.cat((output, hidden[0].unsqueeze(1)), 2)\n",
    "#         print(cat.size())\n",
    "        att_out = self.attn_drop(self.attn(cat))\n",
    "#         print('att_out:',att_out.size())\n",
    "#         att_out = F.softmax(self.attn(cat),dim=1)\n",
    "#         print(\"attn out\",att_out.size(),\"en_out\",encoder_outputs.size())\n",
    "        attn_wts = F.softmax(torch.bmm(encoder_outputs,att_out.transpose(1,2)),dim = 1)\n",
    "        attn_applied = torch.sum(encoder_outputs * attn_wts, dim = 1).unsqueeze(1)\n",
    "#         print(\"attn_applied\" , attn_applied.size())\n",
    "        \n",
    "#         print(\"attn wts\",attn_wts.size(),\"enb out\",output.size())\n",
    "        attn_cat = torch.cat((output, attn_applied), 2)\n",
    "#         print(\"attn cat\",attn_cat.size())\n",
    "        attn_comb = self.attn_combine(attn_cat)\n",
    "#         print(attn_comb.size())\n",
    "        \n",
    "        \n",
    "#         .view(bss,-1,self.hidden_size)\n",
    "#         print(\"ou\",output.size())\n",
    "        output = F.relu(attn_comb)\n",
    "#         print(\"ou rel\",output.size())\n",
    "#         print(\"hidden\",hidden[0].size())\n",
    "        output, hidden = self.gru(output, hidden[0].unsqueeze(0))\n",
    "#         print(\"out\",output.size(),\"hid\",hidden.size())\n",
    "        output = self.out(output.squeeze(dim=1))\n",
    "#         print(output.size())\n",
    "        output = self.softmax(output)\n",
    "#         print(\"sm\",output.size())\n",
    "#         print(output.size(),hidden.size())\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1,bs, self.hidden_size).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def encode_decode(encoder,decoder,data_en,data_de):\n",
    "    use_teacher_forcing = True if random.random() < 0.5 else False\n",
    "#     use_teacher_forcing = True\n",
    "    bss = data_en.size(0)\n",
    "#     print(\"data de\",data_de.size())\n",
    "    en_h = encoder.initHidden(bss)\n",
    "    en_out,en_hid = encoder(data_en,en_h)\n",
    "    \n",
    "    decoder_hidden = en_hid\n",
    "    decoder_input = torch.tensor([[SOS_token]]*bss).cuda()\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        d_out = []\n",
    "        for i in range(MAX_LEN):\n",
    "            decoder_output,decoder_hidden = decoder(decoder_input,decoder_hidden,en_out)\n",
    "            d_out.append(decoder_output.unsqueeze(-1))\n",
    "            decoder_input = data_de[:,i].view(-1,1)\n",
    "#             print(decoder_input.size())\n",
    "        d_hid = decoder_hidden\n",
    "        d_out = torch.cat(d_out,dim=-1)\n",
    "#         d_out,d_hid = decoder(decoder_input,decoder_hidden)  \n",
    "#         d_out,d_hid = decoder(data_de,d_hid)\n",
    "    else:\n",
    "        d_out = []\n",
    "        for i in range(MAX_LEN):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input,decoder_hidden,en_out)\n",
    "            d_out.append(decoder_output.unsqueeze(-1))\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach().view(-1,1)\n",
    "        d_hid = decoder_hidden\n",
    "        d_out = torch.cat(d_out,dim=-1)\n",
    "#         print(len(d_out),d_out[0].size(),torch.cat(d_out,dim=1).size())\n",
    "    return d_out, d_hid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(encoder_optimizer,decoder_optimizer, encoder, decoder, loss_fun, num_epochs=60):\n",
    "    best_score = 0\n",
    "    best_au = 0\n",
    "    loss_hist = {'train': [], 'validate': []}\n",
    "    acc_hist = {'train': [], 'validate': []}\n",
    "    for epoch in range(num_epochs):\n",
    "        for ex, phase in enumerate(['train']):\n",
    "            start = time.time()\n",
    "            total = 0\n",
    "            top1_correct = 0\n",
    "            running_loss = 0\n",
    "            running_total = 0\n",
    "            if phase == 'train':\n",
    "                encoder.train(True)\n",
    "                decoder.train(True)\n",
    "            else:\n",
    "                encoder.train(False)\n",
    "                decoder.train(False)\n",
    "            for data in dataloader[phase]:\n",
    "                encoder_optimizer.zero_grad()\n",
    "                decoder_optimizer.zero_grad()\n",
    "\n",
    "                encoder_i = data[0].cuda()\n",
    "                decoder_i = data[1].cuda()\n",
    "                                \n",
    "                out, hidden = encode_decode(encoder,decoder,encoder_i,decoder_i)\n",
    "#                 print(out.size(),hidden.size())\n",
    "#                 print(out.float().size(),decoder_i.long().view(-1).size())\n",
    "#                 print(out.size())\n",
    "#                 print(decoder_i.size())\n",
    "#                 _, top1_predicted = torch.max(out, dim=2)\n",
    "#                 print(out.float().view(-1,out.size(-1)).size(),decoder_i.long().view(-1).size())\n",
    "#                 print(top1_predicted.float().view(-1).size())\n",
    "#                 loss = loss_fun(out.float().view(-1,out.size(-1)), decoder_i.long().view(-1))\n",
    "                loss = loss_fun(out.float(), decoder_i.long())\n",
    "                N = decoder_i.size(0)\n",
    "#                 print(N)\n",
    "                running_loss += loss.item() * N\n",
    "                \n",
    "                total += N\n",
    "                \n",
    "#                 _, top1_predicted = torch.max(y_out, dim=1)\n",
    "#                 top1_correct += int((top1_predicted == y).sum())\n",
    "\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    encoder_optimizer.step()\n",
    "                    decoder_optimizer.step()\n",
    "                    \n",
    "            loss, score = validation(encoder,decoder, dataloader['validate'],loss_fun, en_lang)\n",
    "            print('Validation Loss = ', loss)\n",
    "            print('Validation BLEU = ', score)\n",
    "            loss, score = validation(encoder,decoder, dataloader['train'],loss_fun, en_lang)\n",
    "            print('Training Loss = ', loss)\n",
    "            print('Traning BLEU = ', score)\n",
    "#                 running_total += N\n",
    "            epoch_loss = running_loss / total\n",
    "#             epoch_acc = top1_correct / total\n",
    "            epoch_acc = 0\n",
    "            loss_hist[phase].append(epoch_loss)\n",
    "            acc_hist[phase].append(epoch_acc)\n",
    "            print(\"epoch {} {} loss = {}, accurancy = {} time = {}\".format(epoch, phase, epoch_loss, epoch_acc,\n",
    "                                                                           time.time() - start))\n",
    "        if phase == 'validate' and epoch_acc > best_score:\n",
    "            best_score = epoch_acc\n",
    "#             torch.save(model, save_dir+save_name+str(n_channel)+str(n_top)+str(vocab_size))\n",
    "    print(\"Training completed. Best accuracy is {}\".format(best_score))\n",
    "    return encoder,decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='gpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47862"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_lang.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "bi=True\n",
    "# bi=False\n",
    "encoder = EncoderRNN(vi_lang.n_words,300,bi).cuda()\n",
    "decoder = DecoderRNN(300,en_lang.n_words,bi, attention_type_2='t2').cuda()\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss =  1.9437234900435623\n",
      "Validation BLEU =  0.875457240950905\n",
      "Training Loss =  1.8946297312027731\n",
      "Traning BLEU =  0.8607497013842056\n",
      "epoch 0 train loss = 2.025610027620784, accurancy = 0 time = 3408.589726448059\n",
      "Training completed. Best accuracy is 0\n"
     ]
    }
   ],
   "source": [
    "enc, dec = train_model(encoder_optimizer,decoder_optimizer, encoder, decoder, criterion, num_epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 47862])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = next(iter(dataloader['train']))\n",
    "out,hid = encode_decode(enc,dec,data[0][1].view(1,-1).cuda(),data[1][1].view(1,-1).cuda())\n",
    "_, top1_predicted = torch.max(out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 68, 104,  21, 105, 102, 106, 107,  93, 108, 109,  95,  68, 110,  16,\n",
       "          96, 111,  98,  46,   1,   3,   3,   3,   3,   3,   3,   3,   3,   3,\n",
       "           3,   3]], device='cuda:0')"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top1_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "given=[]\n",
    "for i in data[1][1]:\n",
    "    given.append(vi_lang.index2word[i.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "for i in top1_predicted[0]:\n",
    "#     print(i.item())\n",
    "    pred.append(en_lang.index2word[i.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trông',\n",
       " '40',\n",
       " 'những',\n",
       " 'quốc_gia',\n",
       " 'nhà',\n",
       " 'khác',\n",
       " 'nhau',\n",
       " 'ipcc',\n",
       " 'gần',\n",
       " '1000',\n",
       " 'ra_bài',\n",
       " 'trông',\n",
       " 'trang',\n",
       " 'khí_quyển',\n",
       " 'họ',\n",
       " 'chủ_đề',\n",
       " 'được',\n",
       " 'để',\n",
       " 'EOS',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD']"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and',\n",
       " 'all',\n",
       " 'of',\n",
       " 'those',\n",
       " 'pages',\n",
       " 'were',\n",
       " 'reviewed',\n",
       " 'by',\n",
       " 'another',\n",
       " '400-plus',\n",
       " 'scientists',\n",
       " 'and',\n",
       " 'reviewers',\n",
       " ',',\n",
       " 'from',\n",
       " '113',\n",
       " 'countries',\n",
       " '.',\n",
       " 'EOS',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD']"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = []\n",
    "for i in data[1][1]:\n",
    "#     print(i.item())\n",
    "    true.append(en_lang.index2word[i.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and',\n",
       " 'all',\n",
       " 'of',\n",
       " 'those',\n",
       " 'pages',\n",
       " 'were',\n",
       " 'reviewed',\n",
       " 'by',\n",
       " 'another',\n",
       " '400-plus',\n",
       " 'scientists',\n",
       " 'and',\n",
       " 'reviewers',\n",
       " ',',\n",
       " 'from',\n",
       " '113',\n",
       " 'countries',\n",
       " '.',\n",
       " 'EOS',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD']"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
